<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>🤖 JetBot 도로 주행 프로젝트 v3 (CORS 최종 수정 시도)</title>
    <style>
        /* --- 기본 스타일 --- */
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: #f8f9fa;
            color: #343a40;
        }

        .container {
            max-width: 1000px;
            margin: 30px auto;
            padding: 30px;
            background-color: #ffffff;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
            border-radius: 8px;
        }

        /* --- 제목 스타일 --- */
        h1, h2, h3, h4 {
            color: #0b3d91;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            font-weight: 600;
        }
        h1 { text-align: center; border-bottom: 3px solid #76b900; }
        h2 { border-bottom: 2px solid #76b900; margin-top: 2.5rem; }
        h3 { border-bottom: 1px dashed #ced4da; margin-top: 2rem; }
        h4 { color: #17a2b8; margin-top: 1.5rem; font-weight: 500;}

        /* --- 코드 블록 --- */
        pre {
            background-color: #282c34;
            color: #abb2bf;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            border: 1px solid #3e4451;
            font-size: 0.9em;
            line-height: 1.5;
        }
        code {
            font-family: 'Fira Code', 'Consolas', 'Monaco', monospace;
        }
        pre code { background-color: transparent; padding: 0; }
        :not(pre) > code {
            background-color: #e9ecef;
            color: #c82c7a;
            padding: 0.2em 0.5em;
            margin: 0 0.1em;
            font-size: 85%;
            border-radius: 4px;
            font-weight: 500;
        }

        /* --- 기타 요소 --- */
        strong { color: #28a745; font-weight: 600; }
        ul, ol { margin-left: 25px; padding-left: 0; }
        li { margin-bottom: 12px; }
        a { color: #007bff; text-decoration: none; font-weight: 500; }
        a:hover { text-decoration: underline; color: #0056b3; }
        hr { border: 0; height: 1px; background-color: #dee2e6; margin: 45px 0; }
        .emoji { margin-right: 7px; vertical-align: -0.1em; }
        blockquote {
            border-left: 4px solid #76b900;
            padding-left: 15px; margin: 20px 0; color: #495057;
            background-color: #f1f3f5; padding-top: 10px; padding-bottom: 10px;
            border-radius: 0 4px 4px 0;
        }
        .warning { color: #dc3545; font-weight: bold; }
        .tip { color: #17a2b8; font-weight: bold; }

        /* --- 3D 뷰어 컨테이너 --- */
        #viewer-container {
            width: 100%; height: 550px; margin: 30px 0;
            border: 1px solid #ced4da; position: relative;
            background-color: #e9ecef; border-radius: 5px; overflow: hidden;
        }
        #loading-indicator {
            position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%);
            font-size: 1.3em; color: #495057; background-color: rgba(255, 255, 255, 0.85);
            padding: 12px 24px; border-radius: 5px; box-shadow: 0 2px 6px rgba(0,0,0,0.15);
            text-align: center;
        }

        /* --- 섹션 구분 --- */
        .section { margin-bottom: 45px; }

        /* --- 반응형 디자인 --- */
        @media (max-width: 768px) {
            .container { margin: 15px; padding: 20px; }
            #viewer-container { height: 450px; }
            h1 { font-size: 1.8em; } h2 { font-size: 1.5em; }
            pre { padding: 15px; }
        }
    </style>
</head>
<body>
    <div class="container">

        <h1><span class="emoji">🤖</span> JetBot 도로 주행 프로젝트: 완전 정복 가이드</h1>
        <p style="text-align: center; font-size: 1.1em; color: #6c757d;">데이터 수집부터 TensorRT 최적화, 실시간 데모까지! (インタラクティブ 3D 모델 포함)</p>

        <hr>

        <div class="section">
            <h2><span class="emoji">✨</span> JetBot 3D 모델 미리보기</h2>
            <p>프로젝트의 주인공, JetBot의 3D 모델을 직접 살펴보세요! 마우스를 사용하여 아래 뷰어에서 자유롭게 조작할 수 있습니다.</p>
            <ul>
                <li><span class="emoji">🔄</span> <strong>회전:</strong> 마우스 왼쪽 버튼 클릭 + 드래그</li>
                <li><span class="emoji">🔍</span> <strong>확대/축소:</strong> 마우스 휠 스크롤</li>
                <li><span class="emoji">🖐️</span> <strong>이동(패닝):</strong> 마우스 오른쪽 버튼 클릭 + 드래그 (또는 Ctrl/Cmd + 왼쪽 버튼 클릭 + 드래그)</li>
            </ul>
            <div id="viewer-container">
                <div id="loading-indicator">⏳ JetBot 모델 로딩 중... 잠시만 기다려주세요!</div>
            </div>
             <blockquote class="tip"><span class="emoji">💡</span> 모델 로딩 방식: GitHub Raw URL을 사용하므로 인터넷 연결이 필요합니다. CORS 문제 해결을 위해 GitHub Pages나 다른 정적 호스팅 서비스를 이용하는 것이 더 안정적일 수 있습니다. 이 예제는 GitHub Raw 접근을 시도합니다.</blockquote>
        </div>

        <hr>

        <!-- 이하 문서 내용은 이전 답변과 동일하게 유지됩니다. -->
        <!-- Project Overview -->
        <div class="section" id="project-overview">
            <h2><span class="emoji">🎯</span> 1. 프로젝트 개요: JetBot, 스스로 길을 찾다!</h2>
             <p>이 문서는 NVIDIA JetBot이 카메라(<span class="emoji">📷</span>)로 세상을 보고, 스스로 판단하여 도로(또는 지정된 트랙)를 따라 주행하는 <strong>자율 주행 시스템</strong>을 만드는 전 과정을 안내하는 종합 가이드입니다. 마치 JetBot에게 운전을 가르치는 과정과 같죠! <span class="emoji">👨‍🏫</span> <span class="emoji">➡️</span> <span class="emoji">🤖</span></p>
             <p>총 4개의 핵심 단계를 거치며 JetBot을 똑똑한 드라이버로 만들어 봅시다:</p>
             <ol>
                 <li><strong><span class="emoji">📸</span> 데이터 수집 (Data Collection):</strong> JetBot의 눈으로 본 주행 경로 이미지와 "이리로 가!"라고 알려주는 목표 지점 데이터를 만듭니다. 양질의 데이터가 똑똑한 AI의 시작입니다!</li>
                 <li><strong><span class="emoji">🧠</span> 모델 학습 (Model Training):</strong> 수집한 데이터를 딥러닝 모델(ResNet-18 기반)에게 학습시켜, 이미지 속에서 최적의 목표 지점을 스스로 예측하는 능력을 길러줍니다.</li>
                 <li><strong><span class="emoji">⚡</span> TensorRT 최적화 (TensorRT Optimization):</strong> 학습된 모델을 Jetson Nano와 같은 엣지 디바이스에서 번개처럼(<span class="emoji">⚡</span>) 빠르게 작동하도록 변환하고 최적화합니다. 실시간 주행의 핵심이죠!</li>
                 <li><strong><span class="emoji">🚀</span> 실시간 데모 (Live Demo):</strong> 최적화된 모델을 JetBot에 탑재하여, 실시간 카메라 영상을 보며 스스로 판단하고 주행하는 놀라운 광경을 직접 확인합니다!</li>
             </ol>
             <p>자, 이제 JetBot과 함께 흥미진진한 자율 주행의 세계로 떠나볼까요?</p>
        </div>
        <hr>
        <!-- Data Collection -->
        <div class="section" id="data-collection">
             <h2><span class="emoji">📸</span> 2. 데이터 수집: AI의 눈을 위한 양식 만들기</h2>
             <p>훌륭한 AI 모델을 만들려면 좋은 '식재료', 즉 <strong>데이터</strong>가 필수입니다. 이 단계에서는 JetBot이 직접 주행할 환경에서 '어떻게 운전해야 하는지'에 대한 정보를 담은 데이터를 수집합니다.</p>
             <h3><span class="emoji">⭐</span> 2.1. 목표: 무엇을, 어떻게 수집하나?</h3>
             <ul>
                 <li>JetBot의 카메라를 통해 실제 주행 경로의 이미지를 캡처합니다.</li>
                 <li>각 이미지마다, JetBot이 다음 순간 향해야 할 <strong>최적의 목표 지점(x, y 픽셀 좌표)</strong>을 마우스 클릭으로 기록(라벨링)합니다.</li>
                 <li><span class="emoji">💡</span> <strong>다양성이 생명!</strong> 모델이 다양한 실제 상황에 잘 대응하도록, 여러 위치, 각도, 조명 아래에서 데이터를 수집하는 것이 매우 중요합니다. (예: 트랙 중앙, 가장자리, 커브 진입/탈출 등) 최소 수백 장 이상 수집하는 것을 권장합니다.</li>
             </ul>
             <h3><span class="emoji">🗺️</span> 2.2. 라벨링 가이드: "어디를 찍어야 할까요?"</h3>
             <p>정확하고 일관된 라벨링은 모델 성능을 좌우합니다. 목표 지점을 선택할 때 다음 가이드라인을 참고하세요:</p>
             <ol>
                 <li><strong>실시간 영상 주시:</strong> Jupyter Lab에 표시되는 JetBot 카메라 영상을 주의 깊게 살펴봅니다.</li>
                 <li><strong>이상적인 경로 상상:</strong> 로봇이 트랙 중앙 또는 특정 라인을 따라 부드럽게 나아가야 할 경로를 그려봅니다.</li>
                 <li><strong>"가장 먼 안전 지점" 선택:</strong> 로봇이 현재 위치에서 그 지점을 향해 <strong>직진했을 때, 경로를 벗어나지 않을 것으로 예상되는 가장 먼 지점</strong>을 클릭합니다.
                     <ul>
                         <li><strong>직선 구간:</strong> 시야가 확보되는 한 최대한 멀리 찍어도 좋습니다.</li>
                         <li><strong>커브 구간:</strong> <span class="warning">주의!</span> 커브 안쪽을 따라 너무 멀리 찍으면 로봇이 경로를 이탈할 수 있습니다. 커브를 안전하게 돌 수 있는 비교적 가까운 지점을 선택해야 합니다.</li>
                     </ul>
                 </li>
                 <li><strong>일관성 유지:</strong> 자신만의 라벨링 기준을 정하고, 데이터 수집 내내 일관되게 적용하는 것이 중요합니다.</li>
             </ol>
             <blockquote>"Garbage in, garbage out." - 데이터 과학의 오랜 격언처럼, 데이터의 품질이 모델의 성능을 결정합니다. 신중하게 라벨링해주세요!</blockquote>
             <h3><span class="emoji">💻</span> 2.3. 구현 코드 (데이터 수집 위젯 설정)</h3>
              <p>Jupyter Lab에서 실행하는 Python 코드입니다. 카메라 영상을 보여주고, 사용자가 이미지를 클릭하면 해당 좌표와 함께 이미지를 저장하는 인터페이스를 만듭니다.</p>
            <pre><code class="language-python">
# --- 필요한 라이브러리 임포트 ---
import ipywidgets # Jupyter 위젯
import traitlets # 위젯 간 연결
from IPython.display import display, HTML # Jupyter 디스플레이
from jetbot import Robot, Camera, bgr8_to_jpeg # JetBot 라이브러리
from uuid import uuid1 # 고유 ID 생성
import os
import glob
import cv2 # OpenCV (이미지 처리)
import time
from jupyter_clickable_image_widget import ClickableImageWidget # 클릭 가능한 이미지 위젯

# --- 설정 ---
DATASET_DIR = 'dataset_xy' # 데이터 저장 디렉토리

# --- 디렉토리 생성 ---
try:
    os.makedirs(DATASET_DIR)
    print(f"<span class='emoji'>📂</span> '{DATASET_DIR}' 디렉토리를 생성했습니다.")
except FileExistsError:
    print(f"<span class='emoji'>✅</span> '{DATASET_DIR}' 디렉토리가 이미 존재합니다.")

# --- 카메라 및 위젯 초기화 ---
camera = Camera(width=224, height=224) # 모델 입력 크기에 맞춘 카메라
camera_widget = ClickableImageWidget(width=camera.width, height=camera.height) # 클릭 가능한 라이브 뷰
snapshot_widget = ipywidgets.Image(width=camera.width, height=camera.height) # 저장된 스냅샷 뷰
count_widget = ipywidgets.IntText(description='<span class="emoji">💾</span> 저장된 이미지:', value=0, disabled=True, layout=ipywidgets.Layout(width='200px')) # 이미지 개수 표시

# --- 카메라 영상 연결 ---
traitlets.dlink((camera, 'value'), (camera_widget, 'value'), transform=bgr8_to_jpeg)

# --- 초기 이미지 개수 업데이트 ---
count_widget.value = len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))

# --- 이미지 클릭 시 실행될 함수 ---
def save_snapshot(_, content, msg):
    if content['event'] == 'click':
        data = content['eventData']
        x = data['offsetX']
        y = data['offsetY']

        # 고유 파일명 생성 (좌표 포함)
        uuid_str = str(uuid1())
        filename = f'xy_{x:03d}_{y:03d}_{uuid_str}.jpg'
        image_path = os.path.join(DATASET_DIR, filename)

        # 이미지 파일 저장 (JPEG)
        try:
            with open(image_path, 'wb') as f:
                f.write(camera_widget.value)
        except Exception as e:
            print(f"<span class='emoji'>❌</span> 이미지 저장 오류: {e}")
            return # 오류 발생 시 중단

        # 스냅샷에 녹색 원 표시
        snapshot = camera.value.copy()
        cv2.circle(snapshot, (x, y), 8, (0, 255, 0), 3) # 녹색 원 (BGR 순서)
        snapshot_widget.value = bgr8_to_jpeg(snapshot)

        # 이미지 개수 업데이트
        count_widget.value = len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))

# --- 클릭 이벤트 핸들러 등록 ---
camera_widget.on_msg(save_snapshot)

# --- 위젯 레이아웃 및 표시 ---
data_collection_widget = ipywidgets.VBox([
    HTML("<h2><span class='emoji'>🖱️</span> 이미지 클릭으로 데이터 수집 시작!</h2>"),
    ipywidgets.HBox([
        ipywidgets.VBox([HTML("<h4>라이브 카메라</h4>"), camera_widget]),
        ipywidgets.VBox([HTML("<h4>저장된 스냅샷 (클릭 지점)</h4>"), snapshot_widget])
    ]),
    count_widget
])

display(data_collection_widget)
print("<span class='emoji'>✅</span> 카메라 및 위젯 준비 완료. 라이브 이미지를 클릭하여 데이터 수집을 시작하세요!")
            </code></pre>
              <h3><span class="emoji">📊</span> 2.4. 데이터 확인: 수집 결과는?</h3>
             <p>데이터 수집을 완료한 후, Jupyter Lab의 파일 브라우저에서 <code>dataset_xy</code> 폴더를 열어보세요. 수많은 <code>xy_<x>_<y>_<uuid>.jpg</code> 파일들이 생성된 것을 확인할 수 있습니다. 파일명에 포함된 <code><x></code>와 <code><y></code> 숫자가 바로 여러분이 클릭한 소중한 라벨 정보입니다. 이 데이터가 다음 단계에서 JetBot의 '지능'을 만드는 데 사용됩니다!</p>
        </div>
        <hr>
        <!-- Model Training -->
        <div class="section" id="model-training">
            <h2><span class="emoji">🧠</span> 3. 모델 학습: JetBot에게 운전 가르치기</h2>
            <p>수집된 데이터를 이용해 JetBot의 '두뇌'가 될 딥러닝 모델을 학습시킵니다. 목표는 이미지를 보고, 우리가 알려준 정답(목표 지점 좌표)을 스스로 예측하도록 만드는 것입니다. 일종의 '이미지 보고 좌표 맞추기' 훈련이죠!</p>
            <h3><span class="emoji">⭐</span> 3.1. 학습 목표</h3>
             <ul>
                 <li>입력 이미지(224x224)로부터 목표 지점의 정규화된 (x, y) 좌표 [-1, 1] 범위를 예측하는 <strong>회귀(Regression)</strong> 모델을 만듭니다.</li>
                 <li>이미지 인식 분야에서 검증된 <strong>ResNet-18</strong> 아키텍처를 기반으로 사용합니다.</li>
                 <li><strong>전이 학습(Transfer Learning)</strong> <span class="emoji">🎓</span>: 이미 방대한 이미지 데이터(ImageNet)로 학습된 ResNet-18 모델을 가져와, 우리의 도로 주행 문제에 맞게 마지막 부분만 미세 조정합니다. 이렇게 하면 적은 데이터로도 효과적으로 학습할 수 있습니다!</li>
                 <li>딥러닝 프레임워크는 <strong>PyTorch</strong>를 사용합니다.</li>
             </ul>
              <h3><span class="emoji">🛠️</span> 3.2. 데이터셋 준비: 학습 재료 손질하기</h3>
             <h4>3.2.1. 커스텀 데이터셋 클래스 (<code>XYDataset</code>)</h4>
              <p>PyTorch가 데이터를 효율적으로 다룰 수 있도록, 우리 데이터 형식에 맞는 <code>Dataset</code> 클래스를 정의합니다. 주요 역할은 다음과 같습니다:</p>
             <ul>
                 <li>이미지 파일 로드 및 경로 관리</li>
                 <li>파일명에서 x, y 좌표 추출 및 [-1, 1] 범위로 <strong>정규화</strong></li>
                 <li><strong>데이터 증강(Data Augmentation)</strong> 적용 (모델 강건성 향상):
                     <ul>
                         <li><span class="emoji">🎨</span> <code>ColorJitter</code>: 밝기, 대비 등 무작위 변경</li>
                         <li><span class="emoji">↔️</span> <code>Random Horizontal Flip</code>: 무작위 좌우 반전 (필요시 x 좌표 부호 변경)</li>
                     </ul>
                 </li>
                 <li>이미지 크기 조정 (224x224) 및 PyTorch <strong>텐서(Tensor)</strong> 변환</li>
                 <li>ImageNet 기반 <strong>정규화</strong> 적용 (평균 빼고 표준편차로 나누기)</li>
             </ul>
             <pre><code class="language-python">
import torch
import torch.optim as optim
import torch.nn.functional as F
import torchvision
import torchvision.datasets as datasets
import torchvision.models as models
import torchvision.transforms as transforms
import glob
import PIL.Image
import os
import numpy as np

# --- 좌표 추출 및 정규화 함수 ---
def get_x(path, width):
    x_pixel = float(int(os.path.basename(path).split('_')[1]))
    return (x_pixel - width / 2) / (width / 2)

def get_y(path, height):
    y_pixel = float(int(os.path.basename(path).split('_')[2]))
    # Y축은 아래로 갈수록 값이 커지므로, 정규화하면 위쪽이 -1, 아래쪽이 1에 가까움
    return (y_pixel - height / 2) / (height / 2)

# --- 커스텀 데이터셋 클래스 ---
class XYDataset(torch.utils.data.Dataset):
    def __init__(self, directory, random_hflips=False):
        self.directory = directory
        self.random_hflips = random_hflips
        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))
        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)
        # 이미지 전처리 파이프라인 정의
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(), # PIL -> Tensor, [0,1]
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet 정규화
        ])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        image = PIL.Image.open(image_path).convert('RGB') # RGB로 로드
        width, height = image.size
        x = float(get_x(image_path, width))
        y = float(get_y(image_path, height))

        # 데이터 증강 (Augmentation)
        if self.random_hflips and np.random.rand() > 0.5:
            image = transforms.functional.hflip(image)
            x = -x # 좌우 반전 시 x 좌표 부호 반전!
        image = self.color_jitter(image) # 색상 증강

        # 최종 전처리 적용
        image = self.transform(image)

        return image, torch.tensor([x, y]).float() # 이미지와 좌표 텐서 반환

# --- 데이터셋 인스턴스 생성 ---
# dataset = XYDataset(DATASET_DIR, random_hflips=False) # 실제 환경에서는 DATASET_DIR 사용
# print(f"<span class='emoji'>📚</span> 데이터셋 로드 완료! 총 이미지: {len(dataset)}") # 실제 환경에서 주석 해제
            </code></pre>
            <h4>3.2.2. 데이터 분할 및 로더 생성</h4>
              <p>전체 데이터를 <strong>훈련용</strong>과 <strong>테스트용</strong>으로 나눕니다(예: 90% 훈련, 10% 테스트). <code>DataLoader</code>는 데이터를 미니배치(minibatch) 단위로 묶고 섞어주어(<span class="emoji">🔀</span>) 효율적인 학습을 돕습니다.</p>
             <pre><code class="language-python">
# --- 데이터 분할 ---
# test_percent = 0.1
# num_total = len(dataset) # 실제 환경에서 dataset 사용
# num_test = int(test_percent * num_total)
# num_train = num_total - num_test
# train_dataset, test_dataset = torch.utils.data.random_split(dataset, [num_train, num_test])
# print(f"데이터 분할: 훈련 {num_train}개 ({100-test_percent*100:.0f}%), 테스트 {num_test}개 ({test_percent*100:.0f}%)")

# --- 데이터 로더 생성 ---
# BATCH_SIZE = 8 # 배치 크기 (GPU 메모리 고려)
# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
# print(f"<span class='emoji'>📦</span> 데이터 로더 생성 완료! 배치 크기: {BATCH_SIZE}")
            </code></pre>
              <h3><span class="emoji">🏗️</span> 3.3. 모델 아키텍처 정의 (ResNet-18 수정)</h3>
              <p>사전 학습된 ResNet-18 모델을 불러와, 마지막 출력 레이어만 (x, y) 좌표 2개를 출력하도록 교체합니다. 이것이 바로 <strong>전이 학습</strong>의 마법!</p>
             <pre><code class="language-python">
# --- 모델 정의 ---
model = models.resnet18(pretrained=True) # ImageNet 사전 학습 가중치 사용
num_output_features = 2 # x, y 좌표 2개 출력
# 마지막 레이어(fc)를 새로운 Linear 레이어로 교체
model.fc = torch.nn.Linear(model.fc.in_features, num_output_features)

# GPU 사용 설정
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
print(f"<span class='emoji'>✅</span> ResNet-18 모델 준비 완료! (사용 디바이스: {device})")
            </code></pre>
             <h3><span class="emoji">🏋️</span> 3.4. 모델 학습 실행</h3>
             <p>이제 정의된 모델과 준비된 데이터를 사용하여 학습을 시작합니다. <strong>옵티마이저(Optimizer)</strong>는 모델의 가중치를 업데이트하는 방법을 결정하고(여기서는 Adam 사용), <strong>손실 함수(Loss Function)</strong>는 모델의 예측이 정답과 얼마나 다른지 측정합니다(여기서는 MSE Loss 사용).</p>
             <p>여러 번의 <strong>에포크(Epoch)</strong>를 반복하며 모델은 점차 오차를 줄여나가고, 테스트 데이터로 평가하여 가장 성능이 좋았던 모델을 저장합니다.</p>
            <pre><code class="language-python">
# --- 학습 설정 ---
NUM_EPOCHS = 70 # 총 에포크 수 (조절 가능)
BEST_MODEL_PATH = 'best_steering_model_xy.pth' # 최고 모델 저장 경로
# best_loss = float('inf') # 최고 성능 기록용 (낮을수록 좋음)

# 옵티마이저 (Adam) 및 손실 함수 (MSE) 정의
# optimizer = optim.Adam(model.parameters())
# criterion = torch.nn.MSELoss() # Mean Squared Error Loss

# print(f"<span class='emoji'>🚀</span> 모델 학습 시작! (총 에포크: {NUM_EPOCHS})")

# --- 학습 루프 (실제 환경에서는 주석 해제 후 실행) ---
# for epoch in range(NUM_EPOCHS):
#     # 훈련
#     model.train()
#     running_train_loss = 0.0
#     for images, labels in train_loader:
#         images, labels = images.to(device), labels.to(device) # 데이터 GPU 이동
#         optimizer.zero_grad() # 기울기 초기화
#         outputs = model(images) # 예측
#         loss = criterion(outputs, labels) # 손실 계산
#         loss.backward() # 역전파 (기울기 계산)
#         optimizer.step() # 가중치 업데이트
#         running_train_loss += loss.item() * images.size(0) # 배치 손실 누적

#     epoch_train_loss = running_train_loss / len(train_loader.dataset)

#     # 평가
#     model.eval()
#     running_test_loss = 0.0
#     with torch.no_grad(): # 평가 시 기울기 계산 불필요
#         for images, labels in test_loader:
#             images, labels = images.to(device), labels.to(device)
#             outputs = model(images)
#             loss = criterion(outputs, labels)
#             running_test_loss += loss.item() * images.size(0)

#     epoch_test_loss = running_test_loss / len(test_loader.dataset)

#     print(f'Epoch [{epoch+1:02d}/{NUM_EPOCHS}], Train Loss: {epoch_train_loss:.6f}, Test Loss: {epoch_test_loss:.6f}')

#     # 최고 모델 저장
#     if epoch_test_loss < best_loss:
#         print(f'  <span class="emoji">✨</span> Test Loss 개선! ({best_loss:.6f} -> {epoch_test_loss:.6f}). 모델 저장: {BEST_MODEL_PATH}')
#         torch.save(model.state_dict(), BEST_MODEL_PATH)
#         best_loss = epoch_test_loss

# print(f"<span class='emoji'>🎉</span> 모델 학습 완료! 최종 최고 Test Loss: {best_loss:.6f}")
            </code></pre>
              <h3><span class="emoji">📊</span> 3.5. 학습 결과: 똑똑해진 JetBot의 두뇌!</h3>
             <p>학습 로그를 통해 훈련 손실과 테스트 손실이 점차 감소하는 것을 확인할 수 있습니다. 학습이 성공적으로 완료되면, 가장 낮은 테스트 손실을 기록한 시점의 모델 가중치가 <code>best_steering_model_xy.pth</code> 파일로 저장됩니다. 이 파일이 바로 TensorRT 최적화 단계에서 사용될 결과물입니다!</p>
        </div>
        <hr>
        <!-- TensorRT Optimization -->
        <div class="section" id="tensorrt-optimization">
            <h2><span class="emoji">⚡</span> 4. TensorRT 최적화: 모델, 더 빠르게!</h2>
              <p>학습된 PyTorch 모델을 Jetson Nano와 같은 엣지 디바이스에서 실시간으로 빠르게 실행하기 위해 <strong>NVIDIA TensorRT</strong>로 최적화합니다. 모델의 연산 그래프를 분석하고 압축하여 추론 속도를 높이고(<span class="emoji">🚀</span>) 메모리 사용량을 줄이는 과정입니다.</p>
             <p>여기서는 <code>torch2trt</code> 라이브러리를 사용하여 이 변환 과정을 쉽게 수행합니다.</p>
             <h3><span class="emoji">⭐</span> 4.1. 최적화 목표</h3>
             <ul>
                 <li>PyTorch 모델(<code>.pth</code>)을 고성능 TensorRT 엔진으로 변환합니다.</li>
                 <li><code>torch2trt</code> 라이브러리를 활용하여 변환 과정을 간소화합니다.</li>
                 <li><strong>FP16 (반정밀도)</strong> 모드를 사용하여 추가적인 속도 향상 및 메모리 절약을 달성합니다. (Jetson Nano는 FP16 연산을 효율적으로 처리합니다!)</li>
                 <li>최적화된 모델을 <code>best_steering_model_xy_trt.pth</code> 파일로 저장합니다.</li>
             </ul>
             <h3><span class="emoji">🔧</span> 4.2. <code>torch2trt</code> 설치 (필요시)</h3>
             <p>JetBot에 <code>torch2trt</code>가 설치되어 있지 않다면, Jetson 터미널에서 다음 명령어를 실행하여 설치합니다.</p>
            <pre><code class="language-bash">
# (Jetson Terminal에서 실행)
cd $HOME
git clone https://github.com/NVIDIA-AI-IOT/torch2trt
cd torch2trt
sudo python3 setup.py install
echo "torch2trt 설치 완료! 터미널 재시작 또는 환경 재로드가 필요할 수 있습니다."
            </code></pre>
              <h3><span class="emoji">🔄</span> 4.3. 모델 로드 및 TensorRT 변환</h3>
              <p>학습된 PyTorch 모델 가중치(<code>best_steering_model_xy.pth</code>)를 다시 로드한 후, 모델 입력과 동일한 형태의 '더미 데이터'를 만들어 <code>torch2trt</code> 함수에 전달합니다. 이 더미 데이터는 TensorRT가 모델의 연산 흐름을 파악하고 최적화하는 데 사용됩니다.</p>
            <pre><code class="language-python">
# import torchvision
# import torch
# from torch2trt import torch2trt, TRTModule # TensorRT 변환 및 로드용

# # --- 모델 구조 정의 및 가중치 로드 ---
# model = torchvision.models.resnet18(pretrained=False)
# model.fc = torch.nn.Linear(512, 2)
# model.load_state_dict(torch.load('best_steering_model_xy.pth')) # 학습된 가중치 로드
# device = torch.device('cuda')
# model = model.cuda().eval().half() # GPU 이동, 평가 모드, FP16 변환
# print("<span class='emoji'>✅</span> PyTorch 모델 로드 및 FP16 준비 완료.")

# # --- TensorRT 변환용 더미 데이터 생성 ---
# dummy_data = torch.zeros((1, 3, 224, 224)).to(device).half() # (배치, 채널, 높이, 너비)
# print("<span class='emoji'>🧪</span> 더미 입력 데이터 생성 완료.")

# # --- TensorRT 변환 실행 (시간 소요) ---
# print("<span class='emoji'>⏳</span> TensorRT 변환 시작... (몇 분 정도 걸릴 수 있습니다)")
# try:
#     model_trt = torch2trt(model, [dummy_data], fp16_mode=True) # FP16 모드 활성화
#     print("<span class='emoji'>🎉</span> TensorRT 변환 성공!")
# except Exception as e:
#     print(f"<span class='emoji'>❌</span> TensorRT 변환 중 오류 발생: {e}")
#     # 오류 발생 시 이후 코드 실행을 막기 위해 여기서 중단하거나 예외 처리 필요
#     raise e

# # --- 최적화된 모델 저장 ---
# TRT_MODEL_PATH = 'best_steering_model_xy_trt.pth'
# torch.save(model_trt.state_dict(), TRT_MODEL_PATH)
# print(f"<span class='emoji'>💾</span> 최적화된 TensorRT 모델 저장 완료: {TRT_MODEL_PATH}")
            </code></pre>
             <p>이제 훨씬 가볍고 빨라진(<span class="emoji">⚡</span>) <code>best_steering_model_xy_trt.pth</code> 모델이 준비되었습니다! 실시간 데모에서 이 모델을 사용할 것입니다.</p>
        </div>
        <hr>
        <!-- Live Demo -->
        <div class="section" id="live-demo">
            <h2><span class="emoji">🚀</span> 5. 실시간 데모: JetBot, 도로를 달리다!</h2>
            <p>드디어 프로젝트의 하이라이트! 최적화된 TensorRT 모델을 사용하여 JetBot이 실시간 카메라 영상을 보며 스스로 도로를 따라 주행하는 데모를 실행합니다. JetBot이 정말로 똑똑해졌는지 확인해볼 시간입니다! <span class="emoji">🤩</span></p>
            <h3><span class="emoji">⭐</span> 5.1. 데모 목표</h3>
            <ul>
                 <li>최적화된 TensorRT 모델(<code>best_steering_model_xy_trt.pth</code>) 로드</li>
                 <li>실시간 카메라 영상(<span class="emoji">📹</span>) 입력 받기</li>
                 <li>영상을 모델 입력 형식에 맞게 <strong>전처리(preprocess)</strong></li>
                 <li>TensorRT 모델로 목표 (x, y) 좌표 <strong>추론(inference)</strong></li>
                 <li>추론 결과를 바탕으로 JetBot의 <strong>조향(steering)</strong> 및 <strong>속도(speed)</strong> 계산 (PD 제어 활용)</li>
                 <li>계산된 값으로 JetBot 모터(<span class="emoji">⚙️</span>)를 제어하여 실시간 주행!</li>
                 <li>사용자가 주행 파라미터를 조절할 수 있는 <strong>위젯 인터페이스</strong> 제공 (<span class="emoji">🎛️</span>)</li>
            </ul>
             <h3><span class="emoji">💾</span> 5.2. TensorRT 모델 로드</h3>
            <p>이전에 저장한 최적화된 TensorRT 모델(<code>.pth</code>)을 <code>TRTModule</code>을 사용하여 로드합니다.</p>
             <pre><code class="language-python">
# import torch
# from torch2trt import TRTModule # TensorRT 모델 로드용

# # --- TensorRT 모델 로드 ---
# model_trt = TRTModule()
# TRT_MODEL_PATH = 'best_steering_model_xy_trt.pth'
# try:
#     model_trt.load_state_dict(torch.load(TRT_MODEL_PATH))
#     print(f"<span class='emoji'>✅</span> TensorRT 모델 '{TRT_MODEL_PATH}' 로드 성공!")
# except Exception as e:
#     print(f"<span class='emoji'>❌</span> TensorRT 모델 로드 실패: {e}")
#     print("   모델 파일이 올바른 경로에 있는지, 변환이 성공했는지 확인하세요.")
#     # 필요한 경우 여기서 실행 중단
#     raise e

# device = torch.device('cuda') # GPU 사용 확인
# # model_trt = model_trt.to(device) # TRTModule은 보통 자동으로 GPU 사용
            </code></pre>
             <h3><span class="emoji">🖼️➡️🧠</span> 5.3. 이미지 전처리 함수 정의</h3>
             <p>카메라 이미지를 모델이 이해할 수 있는 형식(텐서, 정규화 등)으로 변환하는 함수입니다. 학습 시 사용했던 전처리 방식과 동일해야 합니다.</p>
            <pre><code class="language-python">
# import torchvision.transforms as transforms
# import torch.nn.functional as F
# import cv2 # OpenCV for color conversion
# import PIL.Image
# import numpy as np

# # --- 이미지 전처리 함수 ---
# # ImageNet 정규화 파라미터 (GPU, FP16)
# mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()
# std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()

# def preprocess(image_bgr):
#     # BGR (OpenCV) -> RGB (PIL)
#     image_rgb = PIL.Image.fromarray(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB))
#     # PIL -> Tensor, GPU 이동, FP16 변환
#     image_tensor = transforms.functional.to_tensor(image_rgb).to(device).half()
#     # 정규화 (mean 빼고 std로 나누기)
#     image_tensor.sub_(mean[:, None, None]).div_(std[:, None, None])
#     # 배치 차원 추가 [C, H, W] -> [1, C, H, W]
#     return image_tensor[None, ...]

# print("<span class='emoji'>✅</span> 이미지 전처리 함수 'preprocess' 정의 완료.")
            </code></pre>
              <h3><span class="emoji">🤖</span><span class="emoji">📷</span> 5.4. 카메라 및 로봇 인터페이스 설정</h3>
             <p>JetBot의 카메라와 모터를 제어하기 위한 객체를 만들고, 실시간 카메라 영상을 Jupyter Lab에 표시합니다.</p>
            <pre><code class="language-python">
# from IPython.display import display, HTML
# import ipywidgets
# import traitlets
# from jetbot import Camera, bgr8_to_jpeg, Robot

# # --- 카메라 및 로봇 초기화 ---
# camera = Camera(width=224, height=224) # 학습 시와 동일 크기
# robot = Robot() # 모터 제어용
# image_widget = ipywidgets.Image(width=camera.width, height=camera.height) # 영상 표시 위젯

# # --- 카메라 영상 연결 ---
# traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)

# # --- 위젯 표시 ---
# display(ipywidgets.VBox([
#     HTML("<h2><span class='emoji'>👀</span> 실시간 카메라 영상</h2>"),
#     image_widget
# ]))
# print("<span class='emoji'>✅</span> 카메라 및 로봇 객체 준비 완료.")
            </code></pre>
              <h3><span class="emoji">🎛️</span> 5.5. 주행 제어 파라미터 슬라이더</h3>
             <p>JetBot의 주행 스타일을 미세 조정하기 위한 슬라이더입니다. 환경에 맞게 값을 조절하며 최적의 주행 성능을 찾아보세요!</p>
             <ul>
                 <li><span class="emoji">💨</span> <strong>속도 (Speed):</strong> 기본 주행 속도</li>
                 <li><span class="emoji">🧭</span> <strong>조향 게인 (Kp):</strong> 목표와의 각도 오차에 얼마나 민감하게 반응할지 (비례 제어)</li>
                 <li><span class="emoji">⚖️</span> <strong>조향 안정성 (Kd):</strong> 급격한 방향 전환을 얼마나 억제할지 (미분 제어, 진동 감소)</li>
                 <li><span class="emoji">🔧</span> <strong>조향 편향 (Bias):</strong> 로봇이 한쪽으로 쏠리는 현상 보정</li>
             </ul>
             <blockquote class="tip"><span class="emoji">💡</span> <strong>튜닝 팁:</strong> 낮은 속도에서 시작하여, Kp 값을 조절해 경로를 따라가도록 합니다. 이후 Kd 값을 조금씩 높여 진동을 잡고, 필요시 Bias로 쏠림 현상을 보정하세요.</blockquote>
             <pre><code class="language-python">
# --- 제어 파라미터 슬라이더 ---
# speed_gain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.01, value=0.18, description='<span class="emoji">💨</span> 속도', readout_format='.2f')
# steering_gain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.005, value=0.08, description='<span class="emoji">🧭</span> 조향 게인 (Kp)', readout_format='.3f')
# steering_dgain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.001, value=0.02, description='<span class="emoji">⚖️</span> 조향 안정성 (Kd)', readout_format='.3f')
# steering_bias_slider = ipywidgets.FloatSlider(min=-0.3, max=0.3, step=0.01, value=0.0, description='<span class="emoji">🔧</span> 조향 편향', readout_format='.2f')

# # --- 슬라이더 표시 ---
# display(ipywidgets.VBox([
#     HTML("<h2><span class='emoji'>⚙️</span> 주행 파라미터 조절</h2>"),
#     speed_gain_slider,
#     steering_gain_slider,
#     steering_dgain_slider,
#     steering_bias_slider
# ]))
            </code></pre>
            <h3><span class="emoji">📊</span> 5.6. 내부 상태 시각화 슬라이더 (디버깅용)</h3>
             <p>모델의 예측값과 최종 제어값을 실시간으로 확인하면 주행 상태를 이해하고 파라미터를 튜닝하는 데 도움이 됩니다.</p>
             <pre><code class="language-python">
# --- 내부 상태 시각화 슬라이더 ---
# x_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='예측 X', disabled=True, readout_format='.2f')
# steering_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='최종 조향', disabled=True, readout_format='.2f')

# # --- 시각화 슬라이더 표시 ---
# display(ipywidgets.VBox([
#     HTML("<h4><span class='emoji'>📈</span> 내부 예측 및 제어값 (참고용)</h4>"),
#     x_slider,
#     steering_slider
# ]))
            </code></pre>
            <h3><span class="emoji">🧠➡️🚗</span> 5.7. 핵심 실행 함수 (`execute`) 정의 및 연결</h3>
              <p>카메라에서 새 프레임이 들어올 때마다 호출되어, 이미지 처리 → 모델 추론 → 제어 계산 → 모터 구동까지의 전 과정을 수행하는 핵심 함수입니다. <strong>PD 제어</strong> 로직을 사용하여 목표 지점을 향해 부드럽게 조향합니다.</p>
            <pre><code class="language-python">
# import numpy as np
# import time

# # --- PD 제어용 변수 ---
# angle = 0.0
# angle_last = 0.0

# # --- 메인 실행 함수 ---
# def execute(change):
#     global angle, angle_last # 전역 변수 사용 선언

#     # 1. 새 이미지 가져오기
#     image = change['new'] # 'new' 키에 새 이미지 데이터가 들어있음
#     if image is None: return # 이미지가 없으면 중단

#     try:
#         # 2. 이미지 전처리 및 모델 추론
#         xy = model_trt(preprocess(image)).detach().float().cpu().numpy().flatten()
#         x = xy[0]
#         y = (0.5 - xy[1]) / 2.0 # y값 변환 (거리 가중치 역할)

#         # 3. 내부 상태 업데이트 (시각화용)
#         x_slider.value = float(x)

#         # 4. 제어 파라미터 가져오기
#         speed_value = speed_gain_slider.value
#         Kp = steering_gain_slider.value
#         Kd = steering_dgain_slider.value
#         bias = steering_bias_slider.value

#         # 5. 목표 각도 계산 (arctan2)
#         # y값이 매우 작거나 0에 가까워지면 angle이 불안정해질 수 있으므로 작은 값(epsilon) 더하기
#         epsilon = 1e-6
#         angle = np.arctan2(x, y + epsilon)

#         # 6. PD 제어 계산
#         pid = Kp * angle + Kd * (angle - angle_last)

#         # 7. 최종 조향 값 계산 (PD + Bias)
#         steering_value = pid + bias
#         steering_slider.value = float(steering_value) # 시각화 업데이트

#         # 8. 좌/우 모터 속도 계산 및 범위 제한 [0.0, 1.0]
#         left_motor_value = max(min(speed_value + steering_value, 1.0), 0.0)
#         right_motor_value = max(min(speed_value - steering_value, 1.0), 0.0)

#         # 9. 모터 구동
#         robot.left_motor.value = left_motor_value
#         robot.right_motor.value = right_motor_value

#         # 10. 현재 각도 저장 (다음 계산용)
#         angle_last = angle

#     except Exception as e:
#         print(f"<span class='emoji'>❌</span> 실행 중 오류 발생: {e}")
#         # 오류 발생 시 로봇 정지 등의 안전 조치 추가 가능
#         # robot.stop()

# # --- 실행 준비 및 시작 ---
# try:
#     camera.unobserve_all() # 이전 콜백 모두 해제
#     print("<span class='emoji'>🧹</span> 이전 카메라 콜백 정리 완료.")
# except:
#     pass # 처음 실행 시에는 오류 발생 안 함

# # 카메라 값 변경 시 execute 함수 호출하도록 연결
# camera.observe(execute, names='value')

# print("<span class='emoji'>🟢</span> 실시간 주행 시스템 활성화!")
# print("<span class='warning'><span class='emoji'>⚠️</span> 경고: JetBot이 움직입니다! 주변 공간을 확인하고 트랙 위에 두세요!</span>")
# print("   주행을 멈추려면 아래 '주행 중지' 코드를 실행하세요.")
            </code></pre>
             <h3><span class="emoji">🛑</span> 5.8. 주행 시작 및 비상 정지</h3>
              <p>위의 마지막 코드 셀을 실행하면 JetBot이 주행을 시작합니다! 슬라이더를 조절하며 최적의 주행을 찾아보세요.</p>
             <blockquote class="warning"><span class="emoji">🚨</span> <strong>비상 정지:</strong> 문제가 발생하거나 주행을 즉시 멈추고 싶을 때는 아래 코드 셀을 실행하세요! 카메라 연결을 끊고 모터를 정지시킵니다.</blockquote>
            <pre><code class="language-python">
# --- 주행 중지 코드 ---
# print("<span class='emoji'>⏳</span> 주행 중지 시도...")
# try:
#     camera.unobserve(execute, names='value') # 콜백 연결 해제
#     print("<span class='emoji'>🔌</span> 카메라 콜백 연결 해제 완료.")
#     time.sleep(0.1) # 잠시 대기
#     robot.stop() # 모터 정지!
#     print("<span class='emoji'>🛑</span> 로봇 모터 정지 완료.")
#     # camera.stop() # 필요시 카메라 정지
# except Exception as e:
#     print(f"<span class='emoji'>❓</span> 중지 중 오류 발생 (무시 가능): {e}")

# # 모터 값 확인 (정지되었는지)
# print(f"현재 모터 값: Left={robot.left_motor.value:.2f}, Right={robot.right_motor.value:.2f}")
            </code></pre>
        </div>
        <hr>
        <!-- Conclusion -->
        <div class="section" id="conclusion">
            <h2><span class="emoji">🏁</span> 6. 프로젝트 완료 및 향후 과제</h2>
            <p>축하드립니다! <span class="emoji">🥳</span> 여러분은 데이터 수집부터 모델 학습, TensorRT 최적화, 그리고 실시간 자율 주행 데모까지, JetBot 도로 주행 프로젝트의 전 과정을 성공적으로 완수하셨습니다!</p>
            <p>이 프로젝트를 통해 컴퓨터 비전, 딥러닝 회귀, 로봇 제어, 엣지 AI 최적화 등 다양한 기술을 직접 경험하고 통합하는 능력을 키우셨기를 바랍니다. 여기서 얻은 지식과 경험은 앞으로 더 복잡하고 흥미로운 AI 및 로봇 공학 프로젝트를 수행하는 데 훌륭한 밑거름이 될 것입니다.</p>
            <p><strong><span class="emoji">🚀</span> 다음 도전 과제들:</strong></p>
            <ul>
                <li><strong><span class="emoji">🌍</span> 다양한 환경 정복:</strong> 더 복잡한 트랙(교차로, 오르막/내리막), 야외 환경, 다양한 조명 조건에서 데이터를 추가 수집하고 모델의 강건성을 더욱 높여보세요.</li>
                <li><strong><span class="emoji">💡</span> 새로운 모델 시도:</strong> ResNet 외에도 MobileNetV2, EfficientNet-Lite 등 Jetson 환경에 더 적합한 경량 모델을 사용하여 성능과 속도를 비교 분석해보세요.</li>
                <li><strong><span class="emoji">📈</span> 정교한 제어 기법:</strong> PID 제어의 적분(I) 항을 추가하거나, 모델 예측 불확실성을 고려한 제어, 경로 계획 알고리즘(A*, RRT 등)과 결합하여 더욱 안정적이고 지능적인 주행 제어를 구현해보세요.</li>
                <li><strong><span class="emoji">🚦</span><span class="emoji">🚧</span> 기능 확장:</strong> 객체 감지 모델(신호등, 표지판, 장애물 인식)과 결합하여, 단순한 라인 트레이싱을 넘어 주변 상황을 인지하고 반응하는 진정한 자율 주행 시스템으로 발전시켜보세요!</li>
            </ul>
            <p>JetBot과 함께하는 여러분의 AI 여정을 응원합니다! <span class="emoji">🌟</span></p>
        </div>

    </div> <!-- /container -->

    <!-- Three.js 라이브러리 로딩 (Importmap 사용 - esm.sh CDN) -->
    <script type="importmap">
      {
        "imports": {
          "three": "https://esm.sh/three@0.163.0",
          "three/addons/": "https://esm.sh/three@0.163.0/examples/jsm/"
        }
      }
    </script>

    <!-- 3D 뷰어 실행 스크립트 -->
    <script type="module">
        // 필요한 모듈 임포트
        import * as THREE from 'three';
        import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
        import { OrbitControls } from 'three/addons/controls/OrbitControls.js';

        let scene, camera, renderer, controls, model;
        const container = document.getElementById('viewer-container');
        const loadingIndicator = document.getElementById('loading-indicator');
        // GitHub Raw URL은 리다이렉션 및 CORS 문제가 발생할 수 있습니다.
        // GitHub Pages나 다른 정적 호스팅을 사용하는 것이 더 안정적입니다.
        // 우선 jsdelivr CDN을 사용하도록 변경해봅니다. (GitHub Repo 기반)
        const modelUrl = 'https://cdn.jsdelivr.net/gh/hwkims/jetbot.kr@main/jetbot.glb';

        function init3DViewer() {
            try {
                // --- Scene ---
                scene = new THREE.Scene();
                scene.background = new THREE.Color(0xe9ecef);

                // --- Camera ---
                camera = new THREE.PerspectiveCamera(
                    50, container.clientWidth / container.clientHeight, 0.1, 100
                );
                camera.position.set(1.8, 1.5, 3.0);

                // --- Renderer ---
                renderer = new THREE.WebGLRenderer({ antialias: true, alpha: true });
                renderer.setSize(container.clientWidth, container.clientHeight);
                renderer.setPixelRatio(window.devicePixelRatio);
                renderer.toneMapping = THREE.ACESFilmicToneMapping;
                renderer.outputColorSpace = THREE.SRGBColorSpace;
                container.appendChild(renderer.domElement);

                // --- Controls ---
                controls = new OrbitControls(camera, renderer.domElement);
                controls.enableDamping = true;
                controls.dampingFactor = 0.08;
                controls.screenSpacePanning = false;
                controls.minDistance = 0.5;
                controls.maxDistance = 10;
                controls.target.set(0, 0.4, 0);
                controls.update();

                // --- Lighting ---
                scene.add(new THREE.AmbientLight(0xffffff, 0.8));
                const hemiLight = new THREE.HemisphereLight( 0xffffff, 0xaaaaaa, 1.0 );
                hemiLight.position.set( 0, 20, 0 );
                scene.add( hemiLight );
                const dirLight = new THREE.DirectionalLight(0xffffff, 2.5);
                dirLight.position.set(5, 10, 7);
                scene.add(dirLight);
                const dirLightBack = new THREE.DirectionalLight(0xffffff, 0.8);
                dirLightBack.position.set(-5, -5, -7);
                scene.add(dirLightBack);

                // --- GLTF Loader ---
                const loader = new GLTFLoader();
                loader.load(
                    modelUrl,
                    // Success
                    function (gltf) {
                        model = gltf.scene;
                        const box = new THREE.Box3().setFromObject(model);
                        const size = box.getSize(new THREE.Vector3());
                        const center = box.getCenter(new THREE.Vector3());
                        const maxDim = Math.max(size.x, size.y, size.z);
                        const scale = 1.8 / maxDim;

                        model.scale.set(scale, scale, scale);
                        model.position.sub(center.multiplyScalar(scale));
                        model.position.y += size.y * scale / 2; // Adjust base to near origin

                        scene.add(model);
                        loadingIndicator.style.display = 'none';
                        console.log('✅ JetBot 3D 모델 로드 및 설정 완료!');
                    },
                    // Progress
                    function (xhr) {
                         if (xhr.lengthComputable) {
                            const percentLoaded = (xhr.loaded / xhr.total) * 100;
                            loadingIndicator.textContent = `⏳ 모델 로딩 중... ${Math.round(percentLoaded)}%`;
                        } else {
                             loadingIndicator.textContent = `⏳ 모델 로딩 중...`;
                        }
                    },
                    // Error
                    function (error) {
                        console.error('❌ 3D 모델 로드 오류:', error);
                        // CORS 관련 오류 메시지 추가
                        if (error.message.includes('fetch')) {
                             loadingIndicator.innerHTML = '😭 모델 로드 실패!<br/>(CORS 또는 네트워크 문제 가능성 높음)';
                        } else {
                             loadingIndicator.innerHTML = '😭 모델 로드 실패!<br/>(파일 경로 또는 형식 확인)';
                        }
                        loadingIndicator.style.color = 'red';
                    }
                );

                // --- Resize Listener ---
                window.addEventListener('resize', onWindowResize, false);

                // --- Animation Loop ---
                animate();

            } catch (error) {
                 console.error("💥 3D 뷰어 초기화 중 오류 발생:", error);
                 loadingIndicator.innerHTML = '😭 뷰어 초기화 실패!<br/>(브라우저 호환성 또는 스크립트 오류 확인)';
                 loadingIndicator.style.color = 'red';
            }
        }

        // Resize Handler
        function onWindowResize() {
            if (!camera || !renderer || !container) return;
            camera.aspect = container.clientWidth / container.clientHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(container.clientWidth, container.clientHeight);
        }

        // Animation Loop
        function animate() {
            requestAnimationFrame(animate);
            controls.update();
            renderer.render(scene, camera);
        }

        // Initialize after DOM load
        document.addEventListener('DOMContentLoaded', init3DViewer);

    </script>

</body>
</html>
