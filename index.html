<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>🤖 JetBot 도로 주행 프로젝트 (3D 모델 포함)</title>
    <style>
        /* --- 기본 스타일 --- */
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: #f8f9fa; /* 밝은 회색 배경 */
            color: #343a40; /* 어두운 텍스트 색상 */
        }

        .container {
            max-width: 1000px;
            margin: 30px auto;
            padding: 30px;
            background-color: #ffffff; /* 흰색 배경 */
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); /* 부드러운 그림자 */
            border-radius: 8px; /* 모서리 둥글게 */
        }

        /* --- 제목 스타일 --- */
        h1, h2, h3, h4 {
            color: #0b3d91; /* 진한 파란색 (NVIDIA 느낌) */
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
        }
        h1 { text-align: center; border-bottom: 3px solid #76b900; /* NVIDIA 녹색 강조 */}
        h2 { border-bottom: 2px solid #76b900; margin-top: 2.5rem; }
        h3 { border-bottom: 1px dashed #ced4da; margin-top: 2rem; }
        h4 { color: #17a2b8; margin-top: 1.5rem; } /* 정보성 청록색 */

        /* --- 코드 블록 --- */
        pre {
            background-color: #e9ecef; /* 코드 배경 */
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            border: 1px solid #dee2e6; /* 연한 테두리 */
            font-size: 0.9em; /* 코드 폰트 약간 작게 */
        }
        code {
            font-family: 'Courier New', Courier, monospace;
        }
        pre code {
            background-color: transparent;
            padding: 0;
        }
        /* 인라인 코드 */
        :not(pre) > code {
            background-color: #e9ecef;
            padding: 0.2em 0.4em;
            margin: 0;
            font-size: 85%;
            border-radius: 3px;
        }

        /* --- 기타 요소 --- */
        strong {
            color: #28a745; /* 녹색 강조 */
        }
        ul, ol {
            margin-left: 25px;
            padding-left: 0;
        }
        li {
            margin-bottom: 12px;
        }
        a {
            color: #007bff; /* 링크 파란색 */
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        hr {
            border: 0;
            height: 1px;
            background-color: #dee2e6;
            margin: 40px 0;
        }
        .emoji { /* 이모지 스타일 (선택 사항) */
            margin-right: 5px;
            vertical-align: middle;
        }

        /* --- 3D 뷰어 컨테이너 --- */
        #viewer-container {
            width: 100%;
            height: 550px; /* 높이 증가 */
            margin: 30px 0;
            border: 1px solid #ced4da; /* 테두리 */
            position: relative;
            background-color: #f1f3f5; /* 뷰어 배경 */
            border-radius: 5px;
            overflow: hidden; /* 컨테이너 경계 유지 */
        }
        #loading-indicator {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            font-size: 1.3em;
            color: #495057;
            background-color: rgba(255, 255, 255, 0.8);
            padding: 10px 20px;
            border-radius: 5px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        /* --- 섹션 구분 --- */
        .section {
            margin-bottom: 40px;
        }

        /* --- 반응형 디자인 --- */
        @media (max-width: 768px) {
            .container {
                margin: 15px;
                padding: 20px;
            }
            #viewer-container {
                height: 450px;
            }
            h1 { font-size: 1.8em; }
            h2 { font-size: 1.5em; }
        }

    </style>
</head>
<body>
    <div class="container">

        <h1><span class="emoji">🤖</span> JetBot 도로 주행 프로젝트: 데이터 수집부터 실시간 데모까지</h1>
        <p style="text-align: center; font-size: 1.1em; color: #6c757d;">TensorRT 최적화 및インタラクティブ 3D 모델 뷰어를 포함한 전체 가이드</p>

        <hr>

        <div class="section">
            <h2><span class="emoji">✨</span> JetBot 3D 모델 뷰어</h2>
            <p>아래 뷰어에서 JetBot의 3D 모델을 직접 살펴보세요! 마우스를 사용하여 <strong>회전</strong>(왼쪽 버튼 클릭+드래그), <strong>확대/축소</strong>(휠 스크롤), <strong>이동</strong>(오른쪽 버튼 클릭+드래그 또는 Ctrl/Cmd+왼쪽 버튼 클릭+드래그)할 수 있습니다.</p>
            <div id="viewer-container">
                <div id="loading-indicator">⏳ 모델 로딩 중... 잠시만 기다려주세요!</div>
            </div>
             <p><em>💡 모델 파일: <code>jetbot.glb</code> (이 HTML 파일과 같은 폴더에 있어야 합니다.)</em></p>
        </div>

        <hr>

        <div class="section" id="project-overview">
            <h2><span class="emoji">🎯</span> 1. 프로젝트 개요</h2>
            <p>이 문서는 NVIDIA JetBot을 사용하여 카메라 이미지를 분석하고, 이를 기반으로 도로(또는 지정된 트랙)를 따라 자율적으로 주행하는 프로젝트의 전 과정을 상세히 안내합니다. 복잡해 보일 수 있지만, 차근차근 따라오시면 JetBot이 스스로 길을 찾는 놀라운 경험을 하실 수 있습니다! 😉</p>
            <p>프로젝트는 크게 네 단계로 진행됩니다:</p>
            <ol>
                <li><strong><span class="emoji">📸</span> 데이터 수집:</strong> JetBot의 눈(카메라)으로 세상을 보고, 어디로 가야 할지 알려주는 데이터를 만듭니다.</li>
                <li><strong><span class="emoji">🧠</span> 모델 학습:</strong> 수집한 데이터를 인공지능(딥러닝 모델)에게 학습시켜, 이미지 속에서 목표 지점을 스스로 찾도록 훈련시킵니다.</li>
                <li><strong><span class="emoji">⚡</span> TensorRT 최적화:</strong> 학습된 모델을 JetBot 위에서 번개처럼 빠르게 실행될 수 있도록 변환하고 최적화합니다.</li>
                <li><strong><span class="emoji">🚀</span> 실시간 데모:</strong> 최적화된 모델을 JetBot에 탑재하여, 실시간 카메라 영상을 보며 스스로 도로를 따라 주행하는 마법을 구현합니다!</li>
            </ol>
        </div>

        <hr>

        <div class="section" id="data-collection">
            <h2><span class="emoji">📸</span> 2. 데이터 수집 (Data Collection)</h2>
            <p>인공지능 모델을 학습시키려면 '교과서', 즉 데이터가 필요합니다. 이 단계에서는 JetBot이 주행할 환경(예: 직접 만든 트랙)에서 JetBot의 시선으로 이미지를 촬영하고, 각 이미지마다 "이쪽으로 가야 해!"라고 알려주는 목표 지점을 표시(라벨링)합니다.</p>

            <h3><span class="emoji">⭐</span> 2.1. 목표</h3>
            <ul>
                <li>JetBot의 카메라(<span class="emoji">📷</span>)를 사용하여 주행 경로 이미지를 캡처합니다.</li>
                <li>각 이미지에서 로봇이 나아가야 할 가장 적절한 목표 지점의 (x, y) 픽셀 좌표를 클릭하여 기록합니다.</li>
                <li><span class="emoji">💡</span> <strong>핵심:</strong> 다양한 상황(다른 위치, 각도, 조명 조건)에서 데이터를 수집해야 모델이 실제 주행 시 예상치 못한 상황에도 잘 대처할 수 있습니다 (강건성 확보!). 최소 수백 장의 이미지를 목표로 하는 것이 좋습니다.</li>
            </ul>

            <h3><span class="emoji">🗺️</span> 2.2. 데이터 라벨링 가이드: 어디를 찍어야 할까?</h3>
            <p>정확한 라벨링은 모델 성능에 매우 중요합니다. 다음 가이드라인을 따라 목표 지점을 신중하게 선택하세요:</p>
            <ol>
                <li><strong>실시간 영상 확인:</strong> Jupyter Lab에 표시되는 JetBot의 실시간 카메라 피드를 주의 깊게 관찰합니다.</li>
                <li><strong>주행 경로 상상:</strong> 로봇이 트랙 중앙 또는 특정 라인을 따라 부드럽게 나아가야 할 이상적인 경로를 머릿속으로 그립니다.</li>
                <li><strong>"당근" 지점 선택:</strong> 마치 로봇 앞에 매달린 당근처럼, 로봇이 그 지점을 향해 <strong>직진해도 경로를 벗어나지 않을 가장 먼 지점</strong>을 클릭합니다.
                    <ul>
                        <li><strong>직선 도로:</strong> 가능한 멀리, 도로가 끝나는 지평선 근처를 선택할 수 있습니다.</li>
                        <li><strong>급격한 커브:</strong> 로봇이 커브를 안전하게 돌 수 있도록, 경로 안쪽의 비교적 가까운 지점을 선택해야 합니다. 너무 먼 곳을 찍으면 로봇이 경로를 이탈할 수 있습니다! <span class="emoji">⚠️</span></li>
                    </ul>
                </li>
                <li><strong>꾸준함이 중요:</strong> 일관된 기준으로 여러 위치와 각도에서 반복적으로 데이터를 수집합니다.</li>
            </ol>

            <h3><span class="emoji">💻</span> 2.3. 구현 코드 (데이터 수집 위젯 설정)</h3>
            <p>Jupyter Lab에서 실행되는 Python 코드입니다. 필요한 라이브러리를 가져오고, 카메라를 켜고, 이미지를 클릭하여 (x, y) 좌표와 함께 저장하는 위젯을 설정합니다.</p>
            <pre><code class="language-python">
# 필요한 라이브러리들을 불러옵니다
import ipywidgets
import traitlets
from IPython.display import display
from jetbot import Robot, Camera, bgr8_to_jpeg # JetBot 제어용
from uuid import uuid1 # 고유 파일명 생성용
import os
import glob
import cv2 # 이미지 처리용
import time
from jupyter_clickable_image_widget import ClickableImageWidget # 클릭 가능한 이미지 위젯

# 데이터셋을 저장할 디렉토리 이름
DATASET_DIR = 'dataset_xy'

# 디렉토리가 없으면 새로 만듭니다 (이미 있으면 넘어갑니다)
try:
    os.makedirs(DATASET_DIR)
    print(f"'{DATASET_DIR}' 디렉토리를 생성했습니다.")
except FileExistsError:
    print(f"'{DATASET_DIR}' 디렉토리가 이미 존재합니다.")

# 카메라 객체를 생성합니다 (모델 입력 크기에 맞게 224x224 설정)
camera = Camera(width=224, height=224)

# 클릭 가능한 라이브 카메라 이미지 위젯 생성
camera_widget = ClickableImageWidget(width=camera.width, height=camera.height)
# 방금 저장한 스냅샷을 보여줄 이미지 위젯 생성
snapshot_widget = ipywidgets.Image(width=camera.width, height=camera.height)

# 카메라의 실시간 영상('value')을 camera_widget에 연결합니다.
# bgr8_to_jpeg 함수를 통해 BGR 형식의 이미지를 웹에서 볼 수 있는 JPEG 형식으로 변환합니다.
traitlets.dlink((camera, 'value'), (camera_widget, 'value'), transform=bgr8_to_jpeg)

# 저장된 이미지 개수를 보여줄 텍스트 위젯 생성
count_widget = ipywidgets.IntText(description='저장된 이미지:', value=0, disabled=True)

# 프로그램 시작 시 저장된 이미지 개수를 세어서 업데이트합니다
count_widget.value = len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))

# camera_widget (라이브 영상)를 클릭했을 때 실행될 함수 정의
def save_snapshot(_, content, msg):
    # 클릭 이벤트일 때만 처리
    if content['event'] == 'click':
        data = content['eventData']
        x = data['offsetX'] # 클릭된 x 좌표 (이미지 내)
        y = data['offsetY'] # 클릭된 y 좌표 (이미지 내)

        # 고유한 파일명을 만듭니다: xy_x좌표_y좌표_고유ID.jpg
        uuid_str = str(uuid1())
        filename = f'xy_{x:03d}_{y:03d}_{uuid_str}.jpg' # x,y 좌표는 3자리 숫자로 포맷팅
        image_path = os.path.join(DATASET_DIR, filename)

        # 현재 카메라 위젯의 이미지(JPEG 형식)를 파일로 저장합니다
        with open(image_path, 'wb') as f:
            f.write(camera_widget.value)

        # 방금 저장한 스냅샷에 녹색 원을 그려서 보여줍니다
        snapshot = camera.value.copy() # 현재 카메라 프레임(BGR) 복사
        cv2.circle(snapshot, (x, y), 8, (0, 255, 0), 3) # (x, y)에 녹색 원 그리기
        snapshot_widget.value = bgr8_to_jpeg(snapshot) # JPEG로 변환하여 위젯에 표시

        # 이미지 개수 텍스트 위젯 업데이트
        count_widget.value = len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))

# camera_widget에서 메시지(클릭 포함)가 발생하면 save_snapshot 함수를 호출하도록 등록
camera_widget.on_msg(save_snapshot)

# 위젯들을 보기 좋게 배치합니다 (수직/수평 박스 사용)
data_collection_widget = ipywidgets.VBox([
    ipywidgets.HTML("<h3><span class='emoji'>🖱️</span> 이미지 클릭하여 데이터 저장</h3>"),
    ipywidgets.HBox([camera_widget, snapshot_widget]), # 라이브 영상과 스냅샷을 나란히 표시
    count_widget # 저장된 이미지 개수 표시
])

# 최종적으로 구성된 위젯을 화면에 표시합니다
display(data_collection_widget)
print("카메라 준비 완료! 라이브 이미지를 클릭하여 데이터를 저장하세요.")
            </code></pre>

            <h3><span class="emoji">📂</span> 2.4. 데이터 확인</h3>
            <p>데이터 수집을 마치면, Jupyter Lab의 파일 탐색기에서 <code>dataset_xy</code> 폴더를 확인하세요. 그 안에 <code>xy_<x>_<y>_<uuid>.jpg</code> 형식의 파일들이 가득 차 있을 것입니다. 파일명에 포함된 <code><x></code>와 <code><y></code> 좌표가 바로 우리가 힘들게 찍은 라벨 정보이며, 다음 단계에서 모델 학습에 사용됩니다.</p>
        </div>

        <hr>

        <div class="section" id="model-training">
            <h2><span class="emoji">🧠</span> 3. 모델 학습 (Train Model)</h2>
            <p>이제 수집한 데이터를 이용해 JetBot의 '뇌'에 해당하는 딥러닝 모델을 학습시킬 차례입니다. 목표는 이미지를 입력받았을 때, 우리가 라벨링했던 목표 지점의 (x, y) 좌표를 정확하게 예측하도록 만드는 것입니다. 여기서는 <strong>회귀(Regression)</strong> 문제를 푸는 것입니다.</p>

            <h3><span class="emoji">⭐</span> 3.1. 목표</h3>
            <ul>
                <li>입력 이미지(224x224)로부터 목표 지점의 정규화된 (x, y) 좌표 [-1, 1] 범위를 예측하는 회귀 모델을 학습합니다.</li>
                <li>강력한 이미지 인식 성능으로 유명한 <strong>ResNet-18</strong> 아키텍처를 사용합니다.</li>
                <li><strong>전이 학습(Transfer Learning)</strong> 기법을 활용합니다: 이미 수백만 장의 이미지로 학습된 ResNet-18 모델을 가져와, 우리의 도로 주행 문제에 맞게 마지막 부분만 살짝 수정하여 학습합니다. 이렇게 하면 적은 데이터로도 좋은 성능을 얻을 수 있습니다! <span class="emoji">🎓</span></li>
                <li>PyTorch 딥러닝 프레임워크를 사용합니다.</li>
            </ul>

            <h3><span class="emoji">🛠️</span> 3.2. 데이터셋 준비</h3>

            <h4>3.2.1. 커스텀 데이터셋 클래스 (<code>XYDataset</code>)</h4>
            <p>PyTorch의 <code>Dataset</code> 클래스를 상속받아 우리 데이터에 맞는 커스텀 데이터셋 클래스를 만듭니다. 이 클래스는 다음 역할을 합니다:</p>
            <ul>
                <li>지정된 디렉토리에서 이미지 파일 목록을 가져옵니다.</li>
                <li>이미지 파일을 로드합니다.</li>
                <li>파일명에서 x, y 좌표를 추출하고, [-1, 1] 범위로 <strong>정규화</strong>합니다. 모델이 학습하기 쉬운 형태로 만들어주는 과정입니다.</li>
                <li><strong>데이터 증강(Data Augmentation)</strong>을 적용합니다 (선택 사항):
                    <ul>
                        <li><code>ColorJitter</code>: 이미지의 밝기, 대비, 채도 등을 무작위로 약간씩 변경하여 모델이 다양한 조명 조건에 강해지도록 합니다.</li>
                        <li><code>Random Horizontal Flip</code>: 이미지를 무작위로 좌우 반전시킵니다. 만약 트랙이 대칭적이라면, 데이터 양을 두 배로 늘리는 효과를 줄 수 있습니다. (좌우 반전 시 x 좌표의 부호도 바꿔줘야 합니다!)</li>
                    </ul>
                </li>
                <li>이미지 크기를 모델 입력에 맞게 224x224로 조절합니다.</li>
                <li>이미지를 PyTorch 텐서(Tensor) 형식으로 변환하고, ImageNet 데이터셋의 평균과 표준편차로 <strong>정규화</strong>합니다. (사전 학습된 모델을 사용하기 때문에 동일한 정규화 방식을 따르는 것이 중요합니다.)</li>
            </ul>
            <pre><code class="language-python">
import torch
import torch.optim as optim
import torch.nn.functional as F
import torchvision
import torchvision.datasets as datasets
import torchvision.models as models
import torchvision.transforms as transforms
import glob
import PIL.Image
import os
import numpy as np

# 파일 경로에서 x 좌표를 추출하고 [-1, 1] 범위로 정규화하는 함수
def get_x(path, width):
    """이미지 파일명에서 x 값을 추출합니다."""
    # 파일명 예시: 'xy_112_150_uuid.jpg'
    x_pixel = float(int(os.path.basename(path).split('_')[1]))
    # 픽셀 좌표를 [-1, 1] 범위로 변환 (이미지 중앙이 0)
    return (x_pixel - width / 2) / (width / 2)

# 파일 경로에서 y 좌표를 추출하고 [-1, 1] 범위로 정규화하는 함수
def get_y(path, height):
    """이미지 파일명에서 y 값을 추출합니다."""
    y_pixel = float(int(os.path.basename(path).split('_')[2]))
    # 픽셀 좌표를 [-1, 1] 범위로 변환 (이미지 중앙이 0)
    # 참고: 이미지 좌표계는 위쪽이 0, 아래쪽이 height입니다.
    # 따라서 정규화된 y값은 위쪽이 -1, 아래쪽이 1에 가까워집니다.
    return (y_pixel - height / 2) / (height / 2)

# PyTorch Dataset 클래스를 상속받아 커스텀 데이터셋 정의
class XYDataset(torch.utils.data.Dataset):
    def __init__(self, directory, random_hflips=False):
        self.directory = directory
        self.random_hflips = random_hflips # 좌우 반전 증강 사용 여부
        # 디렉토리 내 모든 .jpg 파일 경로를 가져옵니다.
        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))
        # 데이터 증강: 색상 변형 설정 (밝기, 대비, 채도, 색조)
        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)

        # 데이터 전처리를 위한 Transform 정의
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)), # 이미지 크기 224x224로 조정
            transforms.ToTensor(), # 이미지를 PyTorch Tensor로 변환 ([0, 1] 범위)
            # ImageNet 평균/표준편차로 정규화
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])

    def __len__(self):
        # 데이터셋의 총 이미지 개수 반환
        return len(self.image_paths)

    def __getitem__(self, idx):
        # 주어진 인덱스(idx)에 해당하는 이미지 경로 가져오기
        image_path = self.image_paths[idx]

        # PIL 라이브러리를 사용하여 이미지 열기 (RGB 형식)
        image = PIL.Image.open(image_path).convert('RGB')
        width, height = image.size

        # 파일명에서 x, y 좌표 추출 및 정규화
        x = float(get_x(image_path, width))
        y = float(get_y(image_path, height))

        # 데이터 증강: Random Horizontal Flip (좌우 반전)
        if self.random_hflips and np.random.rand() > 0.5:
            image = transforms.functional.hflip(image)
            x = -x # 좌우 반전 시 x 좌표 부호 변경

        # 데이터 증강: Color Jitter (색상 변형)
        image = self.color_jitter(image)

        # 정의된 Transform 적용 (크기 조정, Tensor 변환, 정규화)
        image = self.transform(image)

        # 이미지 텐서와 좌표 텐서([x, y]) 반환
        return image, torch.tensor([x, y]).float()

# 데이터셋 인스턴스 생성 (좌우 반전은 사용하지 않음)
dataset = XYDataset(DATASET_DIR, random_hflips=False)
print(f"데이터셋 로드 완료! 총 이미지 개수: {len(dataset)}")
            </code></pre>

            <h4>3.2.2. 데이터 분할 및 로더 생성</h4>
            <p>전체 데이터셋을 <strong>훈련(Train) 세트</strong>와 <strong>테스트(Test) 세트</strong>로 나눕니다. 보통 80:20 또는 90:10 비율을 사용합니다. 모델은 훈련 세트로 학습하고, 테스트 세트로는 학습 중에 모델 성능을 객관적으로 평가하는 데 사용합니다.</p>
            <p><code>DataLoader</code>는 데이터를 미니배치(minibatch) 단위로 묶어주고, 데이터를 섞어주며(shuffling), 병렬 처리(multi-processing)를 가능하게 하여 학습 효율을 높여줍니다.</p>
             <pre><code class="language-python">
# 테스트 데이터 비율 설정 (예: 10%)
test_percent = 0.1
num_total = len(dataset)
num_test = int(test_percent * num_total)
num_train = num_total - num_test

print(f"데이터 분할: 훈련 데이터 {num_train}개, 테스트 데이터 {num_test}개")

# 데이터셋을 훈련 세트와 테스트 세트로 무작위 분할
train_dataset, test_dataset = torch.utils.data.random_split(dataset, [num_train, num_test])

# 데이터 로더(DataLoader) 생성
BATCH_SIZE = 8 # 미니배치 크기 (Jetson Nano의 GPU 메모리에 따라 조절 필요)
# 훈련 데이터 로더 (데이터 섞기 활성화)
train_loader = torch.utils.data.DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=0 # Jetson에서는 0으로 설정하는 것이 안정적일 수 있음
)
# 테스트 데이터 로더 (데이터 섞기 비활성화)
test_loader = torch.utils.data.DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=0
)
print(f"데이터 로더 생성 완료! 배치 크기: {BATCH_SIZE}")
            </code></pre>

            <h3><span class="emoji">🏗️</span> 3.3. 모델 정의 (ResNet-18)</h3>
            <p>사전 학습된 ResNet-18 모델을 불러옵니다. ResNet-18은 이미지 분류 문제에서 뛰어난 성능을 보인 모델 구조입니다. 우리는 이 모델의 마지막 출력 레이어(Fully Connected Layer)만 교체하여, 이미지 분류(예: 1000개 클래스) 대신 우리의 목표인 (x, y) 좌표 2개를 출력하도록 만듭니다.</p>
            <pre><code class="language-python">
# ImageNet 데이터로 사전 학습된 ResNet-18 모델 불러오기
model = models.resnet18(pretrained=True)

# ResNet-18의 마지막 Fully Connected Layer(fc) 확인 (기본 출력: 1000개)
# print(model.fc)

# 마지막 FC 레이어를 새로운 레이어로 교체합니다.
# 입력 특징(in_features)은 ResNet-18의 마지막 특징 맵 크기(512)와 동일하게 유지하고,
# 출력 특징(out_features)은 우리가 예측하려는 값의 개수(x, y 좌표 2개)로 설정합니다.
num_output_features = 2
model.fc = torch.nn.Linear(model.fc.in_features, num_output_features)

# 모델을 GPU로 이동시킵니다 (Jetson Nano에는 GPU가 있습니다!)
device = torch.device('cuda')
model = model.to(device)

print("ResNet-18 모델 정의 및 출력 레이어 수정 완료!")
# print(model) # 모델 구조 확인 (선택 사항)
            </code></pre>

            <h3><span class="emoji">🏋️</span> 3.4. 모델 학습</h3>
            <p>이제 모델을 본격적으로 학습시킬 시간입니다! 학습 과정을 요약하면 다음과 같습니다:</p>
            <ol>
                <li><strong>에포크(Epoch) 반복:</strong> 전체 훈련 데이터를 모두 사용하는 과정을 1 에포크라고 하며, 이 과정을 여러 번 반복합니다(예: 30~70 에포크).</li>
                <li><strong>미니배치(Minibatch) 학습:</strong> 각 에포크 내에서 훈련 데이터를 미니배치 단위로 나누어 처리합니다.
                    <ul>
                        <li>모델이 미니배치 이미지를 보고 (x, y) 좌표를 예측합니다.</li>
                        <li>예측값과 실제 라벨(정답 좌표) 사이의 오차(손실, Loss)를 계산합니다. 회귀 문제에서는 주로 <strong>평균 제곱 오차(Mean Squared Error, MSE)</strong>를 사용합니다.</li>
                        <li>계산된 손실을 바탕으로 모델의 가중치(파라미터)를 어느 방향으로 얼마나 업데이트해야 오차를 줄일 수 있는지 계산합니다 (역전파, Backpropagation).</li>
                        <li><strong>옵티마이저(Optimizer)</strong> (예: Adam)를 사용하여 계산된 방향으로 모델 가중치를 업데이트합니다.</li>
                    </ul>
                </li>
                <li><strong>성능 평가:</strong> 각 에포크가 끝날 때마다 테스트 세트를 사용하여 현재 모델의 성능(테스트 손실)을 평가합니다. 이 평가는 모델이 학습 데이터에만 과적합(overfitting)되지 않았는지 확인하는 데 중요합니다.</li>
                <li><strong>최고 모델 저장:</strong> 테스트 손실이 이전에 기록된 최고 성능(가장 낮은 손실)보다 좋을 때마다, 현재 모델의 가중치를 파일(<code>best_steering_model_xy.pth</code>)로 저장합니다.</li>
            </ol>
            <pre><code class="language-python">
# 학습 파라미터 설정
NUM_EPOCHS = 70 # 총 학습 에포크 수 (데이터 양과 모델 복잡도에 따라 조절)
BEST_MODEL_PATH = 'best_steering_model_xy.pth' # 최고 성능 모델 가중치 저장 경로
best_loss = float('inf') # 최고 성능(최저 테스트 손실) 기록 변수 (초기값: 무한대)

# 옵티마이저 정의 (Adam 사용, 학습률 기본값 사용)
# model.parameters()는 모델 내에서 학습 가능한 모든 가중치를 옵티마이저에게 알려줍니다.
optimizer = optim.Adam(model.parameters())

print(f"모델 학습 시작! 총 에포크: {NUM_EPOCHS}")

# 학습 루프 (지정된 에포크 수만큼 반복)
for epoch in range(NUM_EPOCHS):
    # --- 훈련 단계 ---
    model.train() # 모델을 훈련 모드로 설정 (Dropout 등 활성화)
    train_loss = 0.0 # 현재 에포크의 훈련 손실 누적 변수

    # 훈련 데이터 로더에서 미니배치 단위로 데이터 가져오기
    for images, labels in train_loader:
        # 데이터를 GPU로 이동
        images = images.to(device)
        labels = labels.to(device)

        # 옵티마이저의 기울기(gradient) 초기화
        optimizer.zero_grad()

        # 모델 예측 수행
        outputs = model(images)

        # 손실 계산 (MSE Loss 사용)
        loss = F.mse_loss(outputs, labels)

        # 역전파 수행 (손실에 대한 각 가중치의 기울기 계산)
        loss.backward()

        # 옵티마이저 스텝 (계산된 기울기를 바탕으로 가중치 업데이트)
        optimizer.step()

        # 현재 미니배치의 손실을 누적 (loss.item()은 텐서에서 숫자 값만 추출)
        train_loss += loss.item()

    # 평균 훈련 손실 계산
    avg_train_loss = train_loss / len(train_loader)

    # --- 평가 단계 ---
    model.eval() # 모델을 평가 모드로 설정 (Dropout 등 비활성화)
    test_loss = 0.0 # 현재 에포크의 테스트 손실 누적 변수
    with torch.no_grad(): # 평가 시에는 기울기 계산이 필요 없으므로 비활성화 (메모리 절약, 속도 향상)
        # 테스트 데이터 로더에서 미니배치 단위로 데이터 가져오기
        for images, labels in test_loader:
            # 데이터를 GPU로 이동
            images = images.to(device)
            labels = labels.to(device)

            # 모델 예측 수행
            outputs = model(images)

            # 손실 계산
            loss = F.mse_loss(outputs, labels)

            # 현재 미니배치의 손실 누적
            test_loss += loss.item()

    # 평균 테스트 손실 계산
    avg_test_loss = test_loss / len(test_loader)

    # 에포크 결과 출력
    print(f'에포크 [{epoch+1}/{NUM_EPOCHS}], 훈련 손실: {avg_train_loss:.6f}, 테스트 손실: {avg_test_loss:.6f}')

    # 최고 성능 모델 저장 (테스트 손실 기준)
    if avg_test_loss < best_loss:
        print(f'    🚀 테스트 손실 개선 ({best_loss:.6f} -> {avg_test_loss:.6f}). 모델 저장 중...')
        # 모델의 state_dict (학습 가능한 파라미터들) 저장
        torch.save(model.state_dict(), BEST_MODEL_PATH)
        best_loss = avg_test_loss # 최고 손실 업데이트

print(f"🎉 모델 학습 완료! 최종 최고 테스트 손실: {best_loss:.6f}")
print(f"최고 성능 모델이 '{BEST_MODEL_PATH}' 파일로 저장되었습니다.")
            </code></pre>

            <h3><span class="emoji">📊</span> 3.5. 학습 결과 확인</h3>
            <p>학습이 진행되면서 각 에포크마다 훈련 손실과 테스트 손실이 출력됩니다. 이상적으로는 두 손실 모두 꾸준히 감소해야 합니다. 훈련 손실만 계속 줄어들고 테스트 손실은 증가하거나 정체된다면, 모델이 훈련 데이터에 <strong>과적합(Overfitting)</strong>되고 있다는 신호일 수 있습니다. <span class="emoji">🤔</span></p>
            <p>학습이 완료되면, 테스트 손실이 가장 낮았던 시점의 모델 가중치가 <code>best_steering_model_xy.pth</code> 파일로 저장됩니다. 이 파일이 바로 우리가 다음 단계에서 사용할 소중한 결과물입니다!</p>
        </div>

        <hr>

        <div class="section" id="tensorrt-optimization">
            <h2><span class="emoji">⚡</span> 4. TensorRT 최적화 (Build TensorRT model)</h2>
            <p>PyTorch로 학습한 모델도 훌륭하지만, Jetson Nano와 같은 엣지 디바이스에서 실시간으로 빠르게 추론(예측)하기에는 약간 무거울 수 있습니다. <strong>NVIDIA TensorRT</strong>는 딥러닝 모델을 최적화하여 추론 속도를 높이고 리소스 사용량을 줄여주는 강력한 도구입니다. <span class="emoji">🚀</span></p>
            <p>이 단계에서는 <code>torch2trt</code> 라이브러리를 사용하여, 우리가 학습한 PyTorch 모델(<code>best_steering_model_xy.pth</code>)을 TensorRT 엔진으로 변환하고 최적화합니다.</p>

            <h3><span class="emoji">⭐</span> 4.1. 목표</h3>
            <ul>
                <li>학습된 PyTorch 모델을 TensorRT 형식으로 변환하여 추론 성능을 극대화합니다.</li>
                <li><code>torch2trt</code> 라이브러리를 활용하여 PyTorch 모델에서 TensorRT 모델로 쉽게 변환합니다.</li>
                <li>최적화된 모델을 <code>best_steering_model_xy_trt.pth</code> 파일로 저장하여 실시간 데모에서 사용합니다.</li>
                <li><strong>FP16 (반정밀도 부동소수점)</strong> 모드를 사용하여 추가적인 속도 향상 및 메모리 절약을 꾀합니다. (Jetson Nano는 FP16 연산을 지원합니다!)</li>
            </ul>

            <h3><span class="emoji">🔧</span> 4.2. <code>torch2trt</code> 설치 (필요시)</h3>
            <p>만약 JetBot에 <code>torch2trt</code> 라이브러리가 설치되어 있지 않다면, Jetson 터미널에서 다음 명령어를 실행하여 설치해야 합니다.</p>
            <pre><code class="language-bash">
# 사용자 홈 디렉토리로 이동
cd $HOME
# torch2trt 저장소 복제
git clone https://github.com/NVIDIA-AI-IOT/torch2trt
# 복제된 디렉토리로 이동
cd torch2trt
# 설치 실행 (관리자 권한 필요)
sudo python3 setup.py install
# 설치 후 터미널 재시작 또는 환경 재로드 필요할 수 있음
echo "torch2trt 설치 완료!"
            </code></pre>

            <h3><span class="emoji">🔄</span> 4.3. 모델 로드 및 변환</h3>
            <p>먼저 학습된 PyTorch 모델(<code>best_steering_model_xy.pth</code>)을 다시 로드합니다. 그런 다음, 모델과 동일한 입력 형태를 가진 더미(dummy) 데이터를 생성하여 <code>torch2trt</code> 함수에 전달합니다. <code>torch2trt</code>는 이 더미 데이터를 이용해 모델의 연산 그래프를 분석하고 TensorRT 엔진으로 최적화합니다.</p>
             <pre><code class="language-python">
import torchvision
import torch
from torch2trt import torch2trt, TRTModule # torch2trt 관련 함수 및 클래스 임포트

# 모델 구조 정의 (학습 시와 동일하게)
model = torchvision.models.resnet18(pretrained=False) # 사전 학습 가중치는 여기서 로드 안 함
model.fc = torch.nn.Linear(512, 2) # 출력 레이어 수정

# 학습된 모델 가중치 로드 (best_steering_model_xy.pth)
model.load_state_dict(torch.load('best_steering_model_xy.pth'))

# 모델을 GPU로 이동하고, 평가(eval) 모드로 설정하고, FP16(half) 정밀도로 변경
device = torch.device('cuda')
model = model.cuda().eval().half()

print("PyTorch 모델 로드 및 FP16 변환 완료.")

# TensorRT 변환을 위한 더미 입력 데이터 생성
# 모델 입력과 동일한 형태: (배치 크기=1, 채널=3, 높이=224, 너비=224)
# 데이터 타입도 모델과 동일하게 FP16(half)으로 설정
dummy_data = torch.zeros((1, 3, 224, 224)).to(device).half()

print("더미 입력 데이터 생성 완료. TensorRT 변환을 시작합니다...")
# 이 과정은 몇 분 정도 소요될 수 있습니다! 커피 한 잔의 여유를... ☕

# torch2trt 함수를 사용하여 모델 변환
# model: 변환할 PyTorch 모델
# [dummy_data]: 모델 입력 형태를 알려주기 위한 더미 데이터 리스트
# fp16_mode=True: FP16 정밀도로 최적화 활성화
model_trt = torch2trt(model, [dummy_data], fp16_mode=True)

print("TensorRT 모델 변환 완료!")

# 최적화된 TensorRT 모델의 state_dict 저장
TRT_MODEL_PATH = 'best_steering_model_xy_trt.pth'
torch.save(model_trt.state_dict(), TRT_MODEL_PATH)

print(f"최적화된 TensorRT 모델이 '{TRT_MODEL_PATH}' 파일로 저장되었습니다.")
            </code></pre>
            <p>이제 훨씬 가볍고 빨라진 <code>best_steering_model_xy_trt.pth</code> 모델이 준비되었습니다! 이 모델을 가지고 실시간 도로 주행 데모를 진행할 수 있습니다.</p>
        </div>

        <hr>

        <div class="section" id="live-demo">
             <h2><span class="emoji">🚀</span> 5. 실시간 데모 (Live demo - TensorRT)</h2>
             <p>드디어 마지막 단계입니다! 최적화된 TensorRT 모델을 JetBot에 탑재하여 실시간 카메라 영상을 보면서 스스로 도로를 따라 주행하는 데모를 실행합니다. <span class="emoji">🎉</span></p>

             <h3><span class="emoji">⭐</span> 5.1. 목표</h3>
             <ul>
                 <li>최적화된 TensorRT 모델(<code>best_steering_model_xy_trt.pth</code>)을 로드합니다.</li>
                 <li>JetBot의 카메라로부터 실시간 영상을 받아옵니다.</li>
                 <li>영상을 모델이 이해할 수 있는 형태로 <strong>전처리(preprocess)</strong>합니다.</li>
                 <li>TensorRT 모델을 사용하여 영상으로부터 목표 (x, y) 좌표를 <strong>추론(inference)</strong>합니다.</li>
                 <li>추론된 (x, y) 좌표를 바탕으로 JetBot의 <strong>조향(steering)</strong> 각도와 <strong>속도(speed)</strong>를 계산합니다.</li>
                 <li>계산된 값으로 JetBot의 모터를 제어하여 실제로 주행합니다!</li>
                 <li>사용자가 주행 파라미터(속도, 조향 게인 등)를 실시간으로 조절할 수 있는 위젯 인터페이스를 제공하여 주행 성능을 미세 조정합니다.</li>
             </ul>

             <h3><span class="emoji">💾</span> 5.2. TensorRT 모델 로드</h3>
             <p><code>TRTModule</code> 클래스를 사용하여 저장된 TensorRT 모델(<code>.pth</code> 파일)을 불러옵니다. 이 모델은 이미 최적화된 상태이므로 별도의 변환 과정 없이 바로 사용할 수 있습니다.</p>
             <pre><code class="language-python">
import torch
from torch2trt import TRTModule # TensorRT 모델 로드용 클래스

# TRTModule 인스턴스 생성
model_trt = TRTModule()

# 저장된 TensorRT 모델 가중치 로드
TRT_MODEL_PATH = 'best_steering_model_xy_trt.pth'
model_trt.load_state_dict(torch.load(TRT_MODEL_PATH))

# GPU 설정 (필수는 아니지만 명시적으로)
device = torch.device('cuda')
# model_trt = model_trt.to(device) # TRTModule은 로드 시 자동으로 GPU에 올라갈 수 있음

print(f"TensorRT 모델 '{TRT_MODEL_PATH}' 로드 완료.")
            </code></pre>

             <h3><span class="emoji">🖼️➡️🧠</span> 5.3. 전처리 함수 정의</h3>
             <p>카메라에서 얻은 이미지(NumPy 배열, HWC, BGR, [0, 255] 범위)는 모델이 바로 이해할 수 없습니다. 모델 학습 시 사용했던 것과 동일한 방식으로 이미지를 변환(전처리)해줘야 합니다.</p>
             <ol>
                 <li>NumPy 배열 (BGR) -> PIL 이미지 (RGB) 변환</li>
                 <li>PIL 이미지 -> PyTorch 텐서 (CHW, [0, 1] 범위) 변환</li>
                 <li>텐서를 GPU로 이동시키고 FP16(half) 타입으로 변경</li>
                 <li>ImageNet 평균/표준편차로 정규화</li>
                 <li>배치(batch) 차원 추가 (모델 입력 형태 [1, C, H, W] 맞추기)</li>
             </ol>
             <pre><code class="language-python">
import torchvision.transforms as transforms
import torch.nn.functional as F
import cv2 # OpenCV for BGR to RGB conversion
import PIL.Image
import numpy as np

# ImageNet 정규화 파라미터 (GPU에 FP16(half) 타입으로 미리 올려둠)
mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()
std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()

def preprocess(image_bgr):
    # 1. NumPy 배열(BGR)을 PIL 이미지(RGB)로 변환
    image_rgb = PIL.Image.fromarray(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB))

    # 2. PIL 이미지를 Tensor로 변환 (크기 조정은 카메라에서 이미 수행했다고 가정)
    #    ToTensor()는 [0, 255] PIL 이미지를 [0, 1] FloatTensor (CHW)로 변환
    image_tensor = transforms.functional.to_tensor(image_rgb)

    # 3. 텐서를 GPU로 이동시키고 FP16 타입으로 변경
    image_tensor = image_tensor.to(device).half()

    # 4. 정규화 수행 (inplace 연산으로 메모리 효율성 증대)
    #    mean을 빼고 std로 나눔. 브로드캐스팅을 위해 차원 확장([:, None, None]) 사용
    image_tensor.sub_(mean[:, None, None]).div_(std[:, None, None])

    # 5. 배치 차원 추가 (맨 앞에 차원 추가: [C, H, W] -> [1, C, H, W])
    return image_tensor[None, ...]

print("이미지 전처리 함수 'preprocess' 정의 완료.")
            </code></pre>

             <h3><span class="emoji">🤖</span><span class="emoji">📷</span> 5.4. 카메라 및 로봇 인터페이스 설정</h3>
             <p>JetBot의 카메라와 모터를 제어하기 위한 객체를 생성하고, 실시간 카메라 영상을 Jupyter Lab에서 볼 수 있도록 위젯을 설정합니다.</p>
             <pre><code class="language-python">
from IPython.display import display
import ipywidgets
import traitlets
from jetbot import Camera, bgr8_to_jpeg, Robot

# 카메라 초기화 (데이터 수집/학습 시와 동일한 224x224 크기 사용)
camera = Camera(width=224, height=224)

# 실시간 카메라 피드를 보여줄 이미지 위젯
image_widget = ipywidgets.Image(width=camera.width, height=camera.height)

# 카메라의 영상('value')을 이미지 위젯에 연결 (BGR -> JPEG 변환 필요)
traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)

# JetBot 로봇 객체 초기화 (모터 제어용)
robot = Robot()

# 위젯들을 보기 좋게 배치하고 화면에 표시
display(ipywidgets.VBox([
    ipywidgets.HTML("<h3><span class='emoji'>👀</span> 실시간 카메라 영상</h3>"),
    image_widget
]))

print("카메라 및 로봇 객체 초기화 완료.")
            </code></pre>

             <h3><span class="emoji">🎛️</span> 5.5. 제어 파라미터 슬라이더</h3>
             <p>JetBot의 주행 성능을 실시간으로 미세 조정할 수 있는 슬라이더를 만듭니다. 최적의 값은 트랙 환경, JetBot의 상태(배터리, 모터 편차 등)에 따라 달라질 수 있으므로 직접 조절하며 찾아야 합니다.</p>
             <ul>
                 <li><code>speed_gain_slider</code>: <strong>기본 속도</strong>를 조절합니다. 값이 높을수록 빨라집니다. (0.0 ~ 1.0)</li>
                 <li><code>steering_gain_slider (Kp)</code>: <strong>조향 민감도 (비례 제어)</strong>를 조절합니다. 목표 지점과의 각도 차이(오차)에 비례하여 조향 강도를 결정합니다. 값이 높으면 작은 오차에도 크게 반응하여 민첩해지지만, 너무 높으면 좌우로 심하게 흔들릴(oscillate) 수 있습니다. (0.0 ~ 1.0)</li>
                 <li><code>steering_dgain_slider (Kd)</code>: <strong>조향 안정성 (미분 제어)</strong>를 조절합니다. 각도 오차의 변화율에 비례하여 조향 강도를 조절합니다. 급격한 방향 전환을 억제하고 진동을 줄여 주행을 부드럽게 만드는 데 도움을 줍니다. (0.0 ~ 0.5)</li>
                 <li><code>steering_bias_slider</code>: <strong>조향 편향</strong>을 조절합니다. 로봇이 특정 방향(왼쪽 또는 오른쪽)으로 계속 쏠리는 경향이 있을 때 이를 보정합니다. 모터 자체의 미세한 차이나 카메라 장착 위치의 비대칭성 때문에 필요할 수 있습니다. (-0.3 ~ 0.3)</li>
             </ul>
             <p>💡 <strong>팁:</strong> 처음에는 낮은 속도(<code>speed_gain</code>)로 설정하고, <code>steering_gain</code>을 조절하여 안정적으로 따라가는 값을 찾습니다. 그 후, 필요에 따라 <code>steering_dgain</code>과 <code>steering_bias</code>를 미세 조정하세요.</p>
            <pre><code class="language-python">
# --- 제어 파라미터 슬라이더 생성 ---
# 기본 속도 조절
speed_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.18, description='속도 (Speed)')
# 조향 민감도 (P 제어 게인)
steering_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.15, description='조향 게인 (Kp)')
# 조향 안정성 (D 제어 게인)
steering_dgain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.001, value=0.03, description='조향 안정성 (Kd)')
# 조향 편향 보정
steering_bias_slider = ipywidgets.FloatSlider(min=-0.3, max=0.3, step=0.01, value=0.0, description='조향 편향 (Bias)')

# 슬라이더들을 화면에 표시
display(ipywidgets.VBox([
    ipywidgets.HTML("<h3><span class='emoji'>⚙️</span> 주행 파라미터 조절</h3>"),
    speed_gain_slider,
    steering_gain_slider,
    steering_dgain_slider,
    steering_bias_slider
]))
            </code></pre>

             <h3><span class="emoji">📊</span> 5.6. 예측값 및 조향 시각화 슬라이더 (선택 사항)</h3>
             <p>모델이 예측하는 내부 값(x, y)과 최종 계산된 조향 값을 실시간으로 확인하면 디버깅과 파라미터 튜닝에 도움이 됩니다.</p>
             <pre><code class="language-python">
# --- 내부 상태 시각화 슬라이더 ---
# 예측된 x 좌표 (-1 ~ 1) 표시 (읽기 전용)
x_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='예측 X', disabled=True, readout_format='.2f')
# 주행 로직에 사용되는 변환된 y 값 (0 ~ 0.5) 표시 (읽기 전용)
# y_slider = ipywidgets.FloatSlider(min=0, max=0.5, orientation='vertical', description='예측 Y (변환값)', disabled=True, readout_format='.2f') # y값 시각화는 덜 직관적일 수 있음
# 최종 계산된 조향 값 (-1 ~ 1) 표시 (읽기 전용)
steering_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='최종 조향', disabled=True, readout_format='.2f')
# 현재 적용되는 기본 속도 표시 (읽기 전용)
# speed_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='현재 속도', disabled=True, readout_format='.2f')

# 시각화 슬라이더 표시
display(ipywidgets.VBox([
    ipywidgets.HTML("<h4><span class='emoji'>📈</span> 내부 예측 및 제어값</h4>"),
    x_slider,
    # y_slider, # 필요시 주석 해제
    steering_slider,
    # speed_slider # 필요시 주석 해제
]))
            </code></pre>

             <h3><span class="emoji">🧠➡️🚗</span> 5.7. 실행 함수 (`execute`) 정의 및 연결</h3>
             <p>이것이 바로 실시간 주행의 핵심 로직입니다! 카메라에서 새로운 프레임이 들어올 때마다 이 함수가 호출되어 다음 작업을 순차적으로 수행합니다:</p>
             <ol>
                 <li>새로운 카메라 이미지 가져오기.</li>
                 <li>이미지 <strong>전처리</strong> (<code>preprocess</code> 함수 사용).</li>
                 <li>TensorRT 모델로 (x, y) 좌표 <strong>추론</strong>.</li>
                 <li>모델 출력값(x, y)을 <strong>해석</strong>하여 주행 제어에 사용할 형태로 변환.
                    *   원본 코드의 `y = (0.5 - xy[1]) / 2.0`는 모델의 y 출력값(위쪽이 1, 아래쪽이 -1)을 `atan2` 함수에 사용하기 적합한 형태(위쪽/멀리 갈수록 0에 가깝고, 아래쪽/가까울수록 0.5에 가까움)로 변환하는 과정입니다. 이 y값은 각도 계산 시 일종의 '거리 가중치' 역할을 하여, 같은 x 편차라도 y값이 작을수록 (목표가 멀리 있을수록) 더 큰 각도 변화를 유도합니다.
                 </li>
                 <li>`np.arctan2(x, y)`를 사용하여 현재 로봇이 목표 지점을 향하기 위해 회전해야 할 <strong>목표 각도(angle)</strong>를 라디안 단위로 계산합니다.</li>
                 <li><strong>PD 제어(Proportional-Derivative Control)</strong> 로직 적용:
                     *   **P (비례) 제어:** 현재 목표 각도(<code>angle</code>) 오차에 비례하여 조향 강도를 결정합니다 (<code>angle * Kp</code>).
                     *   **D (미분) 제어:** 각도 오차의 변화율(<code>angle - angle_last</code>)에 비례하여 조향 강도를 결정합니다 (<code>(angle - angle_last) * Kd</code>). 급격한 변화를 억제하여 안정성을 높입니다.
                 </li>
                 <li>최종 조향 값 계산: PD 제어 결과에 <strong>조향 편향(bias)</strong>을 더합니다.</li>
                 <li>계산된 기본 속도와 최종 조향 값을 조합하여 <strong>왼쪽/오른쪽 모터 각각의 속도</strong>를 결정합니다. 조향 값에 따라 좌우 모터 속도에 차이를 주어 회전합니다. 모터 값은 항상 [0.0, 1.0] 범위 내로 제한(clamping)합니다.</li>
                 <li>계산된 속도로 로봇 <strong>모터를 구동</strong>합니다!</li>
                 <li>다음 계산을 위해 현재 각도를 <code>angle_last</code>에 저장합니다.</li>
             </ol>
             <p>마지막으로, `camera.observe()` 함수를 사용하여 카메라의 `value` 속성(즉, 새 프레임)이 변경될 때마다 이 `execute` 함수가 자동으로 호출되도록 연결합니다.</p>
             <pre><code class="language-python">
import numpy as np
import time # 로봇 정지 전 잠시 대기 위해 추가

# PD 제어를 위한 이전 각도 값 저장 변수
angle = 0.0
angle_last = 0.0

# 카메라 프레임이 업데이트될 때마다 실행될 메인 함수
def execute(change):
    global angle, angle_last # 함수 외부의 angle, angle_last 변수 사용 선언

    # 1. 새로운 카메라 이미지 가져오기
    image = change['new'] # 'new' 키에 새 이미지 데이터가 들어있음

    # 2. 이미지 전처리 및 모델 추론
    xy = model_trt(preprocess(image)).detach().float().cpu().numpy().flatten()
    x = xy[0] # 예측된 x 좌표 (-1 ~ 1)
    # 3. 모델 y 출력값 변환 (주행 로직용)
    y = (0.5 - xy[1]) / 2.0 # 값이 작을수록 멀리 있는 것으로 해석 (0 ~ 0.5 범위)

    # (선택 사항) 시각화 슬라이더 업데이트
    x_slider.value = float(x)
    # y_slider.value = float(y) # 필요시 주석 해제

    # 현재 설정된 기본 속도 가져오기
    speed_value = speed_gain_slider.value
    # speed_slider.value = speed_value # 필요시 주석 해제

    # 4. 목표 각도 계산 (arctan2 사용, 라디안 단위)
    # atan2(x, y)는 y축(전방) 기준 x 방향으로의 각도를 반환
    angle = np.arctan2(x, y)

    # 5. PD 제어 계산
    Kp = steering_gain_slider.value
    Kd = steering_dgain_slider.value
    # 비례항(P) + 미분항(D)
    pid = Kp * angle + Kd * (angle - angle_last)

    # 6. 현재 각도를 다음 계산을 위해 저장
    angle_last = angle

    # 7. 최종 조향 값 계산 (PD 제어 값 + 조향 편향)
    bias = steering_bias_slider.value
    steering_value = pid + bias

    # (선택 사항) 조향 시각화 슬라이더 업데이트
    steering_slider.value = float(steering_value)

    # 8. 좌/우 모터 속도 계산 및 범위 제한 [0.0, 1.0]
    # 왼쪽 모터: 기본 속도 + 조향 값 (오른쪽으로 돌 때 빨라짐)
    # 오른쪽 모터: 기본 속도 - 조향 값 (왼쪽으로 돌 때 빨라짐)
    left_motor_value = max(min(speed_value + steering_value, 1.0), 0.0)
    right_motor_value = max(min(speed_value - steering_value, 1.0), 0.0)

    # 9. 로봇 모터 구동!
    robot.left_motor.value = left_motor_value
    robot.right_motor.value = right_motor_value

# ---- 실행 준비 및 시작 ----

# 이전에 등록된 observe 함수가 있다면 해제 (코드 재실행 시 중복 방지)
try:
    camera.unobserve_all()
    print("이전 카메라 observe 콜백 해제 완료.")
except Exception as e:
    print(f"이전 콜백 해제 중 오류 (무시 가능): {e}")

# 초기 실행 (선택 사항, 첫 프레임 처리를 원할 경우)
# print("첫 프레임 처리 시도...")
# execute({'new': camera.value})

# 카메라의 'value'가 변경될 때마다 execute 함수를 호출하도록 등록!
camera.observe(execute, names='value')

print("🚀 실시간 주행 시작 준비 완료!")
print("⚠️ 경고: JetBot이 움직일 수 있습니다! 주변 공간을 확보하고 트랙 위에 올려놓으세요.")
print("   주행 중 문제가 발생하면 아래 '주행 중지' 코드를 실행하세요.")

            </code></pre>

            <h3><span class="emoji">🛑</span> 5.8. 주행 시작 및 중지</h3>
            <p>위의 마지막 코드 셀을 실행하면, JetBot은 카메라로부터 새로운 프레임이 들어올 때마다 <code>execute</code> 함수를 실행하여 스스로 주행을 시작합니다! 슬라이더를 조절하며 JetBot이 트랙을 부드럽게 따라가는 최적의 파라미터를 찾아보세요.</p>
            <p><strong>주행을 멈추고 싶을 때는</strong>, 아래 코드 셀을 실행하여 카메라와 <code>execute</code> 함수 간의 연결(observe)을 끊고 모터를 정지시키세요.</p>
            <pre><code class="language-python">
# 카메라 observe 해제 (execute 함수 호출 중단)
camera.unobserve(execute, names='value')
print("카메라 observe 해제 완료.")

# 로봇 모터 즉시 정지
time.sleep(0.1) # 혹시 모를 딜레이 후 정지
robot.stop()
print("🛑 로봇 모터 정지 완료.")

# (선택 사항) 카메라 정지
# camera.stop()
# print("카메라 정지 완료.")
            </code></pre>
        </div>

        <hr>

        <div class="section" id="conclusion">
            <h2><span class="emoji">🏁</span> 6. 결론 및 다음 단계</h2>
            <p>축하드립니다! 🎉 이 긴 여정을 통해 여러분은 JetBot으로 도로 주행 데이터를 수집하고, 딥러닝 모델을 학습시키고, TensorRT로 최적화하여, 마침내 실시간 자율 주행 데모까지 구현하는 전체 과정을 경험하셨습니다.</p>
            <p>이 프로젝트는 컴퓨터 비전, 딥러닝(회귀), 로봇 제어(PD 제어), 그리고 엣지 컴퓨팅 최적화(TensorRT) 등 다양한 핵심 기술을 아우르는 훌륭한 실습 예제입니다. 여기서 얻은 지식과 경험은 다른 로봇 공학 및 AI 프로젝트에도 유용하게 활용될 수 있을 것입니다.</p>
            <p><strong>다음 단계로 도전해볼 만한 아이디어들:</strong></p>
            <ul>
                <li><strong><span class="emoji">🏞️</span> 다양한 환경에서 데이터 수집 및 학습:</strong> 더 복잡한 트랙, 다른 조명 조건, 장애물이 있는 환경 등에서 데이터를 추가로 수집하고 모델 성능을 개선해보세요.</li>
                <li><strong><span class="emoji">🦾</span> 모델 아키텍처 변경:</strong> ResNet 외에 MobileNet, EfficientNet 등 다른 경량 모델 아키텍처를 사용하여 성능과 속도를 비교해보세요.</li>
                <li><strong><span class="emoji">🧭</span> 제어 로직 개선:</strong> 단순한 PD 제어 대신 PID 제어 전체를 구현하거나, 경로 계획 알고리즘을 결합하여 더 정교한 주행 제어를 시도해보세요.</li>
                <li><strong><span class="emoji">🚦</span> 추가 기능 통합:</strong> 신호등 인식, 장애물 회피 등 다른 AI 모델과 결합하여 더 지능적인 주행 시스템을 구축해보세요.</li>
            </ul>
            <p>JetBot과 함께하는 AI 여정에 행운이 가득하길 바랍니다! <span class="emoji">💡</span><span class="emoji">🚀</span></p>
        </div>

    </div> <!-- /container -->

    <!-- Three.js 라이브러리 로딩 (Importmap 사용 - CDN 방식) -->
    <!-- 중요: 완전한 단일 파일을 위해서는 이 부분을 제거하고,
         필요한 three.js 라이브러리 코드를 아래 <script type="module"> 안에 직접 삽입해야 합니다.
         그러나 이는 파일 크기를 매우 크게 만듭니다. CDN 방식이 일반적입니다. -->
    <script type="importmap">
      {
        "imports": {
          "three": "https://unpkg.com/three@0.163.0/build/three.module.js",
          "three/addons/": "https://unpkg.com/three@0.163.0/examples/jsm/"
        }
      }
    </script>

    <!-- 3D 뷰어 실행 스크립트 -->
    <script type="module">
        // 필요한 모듈 임포트
        import * as THREE from 'three';
        import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
        import { OrbitControls } from 'three/addons/controls/OrbitControls.js';

        let scene, camera, renderer, controls, model;
        const container = document.getElementById('viewer-container');
        const loadingIndicator = document.getElementById('loading-indicator');

        function init3DViewer() {
            // 1. Scene 생성
            scene = new THREE.Scene();
            scene.background = new THREE.Color(0xf1f3f5); // 컨테이너 배경색과 유사하게
            // scene.fog = new THREE.Fog(0xf1f3f5, 5, 15); // 약간의 안개 효과 (선택 사항)

            // 2. Camera 생성
            camera = new THREE.PerspectiveCamera(
                50, // Field of View (시야각) - 조금 줄여서 왜곡 감소
                container.clientWidth / container.clientHeight, // Aspect Ratio (종횡비)
                0.1, // Near clipping plane (가까운 절단면)
                100 // Far clipping plane (먼 절단면)
            );
            camera.position.set(1.5, 1.2, 2.5); // 초기 카메라 위치 조정

            // 3. Renderer 생성
            renderer = new THREE.WebGLRenderer({ antialias: true }); // 안티앨리어싱 활성화
            renderer.setSize(container.clientWidth, container.clientHeight);
            renderer.setPixelRatio(window.devicePixelRatio); // 고해상도 디스플레이 지원
            // renderer.shadowMap.enabled = true; // 그림자 활성화 (필요시)
            // renderer.outputEncoding = THREE.sRGBEncoding; // 색상 인코딩 (GLB 표준)
            container.appendChild(renderer.domElement);

            // 4. Controls 생성 (OrbitControls)
            controls = new OrbitControls(camera, renderer.domElement);
            controls.enableDamping = true; // 부드러운 카메라 움직임
            controls.dampingFactor = 0.08;
            controls.screenSpacePanning = false; // 패닝(이동) 방식 제한
            controls.minDistance = 0.5; // 최소 줌 거리
            controls.maxDistance = 8; // 최대 줌 거리
            controls.target.set(0, 0.3, 0); // 카메라가 바라볼 초기 지점 (모델 중심으로 조정)
            controls.update();

            // 5. Lighting 설정
            const ambientLight = new THREE.AmbientLight(0xffffff, 1.0); // 전체적으로 부드러운 빛
            scene.add(ambientLight);

            const hemiLight = new THREE.HemisphereLight( 0xffffff, 0x888888, 1.2 ); // 하늘/땅 색상 빛
            hemiLight.position.set( 0, 20, 0 );
            scene.add( hemiLight );

            const dirLight = new THREE.DirectionalLight(0xffffff, 1.5); // 주요 방향성 빛 (태양광 느낌)
            dirLight.position.set(3, 10, 5);
            // dirLight.castShadow = true; // 그림자 생성 (성능 저하 가능)
            scene.add(dirLight);

             const dirLight2 = new THREE.DirectionalLight(0xffffff, 0.8); // 보조 방향성 빛 (반대편)
             dirLight2.position.set(-3, -10, -5);
             scene.add(dirLight2);


            // 6. GLTF 모델 로더 설정 및 로드
            const loader = new GLTFLoader();
            loader.load(
                'jetbot.glb', // GLB 파일 경로 (HTML과 같은 디렉토리)
                // 로드 성공 시 콜백 함수
                function (gltf) {
                    model = gltf.scene;

                    // 모델 자동 중앙 정렬 및 스케일 조정 (뷰어에 잘 보이도록)
                    const box = new THREE.Box3().setFromObject(model);
                    const center = box.getCenter(new THREE.Vector3());
                    const size = box.getSize(new THREE.Vector3());
                    const maxDim = Math.max(size.x, size.y, size.z);
                    const scale = 1.5 / maxDim; // 뷰어 크기에 맞게 스케일 조정 (값 조절 가능)

                    model.scale.set(scale, scale, scale); // 스케일 적용
                    model.position.sub(center.multiplyScalar(scale)); // 중앙으로 이동

                    // 모델 위치 미세 조정 (예: 바닥에 딱 붙도록)
                    model.position.y -= size.y * scale / 2 - 0.01; // 바닥 높이 계산 후 살짝 위로

                    scene.add(model); // 씬에 모델 추가
                    loadingIndicator.style.display = 'none'; // 로딩 인디케이터 숨기기
                    console.log('🤖 JetBot 3D 모델 로드 성공!');

                    // 그림자 설정 (만약 활성화했다면)
                    // model.traverse(function (node) {
                    //     if (node.isMesh) {
                    //         node.castShadow = true;
                    //         node.receiveShadow = true;
                    //     }
                    // });
                },
                // 로딩 진행 중 콜백 함수 (선택 사항)
                function (xhr) {
                    const percentLoaded = (xhr.loaded / xhr.total) * 100;
                    loadingIndicator.textContent = `⏳ 모델 로딩 중... ${Math.round(percentLoaded)}%`;
                    // console.log((xhr.loaded / xhr.total * 100) + '% loaded');
                },
                // 로드 오류 시 콜백 함수
                function (error) {
                    console.error('❌ 3D 모델 로드 중 오류 발생:', error);
                    loadingIndicator.textContent = '😭 모델 로드 실패!';
                    loadingIndicator.style.color = 'red';
                }
            );

            // 7. 창 크기 변경 시 처리
            window.addEventListener('resize', onWindowResize, false);

            // 8. 애니메이션 루프 시작
            animate();
        }

        // 창 크기 변경 핸들러
        function onWindowResize() {
            if (!camera || !renderer || !container) return;
            camera.aspect = container.clientWidth / container.clientHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(container.clientWidth, container.clientHeight);
        }

        // 애니메이션 루프
        function animate() {
            requestAnimationFrame(animate); // 다음 프레임 요청
            controls.update(); // OrbitControls 업데이트 (댐핑 등)
            renderer.render(scene, camera); // 씬 렌더링
        }

        // DOM 로드 완료 후 3D 뷰어 초기화 실행
        document.addEventListener('DOMContentLoaded', init3DViewer);

    </script>

</body>
</html>
