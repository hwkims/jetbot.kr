<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>🤖 JetBot 도로 주행 프로젝트 v2 (CORS 수정 시도)</title>
    <style>
        /* --- 기본 스타일 --- */
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: #f8f9fa; /* 밝은 회색 배경 */
            color: #343a40; /* 어두운 텍스트 색상 */
        }

        .container {
            max-width: 1000px;
            margin: 30px auto;
            padding: 30px;
            background-color: #ffffff; /* 흰색 배경 */
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1); /* 그림자 강조 */
            border-radius: 8px; /* 모서리 둥글게 */
        }

        /* --- 제목 스타일 --- */
        h1, h2, h3, h4 {
            color: #0b3d91; /* 진한 파란색 (NVIDIA 느낌) */
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            font-weight: 600;
        }
        h1 { text-align: center; border-bottom: 3px solid #76b900; /* NVIDIA 녹색 강조 */}
        h2 { border-bottom: 2px solid #76b900; margin-top: 2.5rem; }
        h3 { border-bottom: 1px dashed #ced4da; margin-top: 2rem; }
        h4 { color: #17a2b8; margin-top: 1.5rem; font-weight: 500;} /* 정보성 청록색 */

        /* --- 코드 블록 --- */
        pre {
            background-color: #282c34; /* 어두운 코드 배경 */
            color: #abb2bf; /* 밝은 코드 텍스트 */
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            border: 1px solid #3e4451;
            font-size: 0.9em;
            line-height: 1.5;
        }
        code {
            font-family: 'Fira Code', 'Consolas', 'Monaco', monospace; /* 코딩용 폰트 */
        }
        pre code {
            background-color: transparent;
            padding: 0;
        }
        /* 인라인 코드 */
        :not(pre) > code {
            background-color: #e9ecef;
            color: #c82c7a; /* 핑크 강조 */
            padding: 0.2em 0.5em;
            margin: 0 0.1em;
            font-size: 85%;
            border-radius: 4px;
            font-weight: 500;
        }

        /* --- 기타 요소 --- */
        strong {
            color: #28a745; /* 녹색 강조 */
            font-weight: 600;
        }
        ul, ol {
            margin-left: 25px;
            padding-left: 0;
        }
        li {
            margin-bottom: 12px;
        }
        a {
            color: #007bff;
            text-decoration: none;
            font-weight: 500;
        }
        a:hover {
            text-decoration: underline;
            color: #0056b3;
        }
        hr {
            border: 0;
            height: 1px;
            background-color: #dee2e6;
            margin: 45px 0;
        }
        .emoji {
            margin-right: 7px;
            vertical-align: -0.1em; /* 이모지 수직 정렬 */
        }
        blockquote {
            border-left: 4px solid #76b900; /* 인용구 강조 */
            padding-left: 15px;
            margin: 20px 0;
            color: #495057;
            background-color: #f1f3f5;
            padding-top: 10px;
            padding-bottom: 10px;
            border-radius: 0 4px 4px 0;
        }
        .warning {
            color: #dc3545; /* 경고 빨간색 */
            font-weight: bold;
        }
        .tip {
            color: #17a2b8; /* 팁 청록색 */
            font-weight: bold;
        }

        /* --- 3D 뷰어 컨테이너 --- */
        #viewer-container {
            width: 100%;
            height: 550px;
            margin: 30px 0;
            border: 1px solid #ced4da;
            position: relative;
            background-color: #e9ecef; /* 뷰어 배경 약간 어둡게 */
            border-radius: 5px;
            overflow: hidden;
        }
        #loading-indicator {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            font-size: 1.3em;
            color: #495057;
            background-color: rgba(255, 255, 255, 0.85);
            padding: 12px 24px;
            border-radius: 5px;
            box-shadow: 0 2px 6px rgba(0,0,0,0.15);
            text-align: center;
        }

        /* --- 섹션 구분 --- */
        .section {
            margin-bottom: 45px;
        }

        /* --- 반응형 디자인 --- */
        @media (max-width: 768px) {
            .container {
                margin: 15px;
                padding: 20px;
            }
            #viewer-container {
                height: 450px;
            }
            h1 { font-size: 1.8em; }
            h2 { font-size: 1.5em; }
            pre { padding: 15px; }
        }
    </style>
</head>
<body>
    <div class="container">

        <h1><span class="emoji">🤖</span> JetBot 도로 주행 프로젝트: 완전 정복 가이드</h1>
        <p style="text-align: center; font-size: 1.1em; color: #6c757d;">데이터 수집부터 TensorRT 최적화, 실시간 데모까지! (インタラクティブ 3D 모델 포함)</p>

        <hr>

        <div class="section">
            <h2><span class="emoji">✨</span> JetBot 3D 모델 미리보기</h2>
            <p>프로젝트의 주인공, JetBot의 3D 모델을 직접 살펴보세요! 마우스를 사용하여 아래 뷰어에서 자유롭게 조작할 수 있습니다.</p>
            <ul>
                <li><span class="emoji">🔄</span> <strong>회전:</strong> 마우스 왼쪽 버튼 클릭 + 드래그</li>
                <li><span class="emoji">🔍</span> <strong>확대/축소:</strong> 마우스 휠 스크롤</li>
                <li><span class="emoji">🖐️</span> <strong>이동(패닝):</strong> 마우스 오른쪽 버튼 클릭 + 드래그 (또는 Ctrl/Cmd + 왼쪽 버튼 클릭 + 드래그)</li>
            </ul>
            <div id="viewer-container">
                <div id="loading-indicator">⏳ JetBot 모델 로딩 중... 잠시만 기다려주세요!</div>
            </div>
             <blockquote class="tip"><span class="emoji">💡</span> 모델 URL: 제공해주신 GitHub Raw 링크를 사용합니다. 로딩에 실패하면 URL이나 네트워크 상태를 확인해주세요.</blockquote>
        </div>

        <hr>

        <div class="section" id="project-overview">
            <h2><span class="emoji">🎯</span> 1. 프로젝트 개요: JetBot, 스스로 길을 찾다!</h2>
            <p>이 문서는 NVIDIA JetBot이 카메라(<span class="emoji">📷</span>)로 세상을 보고, 스스로 판단하여 도로(또는 지정된 트랙)를 따라 주행하는 <strong>자율 주행 시스템</strong>을 만드는 전 과정을 안내하는 종합 가이드입니다. 마치 JetBot에게 운전을 가르치는 과정과 같죠! <span class="emoji">👨‍🏫</span> <span class="emoji">➡️</span> <span class="emoji">🤖</span></p>
            <p>총 4개의 핵심 단계를 거치며 JetBot을 똑똑한 드라이버로 만들어 봅시다:</p>
            <ol>
                <li><strong><span class="emoji">📸</span> 데이터 수집 (Data Collection):</strong> JetBot의 눈으로 본 주행 경로 이미지와 "이리로 가!"라고 알려주는 목표 지점 데이터를 만듭니다. 양질의 데이터가 똑똑한 AI의 시작입니다!</li>
                <li><strong><span class="emoji">🧠</span> 모델 학습 (Model Training):</strong> 수집한 데이터를 딥러닝 모델(ResNet-18 기반)에게 학습시켜, 이미지 속에서 최적의 목표 지점을 스스로 예측하는 능력을 길러줍니다.</li>
                <li><strong><span class="emoji">⚡</span> TensorRT 최적화 (TensorRT Optimization):</strong> 학습된 모델을 Jetson Nano와 같은 엣지 디바이스에서 번개처럼(<span class="emoji">⚡</span>) 빠르게 작동하도록 변환하고 최적화합니다. 실시간 주행의 핵심이죠!</li>
                <li><strong><span class="emoji">🚀</span> 실시간 데모 (Live Demo):</strong> 최적화된 모델을 JetBot에 탑재하여, 실시간 카메라 영상을 보며 스스로 판단하고 주행하는 놀라운 광경을 직접 확인합니다!</li>
            </ol>
            <p>자, 이제 JetBot과 함께 흥미진진한 자율 주행의 세계로 떠나볼까요?</p>
        </div>

        <hr>

        <div class="section" id="data-collection">
            <h2><span class="emoji">📸</span> 2. 데이터 수집: AI의 눈을 위한 양식 만들기</h2>
            <p>훌륭한 AI 모델을 만들려면 좋은 '식재료', 즉 <strong>데이터</strong>가 필수입니다. 이 단계에서는 JetBot이 직접 주행할 환경에서 '어떻게 운전해야 하는지'에 대한 정보를 담은 데이터를 수집합니다.</p>

            <h3><span class="emoji">⭐</span> 2.1. 목표: 무엇을, 어떻게 수집하나?</h3>
            <ul>
                <li>JetBot의 카메라를 통해 실제 주행 경로의 이미지를 캡처합니다.</li>
                <li>각 이미지마다, JetBot이 다음 순간 향해야 할 <strong>최적의 목표 지점(x, y 픽셀 좌표)</strong>을 마우스 클릭으로 기록(라벨링)합니다.</li>
                <li><span class="emoji">💡</span> <strong>다양성이 생명!</strong> 모델이 다양한 실제 상황에 잘 대응하도록, 여러 위치, 각도, 조명 아래에서 데이터를 수집하는 것이 매우 중요합니다. (예: 트랙 중앙, 가장자리, 커브 진입/탈출 등) 최소 수백 장 이상 수집하는 것을 권장합니다.</li>
            </ul>

            <h3><span class="emoji">🗺️</span> 2.2. 라벨링 가이드: "어디를 찍어야 할까요?"</h3>
            <p>정확하고 일관된 라벨링은 모델 성능을 좌우합니다. 목표 지점을 선택할 때 다음 가이드라인을 참고하세요:</p>
            <ol>
                <li><strong>실시간 영상 주시:</strong> Jupyter Lab에 표시되는 JetBot 카메라 영상을 주의 깊게 살펴봅니다.</li>
                <li><strong>이상적인 경로 상상:</strong> 로봇이 트랙 중앙 또는 특정 라인을 따라 부드럽게 나아가야 할 경로를 그려봅니다.</li>
                <li><strong>"가장 먼 안전 지점" 선택:</strong> 로봇이 현재 위치에서 그 지점을 향해 <strong>직진했을 때, 경로를 벗어나지 않을 것으로 예상되는 가장 먼 지점</strong>을 클릭합니다.
                    <ul>
                        <li><strong>직선 구간:</strong> 시야가 확보되는 한 최대한 멀리 찍어도 좋습니다.</li>
                        <li><strong>커브 구간:</strong> <span class="warning">주의!</span> 커브 안쪽을 따라 너무 멀리 찍으면 로봇이 경로를 이탈할 수 있습니다. 커브를 안전하게 돌 수 있는 비교적 가까운 지점을 선택해야 합니다.</li>
                    </ul>
                </li>
                <li><strong>일관성 유지:</strong> 자신만의 라벨링 기준을 정하고, 데이터 수집 내내 일관되게 적용하는 것이 중요합니다.</li>
            </ol>
            <blockquote>"Garbage in, garbage out." - 데이터 과학의 오랜 격언처럼, 데이터의 품질이 모델의 성능을 결정합니다. 신중하게 라벨링해주세요!</blockquote>

            <h3><span class="emoji">💻</span> 2.3. 구현 코드 (데이터 수집 위젯 설정)</h3>
            <p>Jupyter Lab에서 실행하는 Python 코드입니다. 카메라 영상을 보여주고, 사용자가 이미지를 클릭하면 해당 좌표와 함께 이미지를 저장하는 인터페이스를 만듭니다.</p>
            <pre><code class="language-python">
# --- 필요한 라이브러리 임포트 ---
import ipywidgets # Jupyter 위젯
import traitlets # 위젯 간 연결
from IPython.display import display, HTML # Jupyter 디스플레이
from jetbot import Robot, Camera, bgr8_to_jpeg # JetBot 라이브러리
from uuid import uuid1 # 고유 ID 생성
import os
import glob
import cv2 # OpenCV (이미지 처리)
import time
from jupyter_clickable_image_widget import ClickableImageWidget # 클릭 가능한 이미지 위젯

# --- 설정 ---
DATASET_DIR = 'dataset_xy' # 데이터 저장 디렉토리

# --- 디렉토리 생성 ---
try:
    os.makedirs(DATASET_DIR)
    print(f"<span class='emoji'>📂</span> '{DATASET_DIR}' 디렉토리를 생성했습니다.")
except FileExistsError:
    print(f"<span class='emoji'>✅</span> '{DATASET_DIR}' 디렉토리가 이미 존재합니다.")

# --- 카메라 및 위젯 초기화 ---
camera = Camera(width=224, height=224) # 모델 입력 크기에 맞춘 카메라
camera_widget = ClickableImageWidget(width=camera.width, height=camera.height) # 클릭 가능한 라이브 뷰
snapshot_widget = ipywidgets.Image(width=camera.width, height=camera.height) # 저장된 스냅샷 뷰
count_widget = ipywidgets.IntText(description='<span class="emoji">💾</span> 저장된 이미지:', value=0, disabled=True, layout=ipywidgets.Layout(width='200px')) # 이미지 개수 표시

# --- 카메라 영상 연결 ---
traitlets.dlink((camera, 'value'), (camera_widget, 'value'), transform=bgr8_to_jpeg)

# --- 초기 이미지 개수 업데이트 ---
count_widget.value = len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))

# --- 이미지 클릭 시 실행될 함수 ---
def save_snapshot(_, content, msg):
    if content['event'] == 'click':
        data = content['eventData']
        x = data['offsetX']
        y = data['offsetY']

        # 고유 파일명 생성 (좌표 포함)
        uuid_str = str(uuid1())
        filename = f'xy_{x:03d}_{y:03d}_{uuid_str}.jpg'
        image_path = os.path.join(DATASET_DIR, filename)

        # 이미지 파일 저장 (JPEG)
        try:
            with open(image_path, 'wb') as f:
                f.write(camera_widget.value)
        except Exception as e:
            print(f"<span class='emoji'>❌</span> 이미지 저장 오류: {e}")
            return # 오류 발생 시 중단

        # 스냅샷에 녹색 원 표시
        snapshot = camera.value.copy()
        cv2.circle(snapshot, (x, y), 8, (0, 255, 0), 3) # 녹색 원 (BGR 순서)
        snapshot_widget.value = bgr8_to_jpeg(snapshot)

        # 이미지 개수 업데이트
        count_widget.value = len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))

# --- 클릭 이벤트 핸들러 등록 ---
camera_widget.on_msg(save_snapshot)

# --- 위젯 레이아웃 및 표시 ---
data_collection_widget = ipywidgets.VBox([
    HTML("<h2><span class='emoji'>🖱️</span> 이미지 클릭으로 데이터 수집 시작!</h2>"),
    ipywidgets.HBox([
        ipywidgets.VBox([HTML("<h4>라이브 카메라</h4>"), camera_widget]),
        ipywidgets.VBox([HTML("<h4>저장된 스냅샷 (클릭 지점)</h4>"), snapshot_widget])
    ]),
    count_widget
])

display(data_collection_widget)
print("<span class='emoji'>✅</span> 카메라 및 위젯 준비 완료. 라이브 이미지를 클릭하여 데이터 수집을 시작하세요!")
            </code></pre>

            <h3><span class="emoji">📊</span> 2.4. 데이터 확인: 수집 결과는?</h3>
            <p>데이터 수집을 완료한 후, Jupyter Lab의 파일 브라우저에서 <code>dataset_xy</code> 폴더를 열어보세요. 수많은 <code>xy_<x>_<y>_<uuid>.jpg</code> 파일들이 생성된 것을 확인할 수 있습니다. 파일명에 포함된 <code><x></code>와 <code><y></code> 숫자가 바로 여러분이 클릭한 소중한 라벨 정보입니다. 이 데이터가 다음 단계에서 JetBot의 '지능'을 만드는 데 사용됩니다!</p>
        </div>

        <hr>

        <!-- Model Training Section (Content from previous good answer, adjusted with emojis) -->
        <div class="section" id="model-training">
            <h2><span class="emoji">🧠</span> 3. 모델 학습: JetBot에게 운전 가르치기</h2>
            <p>수집된 데이터를 이용해 JetBot의 '두뇌'가 될 딥러닝 모델을 학습시킵니다. 목표는 이미지를 보고, 우리가 알려준 정답(목표 지점 좌표)을 스스로 예측하도록 만드는 것입니다. 일종의 '이미지 보고 좌표 맞추기' 훈련이죠!</p>

            <h3><span class="emoji">⭐</span> 3.1. 학습 목표</h3>
            <ul>
                <li>입력 이미지(224x224)로부터 목표 지점의 정규화된 (x, y) 좌표 [-1, 1] 범위를 예측하는 <strong>회귀(Regression)</strong> 모델을 만듭니다.</li>
                <li>이미지 인식 분야에서 검증된 <strong>ResNet-18</strong> 아키텍처를 기반으로 사용합니다.</li>
                <li><strong>전이 학습(Transfer Learning)</strong> <span class="emoji">🎓</span>: 이미 방대한 이미지 데이터(ImageNet)로 학습된 ResNet-18 모델을 가져와, 우리의 도로 주행 문제에 맞게 마지막 부분만 미세 조정합니다. 이렇게 하면 적은 데이터로도 효과적으로 학습할 수 있습니다!</li>
                <li>딥러닝 프레임워크는 <strong>PyTorch</strong>를 사용합니다.</li>
            </ul>

             <h3><span class="emoji">🛠️</span> 3.2. 데이터셋 준비: 학습 재료 손질하기</h3>
             <h4>3.2.1. 커스텀 데이터셋 클래스 (<code>XYDataset</code>)</h4>
             <p>PyTorch가 데이터를 효율적으로 다룰 수 있도록, 우리 데이터 형식에 맞는 <code>Dataset</code> 클래스를 정의합니다. 주요 역할은 다음과 같습니다:</p>
             <ul>
                 <li>이미지 파일 로드 및 경로 관리</li>
                 <li>파일명에서 x, y 좌표 추출 및 [-1, 1] 범위로 <strong>정규화</strong></li>
                 <li><strong>데이터 증강(Data Augmentation)</strong> 적용 (모델 강건성 향상):
                     <ul>
                         <li><span class="emoji">🎨</span> <code>ColorJitter</code>: 밝기, 대비 등 무작위 변경</li>
                         <li><span class="emoji">↔️</span> <code>Random Horizontal Flip</code>: 무작위 좌우 반전 (필요시 x 좌표 부호 변경)</li>
                     </ul>
                 </li>
                 <li>이미지 크기 조정 (224x224) 및 PyTorch <strong>텐서(Tensor)</strong> 변환</li>
                 <li>ImageNet 기반 <strong>정규화</strong> 적용 (평균 빼고 표준편차로 나누기)</li>
             </ul>
            <pre><code class="language-python">
import torch
import torch.optim as optim
import torch.nn.functional as F
import torchvision
import torchvision.datasets as datasets
import torchvision.models as models
import torchvision.transforms as transforms
import glob
import PIL.Image
import os
import numpy as np

# --- 좌표 추출 및 정규화 함수 ---
def get_x(path, width):
    x_pixel = float(int(os.path.basename(path).split('_')[1]))
    return (x_pixel - width / 2) / (width / 2)

def get_y(path, height):
    y_pixel = float(int(os.path.basename(path).split('_')[2]))
    # Y축은 아래로 갈수록 값이 커지므로, 정규화하면 위쪽이 -1, 아래쪽이 1에 가까움
    return (y_pixel - height / 2) / (height / 2)

# --- 커스텀 데이터셋 클래스 ---
class XYDataset(torch.utils.data.Dataset):
    def __init__(self, directory, random_hflips=False):
        self.directory = directory
        self.random_hflips = random_hflips
        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))
        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)
        # 이미지 전처리 파이프라인 정의
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(), # PIL -> Tensor, [0,1]
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet 정규화
        ])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        image = PIL.Image.open(image_path).convert('RGB') # RGB로 로드
        width, height = image.size
        x = float(get_x(image_path, width))
        y = float(get_y(image_path, height))

        # 데이터 증강 (Augmentation)
        if self.random_hflips and np.random.rand() > 0.5:
            image = transforms.functional.hflip(image)
            x = -x # 좌우 반전 시 x 좌표 부호 반전!
        image = self.color_jitter(image) # 색상 증강

        # 최종 전처리 적용
        image = self.transform(image)

        return image, torch.tensor([x, y]).float() # 이미지와 좌표 텐서 반환

# --- 데이터셋 인스턴스 생성 ---
dataset = XYDataset(DATASET_DIR, random_hflips=False) # 좌우 반전 비활성화
print(f"<span class='emoji'>📚</span> 데이터셋 로드 완료! 총 이미지: {len(dataset)}")
            </code></pre>

             <h4>3.2.2. 데이터 분할 및 로더 생성</h4>
             <p>전체 데이터를 <strong>훈련용</strong>과 <strong>테스트용</strong>으로 나눕니다(예: 90% 훈련, 10% 테스트). <code>DataLoader</code>는 데이터를 미니배치(minibatch) 단위로 묶고 섞어주어(<span class="emoji">🔀</span>) 효율적인 학습을 돕습니다.</p>
             <pre><code class="language-python">
# --- 데이터 분할 ---
test_percent = 0.1
num_total = len(dataset)
num_test = int(test_percent * num_total)
num_train = num_total - num_test
train_dataset, test_dataset = torch.utils.data.random_split(dataset, [num_train, num_test])
print(f"데이터 분할: 훈련 {num_train}개 ({100-test_percent*100:.0f}%), 테스트 {num_test}개 ({test_percent*100:.0f}%)")

# --- 데이터 로더 생성 ---
BATCH_SIZE = 8 # 배치 크기 (GPU 메모리 고려)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
print(f"<span class='emoji'>📦</span> 데이터 로더 생성 완료! 배치 크기: {BATCH_SIZE}")

            </code></pre>

             <h3><span class="emoji">🏗️</span> 3.3. 모델 아키텍처 정의 (ResNet-18 수정)</h3>
             <p>사전 학습된 ResNet-18 모델을 불러와, 마지막 출력 레이어만 (x, y) 좌표 2개를 출력하도록 교체합니다. 이것이 바로 <strong>전이 학습</strong>의 마법!</p>
             <pre><code class="language-python">
# --- 모델 정의 ---
model = models.resnet18(pretrained=True) # ImageNet 사전 학습 가중치 사용
num_output_features = 2 # x, y 좌표 2개 출력
# 마지막 레이어(fc)를 새로운 Linear 레이어로 교체
model.fc = torch.nn.Linear(model.fc.in_features, num_output_features)

# GPU 사용 설정
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
print(f"<span class='emoji'>✅</span> ResNet-18 모델 준비 완료! (사용 디바이스: {device})")
            </code></pre>

            <h3><span class="emoji">🏋️</span> 3.4. 모델 학습 실행</h3>
            <p>이제 정의된 모델과 준비된 데이터를 사용하여 학습을 시작합니다. <strong>옵티마이저(Optimizer)</strong>는 모델의 가중치를 업데이트하는 방법을 결정하고(여기서는 Adam 사용), <strong>손실 함수(Loss Function)</strong>는 모델의 예측이 정답과 얼마나 다른지 측정합니다(여기서는 MSE Loss 사용).</p>
            <p>여러 번의 <strong>에포크(Epoch)</strong>를 반복하며 모델은 점차 오차를 줄여나가고, 테스트 데이터로 평가하여 가장 성능이 좋았던 모델을 저장합니다.</p>
            <pre><code class="language-python">
# --- 학습 설정 ---
NUM_EPOCHS = 70 # 총 에포크 수 (조절 가능)
BEST_MODEL_PATH = 'best_steering_model_xy.pth' # 최고 모델 저장 경로
best_loss = float('inf') # 최고 성능 기록용 (낮을수록 좋음)

# 옵티마이저 (Adam) 및 손실 함수 (MSE) 정의
optimizer = optim.Adam(model.parameters())
criterion = torch.nn.MSELoss() # Mean Squared Error Loss

print(f"<span class='emoji'>🚀</span> 모델 학습 시작! (총 에포크: {NUM_EPOCHS})")

# --- 학습 루프 ---
for epoch in range(NUM_EPOCHS):
    # 훈련
    model.train()
    running_train_loss = 0.0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device) # 데이터 GPU 이동
        optimizer.zero_grad() # 기울기 초기화
        outputs = model(images) # 예측
        loss = criterion(outputs, labels) # 손실 계산
        loss.backward() # 역전파 (기울기 계산)
        optimizer.step() # 가중치 업데이트
        running_train_loss += loss.item() * images.size(0) # 배치 손실 누적

    epoch_train_loss = running_train_loss / len(train_loader.dataset)

    # 평가
    model.eval()
    running_test_loss = 0.0
    with torch.no_grad(): # 평가 시 기울기 계산 불필요
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            running_test_loss += loss.item() * images.size(0)

    epoch_test_loss = running_test_loss / len(test_loader.dataset)

    print(f'Epoch [{epoch+1:02d}/{NUM_EPOCHS}], Train Loss: {epoch_train_loss:.6f}, Test Loss: {epoch_test_loss:.6f}')

    # 최고 모델 저장
    if epoch_test_loss < best_loss:
        print(f'  <span class="emoji">✨</span> Test Loss 개선! ({best_loss:.6f} -> {epoch_test_loss:.6f}). 모델 저장: {BEST_MODEL_PATH}')
        torch.save(model.state_dict(), BEST_MODEL_PATH)
        best_loss = epoch_test_loss

print(f"<span class='emoji'>🎉</span> 모델 학습 완료! 최종 최고 Test Loss: {best_loss:.6f}")

            </code></pre>
            <h3><span class="emoji">📊</span> 3.5. 학습 결과: 똑똑해진 JetBot의 두뇌!</h3>
            <p>학습 로그를 통해 훈련 손실과 테스트 손실이 점차 감소하는 것을 확인할 수 있습니다. 학습이 성공적으로 완료되면, 가장 낮은 테스트 손실을 기록한 시점의 모델 가중치가 <code>best_steering_model_xy.pth</code> 파일로 저장됩니다. 이 파일이 바로 TensorRT 최적화 단계에서 사용될 결과물입니다!</p>
        </div>

        <hr>

        <!-- TensorRT Optimization Section (Content from previous good answer, adjusted with emojis) -->
        <div class="section" id="tensorrt-optimization">
             <h2><span class="emoji">⚡</span> 4. TensorRT 최적화: 모델, 더 빠르게!</h2>
             <p>학습된 PyTorch 모델을 Jetson Nano와 같은 엣지 디바이스에서 실시간으로 빠르게 실행하기 위해 <strong>NVIDIA TensorRT</strong>로 최적화합니다. 모델의 연산 그래프를 분석하고 압축하여 추론 속도를 높이고(<span class="emoji">🚀</span>) 메모리 사용량을 줄이는 과정입니다.</p>
             <p>여기서는 <code>torch2trt</code> 라이브러리를 사용하여 이 변환 과정을 쉽게 수행합니다.</p>

             <h3><span class="emoji">⭐</span> 4.1. 최적화 목표</h3>
             <ul>
                 <li>PyTorch 모델(<code>.pth</code>)을 고성능 TensorRT 엔진으로 변환합니다.</li>
                 <li><code>torch2trt</code> 라이브러리를 활용하여 변환 과정을 간소화합니다.</li>
                 <li><strong>FP16 (반정밀도)</strong> 모드를 사용하여 추가적인 속도 향상 및 메모리 절약을 달성합니다. (Jetson Nano는 FP16 연산을 효율적으로 처리합니다!)</li>
                 <li>최적화된 모델을 <code>best_steering_model_xy_trt.pth</code> 파일로 저장합니다.</li>
             </ul>

            <h3><span class="emoji">🔧</span> 4.2. <code>torch2trt</code> 설치 (필요시)</h3>
            <p>JetBot에 <code>torch2trt</code>가 설치되어 있지 않다면, Jetson 터미널에서 다음 명령어를 실행하여 설치합니다.</p>
            <pre><code class="language-bash">
# (Jetson Terminal에서 실행)
cd $HOME
git clone https://github.com/NVIDIA-AI-IOT/torch2trt
cd torch2trt
sudo python3 setup.py install
echo "torch2trt 설치 완료! 터미널 재시작 또는 환경 재로드가 필요할 수 있습니다."
            </code></pre>

             <h3><span class="emoji">🔄</span> 4.3. 모델 로드 및 TensorRT 변환</h3>
             <p>학습된 PyTorch 모델 가중치(<code>best_steering_model_xy.pth</code>)를 다시 로드한 후, 모델 입력과 동일한 형태의 '더미 데이터'를 만들어 <code>torch2trt</code> 함수에 전달합니다. 이 더미 데이터는 TensorRT가 모델의 연산 흐름을 파악하고 최적화하는 데 사용됩니다.</p>
             <pre><code class="language-python">
import torchvision
import torch
from torch2trt import torch2trt, TRTModule # TensorRT 변환 및 로드용

# --- 모델 구조 정의 및 가중치 로드 ---
model = torchvision.models.resnet18(pretrained=False)
model.fc = torch.nn.Linear(512, 2)
model.load_state_dict(torch.load('best_steering_model_xy.pth')) # 학습된 가중치 로드
device = torch.device('cuda')
model = model.cuda().eval().half() # GPU 이동, 평가 모드, FP16 변환
print("<span class='emoji'>✅</span> PyTorch 모델 로드 및 FP16 준비 완료.")

# --- TensorRT 변환용 더미 데이터 생성 ---
dummy_data = torch.zeros((1, 3, 224, 224)).to(device).half() # (배치, 채널, 높이, 너비)
print("<span class='emoji'>🧪</span> 더미 입력 데이터 생성 완료.")

# --- TensorRT 변환 실행 (시간 소요) ---
print("<span class='emoji'>⏳</span> TensorRT 변환 시작... (몇 분 정도 걸릴 수 있습니다)")
try:
    model_trt = torch2trt(model, [dummy_data], fp16_mode=True) # FP16 모드 활성화
    print("<span class='emoji'>🎉</span> TensorRT 변환 성공!")
except Exception as e:
    print(f"<span class='emoji'>❌</span> TensorRT 변환 중 오류 발생: {e}")
    # 오류 발생 시 이후 코드 실행을 막기 위해 여기서 중단하거나 예외 처리 필요
    raise e

# --- 최적화된 모델 저장 ---
TRT_MODEL_PATH = 'best_steering_model_xy_trt.pth'
torch.save(model_trt.state_dict(), TRT_MODEL_PATH)
print(f"<span class='emoji'>💾</span> 최적화된 TensorRT 모델 저장 완료: {TRT_MODEL_PATH}")
            </code></pre>
            <p>이제 훨씬 가볍고 빨라진(<span class="emoji">⚡</span>) <code>best_steering_model_xy_trt.pth</code> 모델이 준비되었습니다! 실시간 데모에서 이 모델을 사용할 것입니다.</p>
        </div>

        <hr>

        <!-- Live Demo Section (Content from previous good answer, adjusted with emojis) -->
        <div class="section" id="live-demo">
              <h2><span class="emoji">🚀</span> 5. 실시간 데모: JetBot, 도로를 달리다!</h2>
              <p>드디어 프로젝트의 하이라이트! 최적화된 TensorRT 모델을 사용하여 JetBot이 실시간 카메라 영상을 보며 스스로 도로를 따라 주행하는 데모를 실행합니다. JetBot이 정말로 똑똑해졌는지 확인해볼 시간입니다! <span class="emoji">🤩</span></p>

              <h3><span class="emoji">⭐</span> 5.1. 데모 목표</h3>
              <ul>
                  <li>최적화된 TensorRT 모델(<code>best_steering_model_xy_trt.pth</code>) 로드</li>
                  <li>실시간 카메라 영상(<span class="emoji">📹</span>) 입력 받기</li>
                  <li>영상을 모델 입력 형식에 맞게 <strong>전처리(preprocess)</strong></li>
                  <li>TensorRT 모델로 목표 (x, y) 좌표 <strong>추론(inference)</strong></li>
                  <li>추론 결과를 바탕으로 JetBot의 <strong>조향(steering)</strong> 및 <strong>속도(speed)</strong> 계산 (PD 제어 활용)</li>
                  <li>계산된 값으로 JetBot 모터(<span class="emoji">⚙️</span>)를 제어하여 실시간 주행!</li>
                  <li>사용자가 주행 파라미터를 조절할 수 있는 <strong>위젯 인터페이스</strong> 제공 (<span class="emoji">🎛️</span>)</li>
              </ul>

              <h3><span class="emoji">💾</span> 5.2. TensorRT 모델 로드</h3>
              <p>이전에 저장한 최적화된 TensorRT 모델(<code>.pth</code>)을 <code>TRTModule</code>을 사용하여 로드합니다.</p>
              <pre><code class="language-python">
import torch
from torch2trt import TRTModule # TensorRT 모델 로드용

# --- TensorRT 모델 로드 ---
model_trt = TRTModule()
TRT_MODEL_PATH = 'best_steering_model_xy_trt.pth'
try:
    model_trt.load_state_dict(torch.load(TRT_MODEL_PATH))
    print(f"<span class='emoji'>✅</span> TensorRT 모델 '{TRT_MODEL_PATH}' 로드 성공!")
except Exception as e:
    print(f"<span class='emoji'>❌</span> TensorRT 모델 로드 실패: {e}")
    print("   모델 파일이 올바른 경로에 있는지, 변환이 성공했는지 확인하세요.")
    # 필요한 경우 여기서 실행 중단
    raise e

device = torch.device('cuda') # GPU 사용 확인
# model_trt = model_trt.to(device) # TRTModule은 보통 자동으로 GPU 사용
            </code></pre>

              <h3><span class="emoji">🖼️➡️🧠</span> 5.3. 이미지 전처리 함수 정의</h3>
              <p>카메라 이미지를 모델이 이해할 수 있는 형식(텐서, 정규화 등)으로 변환하는 함수입니다. 학습 시 사용했던 전처리 방식과 동일해야 합니다.</p>
              <pre><code class="language-python">
import torchvision.transforms as transforms
import torch.nn.functional as F
import cv2 # OpenCV for color conversion
import PIL.Image
import numpy as np

# --- 이미지 전처리 함수 ---
# ImageNet 정규화 파라미터 (GPU, FP16)
mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()
std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()

def preprocess(image_bgr):
    # BGR (OpenCV) -> RGB (PIL)
    image_rgb = PIL.Image.fromarray(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB))
    # PIL -> Tensor, GPU 이동, FP16 변환
    image_tensor = transforms.functional.to_tensor(image_rgb).to(device).half()
    # 정규화 (mean 빼고 std로 나누기)
    image_tensor.sub_(mean[:, None, None]).div_(std[:, None, None])
    # 배치 차원 추가 [C, H, W] -> [1, C, H, W]
    return image_tensor[None, ...]

print("<span class='emoji'>✅</span> 이미지 전처리 함수 'preprocess' 정의 완료.")
            </code></pre>

              <h3><span class="emoji">🤖</span><span class="emoji">📷</span> 5.4. 카메라 및 로봇 인터페이스 설정</h3>
              <p>JetBot의 카메라와 모터를 제어하기 위한 객체를 만들고, 실시간 카메라 영상을 Jupyter Lab에 표시합니다.</p>
              <pre><code class="language-python">
from IPython.display import display, HTML
import ipywidgets
import traitlets
from jetbot import Camera, bgr8_to_jpeg, Robot

# --- 카메라 및 로봇 초기화 ---
camera = Camera(width=224, height=224) # 학습 시와 동일 크기
robot = Robot() # 모터 제어용
image_widget = ipywidgets.Image(width=camera.width, height=camera.height) # 영상 표시 위젯

# --- 카메라 영상 연결 ---
traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)

# --- 위젯 표시 ---
display(ipywidgets.VBox([
    HTML("<h2><span class='emoji'>👀</span> 실시간 카메라 영상</h2>"),
    image_widget
]))
print("<span class='emoji'>✅</span> 카메라 및 로봇 객체 준비 완료.")
            </code></pre>

             <h3><span class="emoji">🎛️</span> 5.5. 주행 제어 파라미터 슬라이더</h3>
             <p>JetBot의 주행 스타일을 미세 조정하기 위한 슬라이더입니다. 환경에 맞게 값을 조절하며 최적의 주행 성능을 찾아보세요!</p>
             <ul>
                 <li><span class="emoji">💨</span> <strong>속도 (Speed):</strong> 기본 주행 속도</li>
                 <li><span class="emoji">🧭</span> <strong>조향 게인 (Kp):</strong> 목표와의 각도 오차에 얼마나 민감하게 반응할지 (비례 제어)</li>
                 <li><span class="emoji">⚖️</span> <strong>조향 안정성 (Kd):</strong> 급격한 방향 전환을 얼마나 억제할지 (미분 제어, 진동 감소)</li>
                 <li><span class="emoji">🔧</span> <strong>조향 편향 (Bias):</strong> 로봇이 한쪽으로 쏠리는 현상 보정</li>
             </ul>
             <blockquote class="tip"><span class="emoji">💡</span> <strong>튜닝 팁:</strong> 낮은 속도에서 시작하여, Kp 값을 조절해 경로를 따라가도록 합니다. 이후 Kd 값을 조금씩 높여 진동을 잡고, 필요시 Bias로 쏠림 현상을 보정하세요.</blockquote>
            <pre><code class="language-python">
# --- 제어 파라미터 슬라이더 ---
speed_gain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.01, value=0.18, description='<span class="emoji">💨</span> 속도', readout_format='.2f')
steering_gain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.005, value=0.08, description='<span class="emoji">🧭</span> 조향 게인 (Kp)', readout_format='.3f')
steering_dgain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.001, value=0.02, description='<span class="emoji">⚖️</span> 조향 안정성 (Kd)', readout_format='.3f')
steering_bias_slider = ipywidgets.FloatSlider(min=-0.3, max=0.3, step=0.01, value=0.0, description='<span class="emoji">🔧</span> 조향 편향', readout_format='.2f')

# --- 슬라이더 표시 ---
display(ipywidgets.VBox([
    HTML("<h2><span class='emoji'>⚙️</span> 주행 파라미터 조절</h2>"),
    speed_gain_slider,
    steering_gain_slider,
    steering_dgain_slider,
    steering_bias_slider
]))
            </code></pre>

             <h3><span class="emoji">📊</span> 5.6. 내부 상태 시각화 슬라이더 (디버깅용)</h3>
             <p>모델의 예측값과 최종 제어값을 실시간으로 확인하면 주행 상태를 이해하고 파라미터를 튜닝하는 데 도움이 됩니다.</p>
             <pre><code class="language-python">
# --- 내부 상태 시각화 슬라이더 ---
x_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='예측 X', disabled=True, readout_format='.2f')
steering_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='최종 조향', disabled=True, readout_format='.2f')

# --- 시각화 슬라이더 표시 ---
display(ipywidgets.VBox([
    HTML("<h4><span class='emoji'>📈</span> 내부 예측 및 제어값 (참고용)</h4>"),
    x_slider,
    steering_slider
]))
            </code></pre>

             <h3><span class="emoji">🧠➡️🚗</span> 5.7. 핵심 실행 함수 (`execute`) 정의 및 연결</h3>
             <p>카메라에서 새 프레임이 들어올 때마다 호출되어, 이미지 처리 → 모델 추론 → 제어 계산 → 모터 구동까지의 전 과정을 수행하는 핵심 함수입니다. <strong>PD 제어</strong> 로직을 사용하여 목표 지점을 향해 부드럽게 조향합니다.</p>
             <pre><code class="language-python">
import numpy as np
import time

# --- PD 제어용 변수 ---
angle = 0.0
angle_last = 0.0

# --- 메인 실행 함수 ---
def execute(change):
    global angle, angle_last # 전역 변수 사용 선언

    # 1. 새 이미지 가져오기
    image = change['new']
    if image is None: return # 이미지가 없으면 중단

    try:
        # 2. 이미지 전처리 및 모델 추론
        xy = model_trt(preprocess(image)).detach().float().cpu().numpy().flatten()
        x = xy[0]
        y = (0.5 - xy[1]) / 2.0 # y값 변환 (거리 가중치 역할)

        # 3. 내부 상태 업데이트 (시각화용)
        x_slider.value = float(x)

        # 4. 제어 파라미터 가져오기
        speed_value = speed_gain_slider.value
        Kp = steering_gain_slider.value
        Kd = steering_dgain_slider.value
        bias = steering_bias_slider.value

        # 5. 목표 각도 계산 (arctan2)
        # y값이 매우 작거나 0에 가까워지면 angle이 불안정해질 수 있으므로 작은 값(epsilon) 더하기
        epsilon = 1e-6
        angle = np.arctan2(x, y + epsilon)

        # 6. PD 제어 계산
        pid = Kp * angle + Kd * (angle - angle_last)

        # 7. 최종 조향 값 계산 (PD + Bias)
        steering_value = pid + bias
        steering_slider.value = float(steering_value) # 시각화 업데이트

        # 8. 좌/우 모터 속도 계산 및 범위 제한 [0.0, 1.0]
        left_motor_value = max(min(speed_value + steering_value, 1.0), 0.0)
        right_motor_value = max(min(speed_value - steering_value, 1.0), 0.0)

        # 9. 모터 구동
        robot.left_motor.value = left_motor_value
        robot.right_motor.value = right_motor_value

        # 10. 현재 각도 저장 (다음 계산용)
        angle_last = angle

    except Exception as e:
        print(f"<span class='emoji'>❌</span> 실행 중 오류 발생: {e}")
        # 오류 발생 시 로봇 정지 등의 안전 조치 추가 가능
        # robot.stop()

# --- 실행 준비 및 시작 ---
try:
    camera.unobserve_all() # 이전 콜백 모두 해제
    print("<span class='emoji'>🧹</span> 이전 카메라 콜백 정리 완료.")
except:
    pass # 처음 실행 시에는 오류 발생 안 함

# 카메라 값 변경 시 execute 함수 호출하도록 연결
camera.observe(execute, names='value')

print("<span class='emoji'>🟢</span> 실시간 주행 시스템 활성화!")
print("<span class='warning'><span class='emoji'>⚠️</span> 경고: JetBot이 움직입니다! 주변 공간을 확인하고 트랙 위에 두세요!</span>")
print("   주행을 멈추려면 아래 '주행 중지' 코드를 실행하세요.")
            </code></pre>

             <h3><span class="emoji">🛑</span> 5.8. 주행 시작 및 비상 정지</h3>
             <p>위의 마지막 코드 셀을 실행하면 JetBot이 주행을 시작합니다! 슬라이더를 조절하며 최적의 주행을 찾아보세요.</p>
             <blockquote class="warning"><span class="emoji">🚨</span> <strong>비상 정지:</strong> 문제가 발생하거나 주행을 즉시 멈추고 싶을 때는 아래 코드 셀을 실행하세요! 카메라 연결을 끊고 모터를 정지시킵니다.</blockquote>
             <pre><code class="language-python">
# --- 주행 중지 코드 ---
print("<span class='emoji'>⏳</span> 주행 중지 시도...")
try:
    camera.unobserve(execute, names='value') # 콜백 연결 해제
    print("<span class='emoji'>🔌</span> 카메라 콜백 연결 해제 완료.")
    time.sleep(0.1) # 잠시 대기
    robot.stop() # 모터 정지!
    print("<span class='emoji'>🛑</span> 로봇 모터 정지 완료.")
    # camera.stop() # 필요시 카메라 정지
except Exception as e:
    print(f"<span class='emoji'>❓</span> 중지 중 오류 발생 (무시 가능): {e}")

# 모터 값 확인 (정지되었는지)
print(f"현재 모터 값: Left={robot.left_motor.value:.2f}, Right={robot.right_motor.value:.2f}")
            </code></pre>
        </div>

        <hr>

        <!-- Conclusion Section (Content from previous good answer, adjusted with emojis) -->
        <div class="section" id="conclusion">
            <h2><span class="emoji">🏁</span> 6. 프로젝트 완료 및 향후 과제</h2>
            <p>축하드립니다! <span class="emoji">🥳</span> 여러분은 데이터 수집부터 모델 학습, TensorRT 최적화, 그리고 실시간 자율 주행 데모까지, JetBot 도로 주행 프로젝트의 전 과정을 성공적으로 완수하셨습니다!</p>
            <p>이 프로젝트를 통해 컴퓨터 비전, 딥러닝 회귀, 로봇 제어, 엣지 AI 최적화 등 다양한 기술을 직접 경험하고 통합하는 능력을 키우셨기를 바랍니다. 여기서 얻은 지식과 경험은 앞으로 더 복잡하고 흥미로운 AI 및 로봇 공학 프로젝트를 수행하는 데 훌륭한 밑거름이 될 것입니다.</p>
            <p><strong><span class="emoji">🚀</span> 다음 도전 과제들:</strong></p>
            <ul>
                <li><strong><span class="emoji">🌍</span> 다양한 환경 정복:</strong> 더 복잡한 트랙(교차로, 오르막/내리막), 야외 환경, 다양한 조명 조건에서 데이터를 추가 수집하고 모델의 강건성을 더욱 높여보세요.</li>
                <li><strong><span class="emoji">💡</span> 새로운 모델 시도:</strong> ResNet 외에도 MobileNetV2, EfficientNet-Lite 등 Jetson 환경에 더 적합한 경량 모델을 사용하여 성능과 속도를 비교 분석해보세요.</li>
                <li><strong><span class="emoji">📈</span> 정교한 제어 기법:</strong> PID 제어의 적분(I) 항을 추가하거나, 모델 예측 불확실성을 고려한 제어, 경로 계획 알고리즘(A*, RRT 등)과 결합하여 더욱 안정적이고 지능적인 주행 제어를 구현해보세요.</li>
                <li><strong><span class="emoji">🚦</span><span class="emoji">🚧</span> 기능 확장:</strong> 객체 감지 모델(신호등, 표지판, 장애물 인식)과 결합하여, 단순한 라인 트레이싱을 넘어 주변 상황을 인지하고 반응하는 진정한 자율 주행 시스템으로 발전시켜보세요!</li>
            </ul>
            <p>JetBot과 함께하는 여러분의 AI 여정을 응원합니다! <span class="emoji">🌟</span></p>
        </div>

    </div> <!-- /container -->

    <!-- Three.js 라이브러리 로딩 (Importmap 사용 - esm.sh CDN) -->
    <script type="importmap">
      {
        "imports": {
          "three": "https://esm.sh/three@0.163.0",
          "three/addons/": "https://esm.sh/three@0.163.0/examples/jsm/"
        }
      }
    </script>

    <!-- 3D 뷰어 실행 스크립트 -->
    <script type="module">
        // 필요한 모듈 임포트
        import * as THREE from 'three';
        import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
        import { OrbitControls } from 'three/addons/controls/OrbitControls.js';

        let scene, camera, renderer, controls, model;
        const container = document.getElementById('viewer-container');
        const loadingIndicator = document.getElementById('loading-indicator');
        const modelUrl = 'https://github.com/hwkims/jetbot.kr/raw/refs/heads/main/jetbot.glb'; // GitHub Raw URL 사용

        function init3DViewer() {
            try {
                // 1. Scene 생성
                scene = new THREE.Scene();
                scene.background = new THREE.Color(0xe9ecef); // CSS 배경색과 맞춤

                // 2. Camera 생성
                camera = new THREE.PerspectiveCamera(
                    50, // FOV
                    container.clientWidth / container.clientHeight, // Aspect Ratio
                    0.1, // Near Plane
                    100 // Far Plane
                );
                camera.position.set(1.8, 1.5, 3.0); // 카메라 위치 조정

                // 3. Renderer 생성
                renderer = new THREE.WebGLRenderer({ antialias: true, alpha: true }); // 배경 투명도 사용 가능
                renderer.setSize(container.clientWidth, container.clientHeight);
                renderer.setPixelRatio(window.devicePixelRatio);
                renderer.toneMapping = THREE.ACESFilmicToneMapping; // 좀 더 사실적인 톤 매핑
                renderer.outputColorSpace = THREE.SRGBColorSpace; // 출력 색 공간
                container.appendChild(renderer.domElement);

                // 4. Controls 생성 (OrbitControls)
                controls = new OrbitControls(camera, renderer.domElement);
                controls.enableDamping = true;
                controls.dampingFactor = 0.08;
                controls.screenSpacePanning = false;
                controls.minDistance = 0.5;
                controls.maxDistance = 10;
                controls.target.set(0, 0.4, 0); // 모델 중심 부근을 바라보도록 설정
                controls.update();

                // 5. Lighting 설정 (개선)
                scene.add(new THREE.AmbientLight(0xffffff, 0.8)); // 기본 주변광

                const hemiLight = new THREE.HemisphereLight( 0xffffff, 0xaaaaaa, 1.0 ); // 하늘/땅 빛
                hemiLight.position.set( 0, 20, 0 );
                scene.add( hemiLight );

                const dirLight = new THREE.DirectionalLight(0xffffff, 2.5); // 주광원 강화
                dirLight.position.set(5, 10, 7);
                scene.add(dirLight);

                const dirLightBack = new THREE.DirectionalLight(0xffffff, 0.8); // 후면 보조광
                dirLightBack.position.set(-5, -5, -7);
                scene.add(dirLightBack);

                // 6. GLTF 모델 로더 설정 및 로드
                const loader = new GLTFLoader();
                loader.load(
                    modelUrl, // GitHub Raw URL 사용
                    // 로드 성공 시
                    function (gltf) {
                        model = gltf.scene;

                        // 모델 자동 중앙 정렬 및 스케일 조정
                        const box = new THREE.Box3().setFromObject(model);
                        const size = box.getSize(new THREE.Vector3());
                        const center = box.getCenter(new THREE.Vector3());
                        const maxDim = Math.max(size.x, size.y, size.z);
                        const scale = 1.8 / maxDim; // 뷰어에 적당한 크기로 스케일 조정

                        model.scale.set(scale, scale, scale);
                        model.position.sub(center.multiplyScalar(scale)); // 모델 원점을 기준으로 중앙 정렬
                        model.position.y += size.y * scale / 2; // 모델 바닥을 원점 근처로 이동

                        scene.add(model);
                        loadingIndicator.style.display = 'none'; // 로딩 완료 후 숨김
                        console.log('✅ JetBot 3D 모델 로드 및 설정 완료!');
                    },
                    // 로딩 진행 중
                    function (xhr) {
                        if (xhr.lengthComputable) {
                            const percentLoaded = (xhr.loaded / xhr.total) * 100;
                            loadingIndicator.textContent = `⏳ 모델 로딩 중... ${Math.round(percentLoaded)}%`;
                        } else {
                             loadingIndicator.textContent = `⏳ 모델 로딩 중... (크기 미확인)`;
                        }
                    },
                    // 로드 오류 시
                    function (error) {
                        console.error('❌ 3D 모델 로드 오류:', error);
                        loadingIndicator.innerHTML = '😭 모델 로드 실패!<br/>(URL 또는 네트워크 확인)';
                        loadingIndicator.style.color = 'red';
                    }
                );

                // 7. 창 크기 변경 시 처리
                window.addEventListener('resize', onWindowResize, false);

                // 8. 애니메이션 루프 시작
                animate();

            } catch (error) {
                 console.error("💥 3D 뷰어 초기화 중 오류 발생:", error);
                 loadingIndicator.innerHTML = '😭 뷰어 초기화 실패!<br/>(브라우저 호환성 확인)';
                 loadingIndicator.style.color = 'red';
            }
        }

        // 창 크기 변경 핸들러
        function onWindowResize() {
            if (!camera || !renderer || !container) return;
            camera.aspect = container.clientWidth / container.clientHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(container.clientWidth, container.clientHeight);
        }

        // 애니메이션 루프
        function animate() {
            requestAnimationFrame(animate);
            controls.update(); // OrbitControls 업데이트
            renderer.render(scene, camera); // 씬 렌더링
        }

        // DOM 로드 완료 후 3D 뷰어 초기화
        document.addEventListener('DOMContentLoaded', init3DViewer);

    </script>

</body>
</html>
