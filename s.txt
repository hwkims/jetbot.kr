발표 대본: JetBot 및 UGV 플랫폼 기반 AI 로봇 제어 시스템 개발 및 탐구

안녕하십니까? 오늘 'JetBot 및 UGV 플랫폼 기반 AI 로봇 제어 시스템 개발 및 탐구: 기본 주행부터 LLM/VLLM 통합까지'라는 주제로 발표하게 된 [발표자 이름]입니다.

최근 인공지능 기술, 특히 대규모 언어 모델의 눈부신 발전은 로봇 공학 분야와 융합되며 새로운 가능성을 열어가고 있습니다. 이러한 시대적 흐름 속에서, 교육 및 연구 커뮤니티에서는 AI 로봇 기술에 대한 접근성을 높이기 위한 노력이 계속되고 있으며, NVIDIA의 JetBot과 같은 저비용 오픈소스 플랫폼이 중요한 역할을 하고 있습니다. 동시에 Ollama와 같은 도구의 등장은 비교적 성능이 낮은 엣지 디바이스에서도 강력한 언어 모델을 구동할 수 있게 함으로써, 로봇이 주변 환경을 더 깊이 이해하고 인간과 상호작용할 수 있는 잠재력을 크게 확장시켰습니다. 더 나아가, 실제 환경에서의 복잡한 임무 수행을 위해서는 UGV Beast와 같은 보다 견고하고 고성능의 로봇 플랫폼이 요구되고 있습니다.

이러한 배경 하에, 본 연구는 저렴하고 교육적인 JetBot 플랫폼을 시작점으로 하여 기본적인 AI 주행 기능을 구현하고, 단계적으로 최신 AI 기술인 LLM과 VLLM을 통합하여 로봇의 지능을 고도화하는 과정을 탐구했습니다. 최종적으로는 고성능 UGV 플랫폼에서의 자연어 상호작용 시스템 구축 가능성까지 모색하는 것을 목표로 삼았습니다.

본격적인 연구 내용에 앞서, 저희가 활용한 핵심 플랫폼과 기술 스택을 간략히 소개하겠습니다. 첫 번째 주력 플랫폼인 JetBot은 NVIDIA Jetson Nano 보드를 핵심 두뇌로 사용하는 저비용 오픈소스 AI 로봇입니다. 합리적인 가격과 쉬운 설정, 그리고 웹 브라우저 기반의 Jupyter Notebook 프로그래밍 환경은 교육 및 연구 초기 단계에 매우 적합합니다. JetBot 프로젝트에서는 주로 Python 언어를 사용했으며, 실시간 제어 및 사용자 인터페이스 구축을 위해 FastAPI, WebSockets, Streamlit과 같은 최신 웹 기술 스택을 적극 활용했습니다. AI 모델로는 초기 주행 과제를 위해 직접 수집한 데이터로 학습시킨 ResNet 기반의 딥러닝 모델을 사용했으며, 이는 Hugging Face를 통해 공개되어 있습니다. 이후 LLM 및 VLLM 통합 단계에서는 Ollama 프레임워크를 기반으로 로컬 환경에서 Gemma와 IBM Granite 모델을 구동했습니다. 또한, 사용자 경험 향상을 위해 Edge TTS를 이용한 한국어 음성 피드백과 웹 브라우저의 Web Speech API를 활용한 음성 명령 인식 기능도 통합했습니다.

두 번째 플랫폼인 UGV Beast는 Raspberry Pi와 ESP32의 효율적인 듀얼 컨트롤러 구조를 가진 고성능 추적형 로봇입니다. ESP32는 모터 PID 제어, IMU 센서 처리 등 저수준 실시간 제어를 담당하고, Raspberry Pi는 ROS 2 노드 실행과 같은 고수준 연산을 수행하여 시스템 부하를 분산시킵니다. 견고한 알루미늄 합금 섀시와 독립 서스펜션 시스템은 복잡한 지형에서의 주행 성능을 보장하며, 2자유도 팬틸트 카메라와 피카티니 레일 등은 높은 확장성을 제공합니다. 이 플랫폼은 로봇 운영체제 표준인 ROS 2를 공식 지원하며, Python 기반의 웹 제어 애플리케이션과 다양한 JupyterLab 튜토리얼을 제공하여 개발 편의성을 높였습니다. 본 연구의 후반부에서는 이 UGV 플랫폼의 잠재력을 활용하여 Ollama 기반 LLM을 통합, 자연어 상호작용 기능을 구현하는 것을 장기적인 목표로 설정하고 관련 계획을 수립했습니다.

이제 저희가 수행한 연구 단계를 순서대로 설명드리겠습니다.

첫 번째 단계는 JetBot의 기본적인 AI 주행 능력을 확보하는 것이었습니다. 저희는 세 가지 핵심 과제, 즉 도로의 중앙선을 따라가는 '도로 따라가기', 특정 객체인 컵라면을 인식하고 따라가는 '객체 따라가기', 그리고 전방의 장애물을 인지하고 회피하는 '충돌 회피' 기능을 구현했습니다. 이를 위해 각 시나리오에 맞춰 JetBot의 카메라로 직접 이미지 데이터셋을 구축했습니다. 예를 들어, 충돌 회피 기능 구현을 위해서는 로봇 시점에서 '안전한 주행 공간(Free)' 이미지와 '장애물로 막힌 공간(Blocked)' 이미지를 다수 수집했습니다. 이렇게 구축된 데이터셋을 사용하여 ResNet-18 아키텍처 기반의 이미지 분류 모델을 학습시켰습니다. 학습된 모델은 실시간으로 입력되는 카메라 영상을 분석하여 현재 상황을 판단하고, 그 결과에 따라 미리 정의된 JetBot의 모터 제어 명령(예: 직진, 좌회전, 정지)을 실행합니다. 개발된 기능들은 실제 JetBot을 주행시키며 성능을 검증했으며, 시연 영상과 학습 데이터셋, 그리고 학습된 모델은 연구 커뮤니티와의 공유를 위해 YouTube와 Hugging Face 플랫폼에 공개하였습니다.

두 번째 단계에서는 JetBot에 대규모 언어 모델, 즉 LLM을 통합하여 한 단계 더 높은 수준의 자율 제어 및 상호작용을 시도했습니다. 목표는 실시간 카메라 영상을 LLM이 '이해'하고, 그 분석 결과를 바탕으로 JetBot을 제어하며, 동시에 사용자에게 상황을 음성으로 설명해주는 시스템을 구축하는 것이었습니다. 시스템 아키텍처는 다음과 같이 설계되었습니다. JetBot은 카메라 영상을 실시간으로 WebSocket 프로토콜을 통해 서버로 스트리밍하고, 서버로부터 WebSocket을 통해 제어 명령을 수신합니다. 서버 측에서는 FastAPI 프레임워크를 이용해 JetBot 및 웹 클라이언트와의 WebSocket 연결을 효율적으로 관리합니다. JetBot으로부터 영상 프레임을 수신하면, 이를 Ollama를 통해 로컬에서 실행 중인 Gemma 언어 모델의 API에 전달하여 영상 분석을 요청합니다. Gemma 모델이 "There is an obstacle ahead."와 같은 텍스트 분석 결과를 반환하면, 서버는 이 텍스트를 파싱하여 '좌회전'과 같은 구체적인 JetBot 제어 명령으로 변환하고, 다시 WebSocket을 통해 JetBot에 전송합니다. 동시에, 이 분석 결과는 Microsoft의 Edge TTS 서비스를 통해 자연스러운 한국어 음성("앞에 장애물이 있어요.")으로 합성되어 웹 클라이언트 사용자에게 실시간으로 전달됩니다. 이 실험을 통해 LLM이 로봇의 시각 인지 및 판단 과정에 직접 관여하는 프로토타입 시스템을 성공적으로 구현하였으며, 기본적인 웹 인터페이스를 통해 원격 제어 및 상황 모니터링이 가능함을 확인했습니다.

세 번째 단계에서는 이미지 분석 능력에 더욱 특화된 비전 언어 모델, VLLM을 도입하여 JetBot의 제어 시스템을 한층 더 고도화했습니다. 이를 위해 Ollama에서 IBM의 Granite Vision 모델을 활용했습니다. 핵심적인 개선 사항은 먼저, 사용자 인터페이스를 기존의 단순 HTML/JavaScript 기반에서 Python 생태계와 긴밀하게 연동되는 Streamlit 프레임워크 기반으로 전환한 것입니다. 이를 통해 개발 생산성과 사용자 인터페이스의 상호작용성을 크게 향상시킬 수 있었습니다. 기능적으로는 단순 자율 주행을 넘어, 사용자가 방향키 등으로 직접 제어하는 '수동 모드', VLLM이 현재 카메라에 보이는 장면을 상세히 묘사하는 '설명 모드', 그리고 사용자가 "Turn around."와 같이 텍스트로 자유롭게 명령을 내리면 VLLM이 이를 해석하여 제어하는 '사용자 정의 모드' 등 다중 제어 모드를 구현했습니다. 각 모드에서 VLLM이 최적의 성능을 발휘하도록 유도하기 위해, 각기 다른 프롬프트를 신중하게 설계하는 프롬프트 엔지니어링 과정이 중요했습니다. 예를 들어, 자율 주행 모드에서는 "장애물을 피해서 앞으로 나아가려면 어떤 명령(forward, backward, left, right, stop 중 하나)을 내려야 할까?"와 같은 질문 형식의 프롬프트를 사용했습니다. 추가적으로 웹 브라우저의 표준 기능인 Web Speech API를 활용하여 키보드 입력 대신 음성으로 명령을 내릴 수 있는 기능도 통합했습니다. 또한 VLLM의 이미지 분석 능력을 보여주는 부가적인 예시로서, 사람의 얼굴 사진을 입력받아 VLLM이 32가지 특징을 분석하고 이를 바탕으로 간략한 관상 평을 JSON 형식으로 출력하는 Streamlit 데모 애플리케이션도 개발했습니다. 이 단계를 통해, VLLM을 활용하여 시각적으로 풍부하고 사용자와 더욱 긴밀하게 상호작용할 수 있는 로봇 제어 인터페이스를 구축할 수 있다는 가능성을 확인했습니다.

마지막 네 번째 단계에서는 JetBot보다 한 단계 높은 성능과 복잡성을 가진 UGV Beast 플랫폼을 심층적으로 분석하고, 여기에 LLM을 통합하여 자연어 기반의 상호작용 시스템을 구현하기 위한 구체적인 로드맵을 수립했습니다. UGV Beast 플랫폼은 견고한 하드웨어 설계, 강력한 구동 모터, 그리고 무엇보다 로봇 개발 표준인 ROS 2 생태계를 적극적으로 활용할 수 있다는 큰 장점을 가지고 있습니다. 기본적으로 웹 기반 제어 인터페이스와 함께 조이스틱 제어, SLAM을 이용한 2D/3D 맵핑, 자율 내비게이션 등 다양한 ROS 2 기반 예제 코드를 제공하여 개발 시작점이 매우 유리합니다. 저희는 이 강력한 플랫폼에 Ollama 기반의 LLM을 통합하는 것을 목표로 설정했습니다. 사용자가 "앞으로 3미터 이동해", "현재 배터리 전압은 얼마야?" 와 같은 자연어 명령을 텍스트나 음성으로 내리면, 로봇이 이를 정확히 이해하고 해당 동작을 수행하거나 관련 정보를 음성 또는 텍스트로 응답하는 시나리오를 구상했습니다. LLM 실행 환경은 Raspberry Pi 5의 성능을 고려하여, 가능하면 네트워크 내의 별도 고성능 PC에서 Ollama 서버를 운영하는 방식을 권장합니다. 통합 방식으로는 기존에 제공되는 Flask 기반 웹 애플리케이션을 수정하여 LLM API를 호출하는 방법과, ROS 2 시스템과의 통합성을 높이기 위해 자연어 명령을 처리하고 표준 ROS 2 메시지(예: /cmd_vel)를 발행하거나 관련 서비스를 호출하는 전용 ROS 2 노드를 새로 개발하는 방안을 모두 고려하고 있습니다. 이 과정에서의 핵심 개발 내용은 효과적인 프롬프트 엔지니어링, LLM 응답의 정확한 파싱 및 로봇 동작 매핑 로직 구현, 그리고 LLM이 로봇의 현재 상태(예: 센서 값, 위치 정보)를 인지하고 응답에 반영할 수 있도록 관련 정보를 프롬프트에 동적으로 통합하는 기술이 될 것입니다. 물론, LLM의 예측 불가능성을 고려한 견고한 오류 처리 및 안전 메커니즘 설계 또한 필수적으로 고려되어야 합니다.

종합적으로, 본 연구를 통해 저희는 다음과 같은 주요 성과를 달성했습니다. JetBot 플랫폼에서는 기본적인 AI 주행 기능 구현부터 시작하여, LLM(Gemma) 기반의 실시간 영상 분석 및 음성 피드백 프로토타입, 그리고 VLLM(IBM Granite) 기반의 다중 모드 제어 시스템과 인터랙티브 Streamlit UI까지 성공적으로 구축했습니다. 이는 Jetson Nano와 같은 저비용 엣지 플랫폼 환경에서도 최신 AI 모델들을 효과적으로 통합하여 로봇의 지능을 향상시킬 수 있음을 실증적으로 보여줍니다. UGV 플랫폼에 대해서는 그 특성과 ROS 2 기반 개발 환경을 면밀히 분석하였으며, 향후 자연어 상호작용 시스템을 구축하기 위한 구체적인 LLM 통합 계획을 수립했습니다. 공통적으로는 이러한 단계별 프로젝트 수행을 통해 AI 로봇 제어 기술 전반에 대한 깊이 있는 학습과 실제 구현 경험을 확보할 수 있었으며, FastAPI, Streamlit과 같은 최신 웹 기술 활용 능력 또한 크게 향상되었습니다. 무엇보다 중요한 것은, 본 연구 과정에서 개발된 모든 소스 코드와 설정 방법, 그리고 학습된 모델 및 데이터셋을 GitHub와 Hugging Face를 통해 투명하게 공개함으로써 연구 결과의 재현성을 높이고 커뮤니티와의 지식 공유 및 발전에 기여하고자 노력했다는 점입니다.

물론, 프로젝트를 진행하며 여러 기술적 어려움과 한계점에도 직면했습니다. Jetson Nano와 Raspberry Pi의 제한된 연산 능력은 특히 고해상도 영상 처리와 복잡한 LLM 추론을 동시에 수행할 때 실시간 성능 저하, 즉 지연(latency) 문제로 이어질 수 있습니다. 또한 Wi-Fi 기반의 네트워크 통신은 환경에 따라 지연 시간이 변동하거나 불안정해질 수 있으며, 이는 특히 실시간 제어가 중요한 로봇 애플리케이션에서는 민감한 문제입니다. 근본적으로 LLM과 VLLM 자체가 가지는 응답의 부정확성 또는 비일관성, 소위 '환각(Hallucination)' 현상은 로봇 제어의 안전성과 예측 가능성에 직접적인 위협이 될 수 있습니다. 원하는 로봇 행동을 일관되고 정확하게 유도하기 위한 효과적인 프롬프트를 설계하는 과정 역시 상당한 시행착오와 노하우를 요구하는 어려운 작업이었습니다.

이러한 어려움들을 바탕으로 몇 가지 개선점도 도출했습니다. 현재 구현된 장애물 회피 로직은 단순히 '장애물'이라는 키워드를 감지하면 미리 정해진 행동(예: 무조건 좌회전)을 하는 수준에 머물러 있습니다. 향후에는 LLM/VLLM의 출력을 더욱 정교하게 분석하여 장애물의 위치, 크기, 회피 가능한 경로 등을 종합적으로 판단하고, 상황에 맞는 동적인 회피 기동을 수행하도록 로직을 고도화할 필요가 있습니다. 사용자 경험을 향상시키기 위한 인터페이스 디자인과 피드백 메커니즘의 지속적인 개선도 중요하며, 통신 오류나 모델 응답 실패, 예기치 않은 로봇 상태 등 다양한 예외 상황에 대한 보다 견고한 에러 핸들링 로직 보강도 필수적입니다. 특히 UGV 프로젝트에서 계획 중인 LLM 통합 시에는, 단순히 스크립트를 연동하는 수준을 넘어 ROS 2의 표준 메시징, 서비스, 액션 시스템을 적극적으로 활용하여 시스템 전체의 모듈성과 확장성을 확보하는 것이 중요할 것입니다.

결론적으로, 본 연구는 접근성 높은 오픈소스 로봇 플랫폼인 JetBot과 고성능 UGV Beast를 기반으로, 기본적인 AI 주행 기술부터 시작하여 최신 AI 기술인 LLM, VLLM, TTS 등을 단계적으로 통합하며 로봇 제어 시스템을 발전시켜 나가는 과정을 성공적으로 수행했습니다. 이를 통해 저비용 엣지 디바이스 환경에서도 최신 AI 기술을 로봇 제어에 효과적으로 접목할 수 있다는 실질적인 가능성을 확인했으며, 동시에 LLM/VLLM을 로보틱스 분야에 적용할 때 고려해야 할 성능, 신뢰성, 안전성 측면의 현재 기술적 한계점과 과제들을 명확히 인식할 수 있었습니다. 또한, 오픈소스 하드웨어와 소프트웨어, 그리고 최신 웹 기술의 효과적인 결합을 통해 사용자 친화적이고 상호작용적인 AI 로봇 시스템을 구축하는 구체적인 방안을 제시했다는 점에서 의의를 찾을 수 있습니다.

향후 연구 방향으로는, 첫째, 현재 계획 단계에 있는 UGV Beast 플랫폼과 Ollama LLM의 통합을 실제로 구현하고 그 성능을 면밀히 평가하는 것이 최우선 과제입니다. 둘째, 단순 반응형 회피를 넘어 VFH나 DWA와 같은 정교한 장애물 회피 및 경로 계획 알고리즘을 도입하거나, 강화학습 기반의 자율적인 회피 능력 개발을 탐구할 계획입니다. 셋째, 특정 객체에 대한 인식 및 장기 추적 능력을 고도화하고, 이를 바탕으로 보다 복잡한 상호작용 시나리오를 구현하는 연구를 진행하고자 합니다. 넷째, 텍스트와 음성뿐만 아니라 제스처 인식 등 다양한 입력 방식을 통합한 멀티모달 로봇 인터페이스 연구도 흥미로운 방향이 될 것입니다. 다섯째, 모델 경량화 기법(Quantization, Pruning 등) 적용, 코드 최적화, 그리고 가능하다면 하드웨어 가속 기능 활용을 통해 시스템의 실시간 성능을 개선하는 노력이 지속적으로 필요합니다. 마지막으로, 실험실 환경을 벗어나 실제 생활 환경과 유사한 복잡하고 예측 불가능한 환경에서의 장시간 구동 테스트를 통해 시스템의 안정성과 강인성(robustness)을 확보하는 것이 중요할 것입니다.

이상으로 저의 발표를 마치겠습니다. 긴 시간 경청해주셔서 대단히 감사합니다. 질문이 있으시면 편하게 해주시기 바랍니다.
