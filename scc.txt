PPT 제목: JetBot과 Vision LLM: 범용 AI 로봇을 향한 여정
부제: 보고 판단하는 AI에서 스스로 움직이는 로봇까지
발표자: hwkims (또는 실제 발표자 이름)
날짜: [발표 날짜]

슬라이드 1: 표지

제목: JetBot과 Vision LLM: 범용 AI 로봇을 향한 여정

부제: 보고 판단하는 AI에서 스스로 움직이는 로봇까지

발표자: [발표자 이름/팀 이름 - 예: hwkims]

소속 (선택 사항): K-Digital Training (또는 실제 소속)

(선택 사항) 이미지: JetBot과 VLLM 상호작용 이미지 또는 프로젝트 로고

슬라이드 2: 목차

프로젝트 개요: 왜, 무엇을?

프로젝트 팀 구성 및 역할: 누가?

프로젝트 수행 절차 및 방법: 어떻게? (단계별 여정)

프로젝트 수행 결과: 그래서 무엇을 만들었나?

자체 평가 및 향후 비전: 성찰과 미래

슬라이드 3: 01 프로젝트 개요 - 시작 동기

제목: 01 프로젝트 개요

부제: 자율주행 기술, 더 가까이 다가갈 수 없을까?

출발점:

자율주행 기술 보급을 위한 컴퓨터 비전 연구 필요성

기존 자율주행차의 높은 가격 및 제한적 보급률

핵심 질문:

합리적 비용으로 다양한 상황에 대처 가능한 자율주행 시스템 구현 방법?

(이미지): 전 세계 자동차 보유 대수 통계 그래프, 테슬라 판매량 관련 그래프

슬라이드 4: 01 프로젝트 개요 - 기존 방식의 한계

제목: 01 프로젝트 개요

부제: 객체는 보지만, 상황은 이해하지 못한다면?

주요 접근 (CNN 기반):

객체 탐지(차, 사람, 차선 등)에는 강점

한계점:

맥락 이해 부족: 상황 전체 파악 어려움 (예: Mark Rober 영상)

돌발 상황 취약: 예측 불가능한 시나리오 대응 능력 미흡

데이터 의존성: 학습되지 않은 새로운 상황 대처 한계

센서 비용/제약: LIDAR 등 고가 센서 부담 및 특정 환경 취약성

(이미지): Mark Rober 영상 썸네일, LIDAR 장단점 및 비용 언급

슬라이드 5: 01 프로젝트 개요 - 새로운 대안: Vision LLM

제목: 01 프로젝트 개요

부제: VLLM: 보고, 이해하고, 소통하는 AI

Vision LLM (VLLM) 특징:

시각 정보 + 언어 모델 결합

이미지 내용/맥락 이해, 질의응답, 지시 수행 가능

도입 이유:

맥락적 이해력: 이미지 전체 상황 파악 가능성

유연한 상호작용: 자연어 명령 이해 및 시각 정보 연계

범용성(General Purpose): 특정 작업 넘어 다양한 시각 기반 임무 수행 잠재력 -> 프로젝트 핵심 방향

프로젝트 목표:

JetBot 플랫폼에 VLLM을 통합, 범용 시각 지능 로봇 가능성 탐구

(이미지): VLLM 작동 방식 개념도, 범용성 상징 이미지

슬라이드 6: 02 프로젝트 팀 구성 및 역할

제목: 02 프로젝트 팀 구성 및 역할

부제: 1인 개발자의 다 역할 수행

팀 구성: 개인 프로젝트 (hwkims)

수행 역할:

기획/설계 (목표 설정, VLLM 도입 결정)

하드웨어 (JetBot 조립, 설정)

소프트웨어 개발 (Python, FastAPI, Streamlit, WebSockets)

AI/ML 엔지니어링 (Ollama 설정/연동, 프롬프트 엔지니어링)

테스트/디버깅

문서화/공유 (GitHub)

(이미지): 다양한 역할을 상징하는 아이콘 일러스트

슬라이드 7: 03 프로젝트 수행 절차 및 방법 - 1단계

제목: 03 프로젝트 수행 절차 및 방법 (1/3)

부제: JetBot 기본기 다지기

주요 활동:

JetBot 하드웨어 조립 및 OS/기본 S/W 설치

네트워크 설정 및 원격 개발 환경 구축

기본 AI 주행 기능 구현 (jetbot.kr):

데이터 수집 (도로, 객체, 장애물)

CNN 기반 모델 활용 (Hugging Face)

기능: 경로/객체 따라가기, 충돌 회피

결과물 공유 (Hugging Face 데이터셋/모델)

결과: 플랫폼 제어 및 기초 AI 모델 적용 경험 확보

(이미지): JetBot 조립 사진, Jupyter Notebook 캡처, Hugging Face 로고

슬라이드 8: 03 프로젝트 수행 절차 및 방법 - 2단계

제목: 03 프로젝트 수행 절차 및 방법 (2/3)

부제: LLM 통합 시도와 현실적 어려움

주요 활동:

Ollama + Gemma(LLM) 로컬 환경 설정

LLM 기반 제어 시스템 프로토타입 개발 (jetbot_gemma):

FastAPI 백엔드: 영상 수신, Ollama 연동, 기본 제어 로직

WebSockets: 실시간 양방향 통신

Edge TTS: 분석 결과 음성 피드백

HTML/JS 웹 UI

결과 및 난관:

LLM 활용 상황 인지/설명 가능성 확인

실시간 제어의 어려움 봉착: LLM 응답을 안정적 제어 명령으로 변환 난이도 높음

(이미지): Ollama 로고, FastAPI 로고, jetbot_gemma UI 스크린샷

슬라이드 9: 03 프로젝트 수행 절차 및 방법 - 3단계 (Pivot)

제목: 03 프로젝트 수행 절차 및 방법 (3/3)

부제: VLLM 핵심 능력 검증: '판단' 능력 테스트

방향 전환 이유: JetBot 실시간 제어 구현 난이도로 인해, VLLM의 핵심 역량(시각 이해 및 판단) 우선 검증으로 목표 수정

주요 활동:

Ollama + IBM Granite Vision(VLLM) 설정

'GoStop' 판단 앱 개발 (gostop):

플랫폼: Streamlit 활용 웹 애플리케이션

기능: 이미지 업로드 -> VLLM 분석 -> '직진(Go)'/'정지(Stop)' 판단 결과 시각적 제시 (🔴/🟢)

결과: VLLM의 기본적인 시각 정보 기반 판단 능력 검증 완료 -> 향후 제어 연동의 기초 마련

(이미지): IBM Granite Vision 로고, Streamlit 로고, gostop 앱 인터페이스 스크린샷

슬라이드 10: 04 프로젝트 수행 결과 - 기초 다지기 (jetbot.kr)

제목: 04 프로젝트 수행 결과 (1/3)

결과물: 기본 AI 주행 기능 구현 (jetbot.kr)

기능: 도로 따라가기, 객체(컵라면) 추종, 충돌 회피

기술: 주로 CNN 기반

성과: JetBot 플랫폼 활용 및 기초 AI 적용 완료

공유: Hugging Face 데이터셋/모델 공개

(이미지/영상): JetBot 주행 영상 캡처/클립, Hugging Face 페이지 스크린샷

슬라이드 11: 04 프로젝트 수행 결과 - LLM 연동 시도 (jetbot_gemma)

제목: 04 프로젝트 수행 결과 (2/3)

결과물: LLM 기반 제어 프로토타입 (jetbot_gemma)

기능: 실시간 영상 분석, 원격 제어 UI, 음성 피드백 (TTS)

기술: Ollama(Gemma), FastAPI, WebSockets

성과: LLM의 로봇 인지/상호작용 향상 가능성 시연

한계: 실시간 제어 안정성 및 응답 속도 이슈

(이미지): jetbot_gemma 웹 인터페이스 스크린샷

슬라이드 12: 04 프로젝트 수행 결과 - VLLM 판단력 검증 (gostop)

제목: 04 프로젝트 수행 결과 (3/3)

결과물: 'GoStop' - VLLM 시각 판단력 테스트 앱 (gostop)

기능: 이미지 업로드 시 VLLM이 '직진/정지' 상황 판단

기술: Ollama(Granite Vision), Streamlit

핵심 성과: VLLM이 시각 정보를 이해하고 기본적인 판단을 내릴 수 있음을 성공적으로 검증

의의: 향후 VLLM 판단 기반 로봇 제어 연구의 실증적 기반 마련

(이미지): gostop Streamlit 앱 인터페이스 스크린샷 (판단 결과 포함)

슬라이드 13: 05 자체 평가 - 성과 및 교훈

제목: 05 자체 평가 및 향후 비전 (1/2)

부제: 성찰: 무엇을 얻고 무엇을 배웠나?

잘된 점 (Strengths):

저비용 플랫폼(JetBot) 기반 VLLM 통합 및 가능성 확인

gostop 앱 통한 VLLM 판단 능력 성공적 검증 (중요 성과)

FastAPI, Streamlit 등 활용, 빠른 프로토타이핑 및 UI 개발

오픈소스 활용 및 결과물 공유

어려웠던 점 & 배운 점 (Weaknesses & Lessons):

엣지 디바이스 성능 한계: VLLM 실시간 구동 어려움 (별도 서버 고려)

실시간 제어 난이도: VLLM 응답 -> 안정적 로봇 명령 변환 어려움 (Pivot 계기)

VLLM 신뢰성/제어: 환각(Hallucination), 부정확성 -> 안전 필수

프롬프트 엔지니어링 중요성 재확인

슬라이드 14: 05 향후 비전 - 판단에서 행동으로

제목: 05 자체 평가 및 향후 비전 (2/2)

부제: VLLM의 판단력을 로봇의 움직임으로

핵심 과제: 검증된 VLLM의 '판단' 능력을 **'실제 로봇 행동'**으로 연결

안정적이고 신뢰성 있는 제어 알고리즘 개발

엣지 디바이스 성능 최적화 (실시간성 확보)

궁극적 목표: 범용 AI 로봇

단순 주행 넘어, 상황 이해 기반 복합 임무 수행

예시: 자율 주행 + 택배 상자 운반/전달 로봇

지향점: 예측 불가능한 환경에서 다재다능하게 활동하는 로봇 지능 구현

연구 방향: 고차원 추론, 멀티태스킹, 실세계 상호작용 능력 강화

(이미지): 로봇이 택배를 나르거나 집안일을 돕는 미래 컨셉 아트

슬라이드 15: Q&A

제목: 감사합니다 (Thank You)

부제: 질의응답 (Q&A)

(선택 사항) 연락처 정보: GitHub, Email, 프로젝트 링크 등
