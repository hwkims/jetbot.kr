오케이, 알겠습니다. 노래 가사 대신, 제공해주신 GitHub README 내용을 바탕으로 상세하고 전문적인 발표용 대본을 작성해 드리겠습니다. 각 슬라이드별 내용을 포함하여 논문 발표 수준으로 구성했습니다.

---

**JetBot 및 UGV 플랫폼 기반 AI 로봇 제어 시스템 개발 및 탐구**

**발표자:** [발표자 이름]
**소속:** [소속 기관/팀 이름 (선택 사항)]
**날짜:** [발표 날짜]

---

**(슬라이드 1: 제목 슬라이드)**

*   **제목:** JetBot 및 UGV 플랫폼 기반 AI 로봇 제어 시스템 개발 및 탐구: 기본 주행부터 LLM/VLLM 통합까지
*   **부제:** 오픈소스 하드웨어와 최신 AI 모델을 활용한 단계별 로봇 지능화 연구
*   **발표자:** [발표자 이름]
*   **소속:** [소속 기관/팀 이름]
*   **(로고: JetBot, UGV, Ollama, FastAPI, Streamlit 등 관련 로고 삽입)**

**(발표자 스크립트 - 슬라이드 1)**
"안녕하십니까? 오늘 저는 'JetBot 및 UGV 플랫폼 기반 AI 로봇 제어 시스템 개발 및 탐구'라는 주제로 발표하게 된 [발표자 이름]입니다. 본 연구는 저렴하고 접근성 높은 오픈소스 로봇 플랫폼인 NVIDIA JetBot과 보다 고성능의 UGV Beast를 활용하여, 기본적인 AI 주행 기술부터 최신 대규모 언어 모델(LLM) 및 비전 언어 모델(VLLM)을 통합한 지능형 로봇 제어 시스템을 단계적으로 개발하고 그 가능성을 탐구하는 과정을 다룹니다."

---

**(슬라이드 2: 연구 배경 및 목표)**

*   **연구 배경:**
    *   AI 기술의 발전과 로봇 공학의 융합 가속화
    *   교육 및 연구 목적의 저비용 고성능 AI 로봇 플랫폼 수요 증대 (NVIDIA JetBot)
    *   엣지 디바이스에서의 LLM/VLLM 구동 및 로봇 제어 적용 가능성 대두 (Ollama 등)
    *   보다 복잡한 환경 탐색 및 상호작용을 위한 고성능 로봇 플랫폼 활용 필요성 (UGV Beast)
*   **연구 목표:**
    *   JetBot 플랫폼을 활용한 기본적인 AI 기반 로봇 주행 기능(도로/객체 추종, 충돌 회피) 구현 및 검증
    *   JetBot에 로컬 LLM(Gemma)을 통합하여 실시간 영상 분석 기반 제어 및 음성 피드백 시스템 프로토타이핑
    *   JetBot에 로컬 VLLM(IBM Granite)을 통합하여 시각 정보 기반의 다중 모드(자율, 설명, 사용자 정의 등) 제어 시스템 구축 및 인터페이스 개발 (Streamlit)
    *   고성능 UGV 플랫폼(Beast PI ROS2)의 특성 분석 및 향후 자연어 기반 상호작용을 위한 LLM(Ollama) 통합 계획 수립
*   **(이미지: JetBot 사진, UGV Beast 사진, LLM/VLLM 로고 등)**

**(발표자 스크립트 - 슬라이드 2)**
"최근 AI 기술, 특히 대규모 언어 모델의 발전은 로봇 공학 분야에도 새로운 가능성을 열어주고 있습니다. 교육 및 연구 커뮤니티에서는 NVIDIA JetBot과 같은 저비용 오픈소스 플랫폼을 통해 AI 로봇 기술에 대한 접근성이 높아졌습니다. 또한 Ollama와 같은 도구는 엣지 디바이스에서도 LLM을 실행할 수 있게 하여, 로봇이 환경을 더 깊이 이해하고 상호작용할 잠재력을 보여줍니다. 더 나아가 UGV Beast와 같은 고성능 플랫폼은 실제 환경에서의 복잡한 임무 수행 가능성을 제시합니다.
따라서 본 연구는 JetBot을 이용한 기본 AI 주행 구현에서 시작하여, LLM과 VLLM을 단계적으로 통합하며 로봇의 지능을 고도화하고, 최종적으로는 UGV 플랫폼에서의 자연어 상호작용 통합 가능성까지 탐구하는 것을 목표로 설정하였습니다."

---

**(슬라이드 3: 활용 플랫폼 및 기술 스택 - JetBot)**

*   **JetBot 플랫폼:**
    *   하드웨어: NVIDIA Jetson Nano 개발 보드, 모터, 카메라, Wi-Fi 모듈
    *   특징: 저비용($250 미만), 오픈소스, 교육적, 웹 브라우저 기반 프로그래밍 (Jupyter Notebook)
*   **JetBot 프로젝트 기술 스택:**
    *   프로그래밍 언어: Python
    *   웹 프레임워크: FastAPI, Streamlit
    *   실시간 통신: WebSockets
    *   프론트엔드: HTML/CSS/JavaScript, Streamlit UI
    *   AI/ML:
        *   딥러닝 모델: ResNet-18 기반 (Road/Object/Collision) - Hugging Face 호스팅
        *   LLM/VLLM: Ollama 기반 (gemma-2b, granite-code-3b-instruct-V2.1/gemma3:4b, IBM granite3.2-vision) *수정 필요: README에 따라 gemma3:4b, IBM granite3.2-vision 명시*
        *   TTS: Edge TTS (한국어 지원 - Jenny Neural)
        *   ASR: Web Speech API (브라우저 기반)
    *   컨테이너화: Docker (JetBot 초기 설정 시 활용)
*   **(이미지: JetBot 구성 요소, Jetson Nano 보드, 관련 기술 로고)**

**(발표자 스크립트 - 슬라이드 3)**
"첫 번째 핵심 플랫폼인 JetBot은 NVIDIA Jetson Nano를 두뇌로 사용하는 저비용 오픈소스 AI 로봇입니다. 조립과 설정이 비교적 용이하며, Jupyter Notebook 환경을 통해 웹 브라우저에서 직접 프로그래밍하고 제어할 수 있다는 장점이 있습니다.
JetBot 프로젝트들에서는 Python을 주 언어로 사용했으며, 실시간 제어 및 웹 인터페이스 구축을 위해 FastAPI, WebSockets, Streamlit과 같은 최신 웹 기술을 활용했습니다. AI 모델은 초기 주행 과제에서는 직접 수집한 데이터로 학습시킨 ResNet 기반 모델을 사용했으며(Hugging Face 공유), 이후 LLM 및 VLLM 통합 단계에서는 Ollama를 통해 Gemma와 IBM Granite 모델을 로컬에서 실행했습니다. 사용자 피드백을 위해 Edge TTS를, 음성 명령 인식을 위해 Web Speech API를 연동했습니다."

---

**(슬라이드 4: 활용 플랫폼 및 기술 스택 - UGV Beast PI ROS2)**

*   **UGV Beast PI ROS2 플랫폼:**
    *   하드웨어: Raspberry Pi 4B/5 (Host), ESP32 (Slave), 듀얼 기어 모터 (인코더 포함), IMU, OLED, 2DOF 팬틸트 카메라, LED 스포트라이트, 3S 배터리 UPS, 알루미늄 합금 섀시, 독립 서스펜션
    *   특징: 고성능 듀얼 컨트롤러 구조, ROS 2 지원, 견고한 설계, 확장성(피카티니 레일), 오프로드 주행 능력
*   **UGV 프로젝트 기술 스택:**
    *   OS: Raspberry Pi OS (Debian Bookworm)
    *   미들웨어: ROS 2 (Robot Operating System 2)
    *   호스트 프로그래밍: Python (Flask 웹앱 제공), C++ (ROS 2 노드)
    *   슬레이브 프로그래밍: (ESP32 제어 로직 - 상세 내용 미포함)
    *   통신: I2C, UART, Wi-Fi (AP/STA 모드)
    *   AI 통합 계획: Ollama (로컬 LLM) 연동 예정
*   **(이미지: UGV Beast 상세 사진, Raspberry Pi, ESP32, ROS 2 로고)**

**(발표자 스크립트 - 슬라이드 4)**
"두 번째 플랫폼인 UGV Beast는 Raspberry Pi와 ESP32의 듀얼 컨트롤러 구조를 가진 고성능 추적형 로봇입니다. ESP32가 모터 PID 제어, IMU 등 저수준 제어를 담당하고, Raspberry Pi가 고수준 연산 및 ROS 2 노드를 실행하여 효율적인 분산 처리가 가능합니다. 견고한 알루미늄 섀시와 독립 서스펜션은 복잡한 지형 주행 능력을 제공하며, 2DOF 팬틸트 카메라와 확장 가능한 설계는 다양한 임무 수행에 적합합니다.
이 플랫폼은 ROS 2를 공식 지원하며, Python 기반의 웹 제어 애플리케이션과 JupyterLab 튜토리얼을 기본 제공합니다. 본 연구의 후반부에서는 이 UGV 플랫폼에 Ollama 기반 LLM을 통합하여 자연어 상호작용 기능을 구현하는 것을 목표로 설정하고 관련 계획을 수립했습니다."

---

**(슬라이드 5: 1단계 - 기본 AI 주행 기능 구현 (JetBot))**

*   **목표:** JetBot을 이용한 기본적인 시각 기반 주행 능력 확보
*   **과제:**
    *   도로 따라가기 (Road Following)
    *   객체 따라가기 (Object Following - 예: 컵라면)
    *   충돌 회피 (Collision Avoidance)
*   **수행 절차:**
    1.  데이터 수집: JetBot 카메라를 이용해 각 상황(도로, 객체 유/무, 장애물 유/무)에 맞는 이미지 데이터셋 구축
    2.  모델 학습: 수집된 데이터를 사용하여 이미지 분류 모델(ResNet-18 기반) 학습 (Jupyter Notebook 활용)
    3.  로직 구현: 학습된 모델의 추론 결과를 바탕으로 JetBot의 모터(전진, 좌/우 회전, 정지)를 제어하는 Python 스크립트 작성
    4.  테스트 및 검증: 실제 환경에서 JetBot을 주행시키며 성능 검증 및 파라미터 튜닝
*   **결과물:**
    *   동작 데모 영상 (YouTube Shorts 링크 제공됨)
    *   데이터셋 및 학습된 모델 (Hugging Face 공개)
*   **(이미지/영상: 데이터 수집 장면, 학습 과정 스크린샷, 실제 주행 데모 영상 (Shorts 임베드 또는 링크))**

**(발표자 스크립트 - 슬라이드 5)**
"프로젝트의 첫 단계에서는 JetBot의 기본적인 AI 주행 능력을 구현했습니다. 크게 세 가지 과제, 즉 도로 따라가기, 특정 객체(컵라면) 따라가기, 그리고 장애물 회피 기능을 개발했습니다.
각 과제별로 JetBot의 카메라를 사용하여 필요한 이미지 데이터를 직접 수집했습니다. 예를 들어, 충돌 회피의 경우 '자유 공간(Free)'과 '막힌 공간(Blocked)' 이미지를 수집했습니다. 이 데이터셋을 이용해 ResNet-18 기반의 이미지 분류 모델을 학습시켰습니다. 학습된 모델은 실시간 카메라 입력 이미지를 받아 '도로 중앙', '컵라면 보임', '장애물 있음' 등을 판단하고, 이 판단 결과에 따라 미리 정의된 JetBot의 움직임(직진, 회전, 정지)을 제어합니다.
개발된 기능들은 실제 JetBot 주행 테스트를 통해 검증되었으며, 관련 데모 영상과 학습에 사용된 데이터셋 및 모델은 YouTube와 Hugging Face를 통해 공개하였습니다."

---

**(슬라이드 6: 2단계 - LLM 기반 제어 시스템 (JetBot + Gemma))**

*   **목표:** JetBot에 LLM을 통합하여 실시간 영상 분석 기반의 자율 제어 및 상호작용 구현
*   **시스템 아키텍처:**
    *   **JetBot:** 카메라 영상 스트리밍 (WebSocket Client) → 제어 명령 수신 (WebSocket Client)
    *   **서버 (PC/Jetson):**
        *   FastAPI 백엔드: WebSocket 서버 역할 (JetBot 및 Web Client 연결 관리)
        *   영상 수신 및 처리: JetBot으로부터 영상 프레임 수신
        *   Ollama 연동: 수신된 영상 프레임을 Ollama API (gemma3:4b 모델)에 전달하여 분석 요청
        *   명령 생성: Ollama 응답(텍스트 분석 결과)을 파싱하여 JetBot 제어 명령(예: '전진', '좌회전') 생성
        *   명령 전송: 생성된 명령을 WebSocket 통해 JetBot으로 전송
        *   TTS 연동: 분석 결과를 Edge TTS API로 보내 음성 생성 후 Web Client로 전송
    *   **Web Client (Browser):** 제어 UI 제공, 실시간 영상 및 분석 결과(텍스트/음성) 확인 (HTML/JS/CSS)
*   **구현 내용:**
    *   Ollama 및 gemma3:4b 모델 설치 및 설정
    *   FastAPI, WebSocket 기반 실시간 통신 시스템 구축
    *   영상 프레임 인코딩/디코딩 및 Ollama API 연동 로직 구현
    *   LLM 응답 기반 기본 제어 로직 설계 (예: 'obstacle' 단어 포함 시 좌회전)
    *   Edge TTS 연동으로 한국어 음성 피드백 기능 추가
*   **결과물:** LLM 기반 실시간 영상 분석 및 제어 프로토타입, 기본 웹 인터페이스
*   **(이미지: 시스템 아키텍처 다이어그램, 웹 인터페이스 스크린샷, Ollama/Gemma 로고)**

**(발표자 스크립트 - 슬라이드 6)**
"두 번째 단계에서는 JetBot에 대규모 언어 모델(LLM)을 통합하는 실험을 진행했습니다. 목표는 실시간 카메라 영상을 LLM이 분석하고, 그 결과를 바탕으로 JetBot을 제어하며 사용자에게 음성 피드백을 제공하는 시스템을 구축하는 것이었습니다.
시스템 아키텍처는 다음과 같습니다. JetBot은 카메라 영상을 WebSocket을 통해 서버로 스트리밍하고, 서버로부터 제어 명령을 수신합니다. 서버 측에서는 FastAPI를 이용해 WebSocket 연결을 관리하며, JetBot에서 받은 영상 프레임을 Ollama를 통해 실행되는 Gemma 모델에 전달하여 분석을 요청합니다. Gemma 모델이 '앞에 장애물이 보인다'와 같은 텍스트 분석 결과를 반환하면, 서버는 이를 파싱하여 '좌회전'과 같은 JetBot 제어 명령으로 변환하고 다시 WebSocket을 통해 JetBot에 전송합니다. 동시에, 분석 결과는 Edge TTS를 통해 한국어 음성으로 변환되어 웹 클라이언트 사용자에게 전달됩니다.
이를 통해 LLM이 로봇의 '눈'과 '뇌'의 일부 역할을 수행하는 프로토타입 시스템을 구현하였고, 기본적인 웹 인터페이스를 통해 제어 및 모니터링이 가능하도록 했습니다."

---

**(슬라이드 7: 3단계 - VLLM 기반 고급 제어 시스템 (JetBot + IBM Granite))**

*   **목표:** 비전 특화 LLM(VLLM)을 활용하여 JetBot의 시각 인지 및 제어 능력 고도화
*   **개선 사항:**
    *   **모델 변경:** 범용 LLM(Gemma) → 비전 LLM (Ollama + IBM granite3.2-vision)
    *   **프론트엔드 변경:** 기본 HTML/JS → Streamlit 기반 인터랙티브 웹 UI
    *   **기능 확장:**
        *   다중 제어 모드: 수동(Manual), 자율(Autonomous), 설명(Describe), 사용자 정의(Custom)
        *   프롬프트 엔지니어링: 각 모드에 최적화된 프롬프트 설계 (예: 자율 모드 - "장애물을 피해 앞으로 가려면 어떤 명령(forward, backward, left, right, stop)을 내려야 할까?")
        *   음성 명령 인식: Web Speech API 활용 (브라우저 기능)
    *   **부가 기능:** 얼굴 특징 분석 Streamlit 데모 앱 개발 (VLLM 활용 예시)
*   **시스템 아키텍처:** (2단계와 유사하나, 모델과 프론트엔드 변경 및 기능 확장)
    *   **서버 (FastAPI):** VLLM(Granite) API 연동 로직, 다중 모드 처리 로직 추가
    *   **프론트엔드 (Streamlit):** 실시간 영상 표시, 제어 버튼, 모드 선택, 분석 결과 시각화, 음성 입력 버튼 등 제공
*   **결과물:** VLLM 기반 다중 모드 제어 시스템, Streamlit 기반 인터랙티브 UI, 얼굴 분석 데모 앱
*   **(이미지: Streamlit UI 스크린샷 (다중 모드), 시스템 아키텍처 (변경점 강조), IBM Granite 로고, 얼굴 분석 데모 스크린샷)**

**(발표자 스크립트 - 슬라이드 7)**
"세 번째 단계에서는 이미지 분석에 더 특화된 비전 언어 모델(VLLM)인 IBM Granite 모델을 도입하여 JetBot의 제어 시스템을 한층 더 발전시켰습니다.
주요 개선점은 모델을 VLLM으로 변경하고, 사용자 인터페이스를 기존 HTML/JS에서 Python 기반의 Streamlit으로 전환하여 개발 편의성과 상호작용성을 높인 것입니다. 기능적으로는 단순 자율 주행 외에도, 사용자가 직접 제어하는 '수동 모드', VLLM이 현재 장면을 묘사하는 '설명 모드', 사용자가 텍스트로 자유롭게 명령을 내리는 '사용자 정의 모드' 등 다중 제어 모드를 구현했습니다. 각 모드의 성능을 최적화하기 위해 VLLM에 전달하는 프롬프트 설계에 노력을 기울였습니다. 또한 웹 브라우저의 Web Speech API를 활용하여 음성 명령 입력 기능도 추가했습니다.
부가적으로, VLLM의 이미지 분석 능력을 보여주는 예시로 얼굴 사진을 입력받아 관상을 분석하는 간단한 Streamlit 데모 앱도 개발했습니다. 이를 통해 VLLM 기반의 시각적으로 풍부하고 상호작용적인 로봇 제어 인터페이스를 구축할 수 있었습니다."

---

**(슬라이드 8: 4단계 - UGV 플랫폼 분석 및 LLM 통합 계획)**

*   **목표:** 고성능 UGV 플랫폼(Beast PI ROS2) 분석 및 자연어 상호작용을 위한 LLM 통합 로드맵 수립
*   **UGV Beast 플랫폼 분석:**
    *   강점: 견고한 하드웨어, 강력한 모터, ROS 2 생태계 활용 가능, 확장성, 듀얼 컨트롤러 효율성
    *   기본 기능: 웹 기반 제어, JupyterLab 튜토리얼, ROS 2 기반 제어/센싱 노드 제공 (조이스틱 제어, 매핑, 내비게이션 등)
*   **LLM (Ollama) 통합 계획:**
    *   **목표:** 자연어 명령(텍스트 또는 음성)을 이해하고 UGV의 동작(이동, 카메라 제어, 센서 정보 응답 등)으로 변환하는 시스템 구축
    *   **Ollama 실행 위치:** Raspberry Pi 5 직접 실행 (소형 모델) 또는 네트워크 내 별도 PC 실행 (권장)
    *   **통합 방식 (택 1 또는 혼합):**
        1.  기존 Flask 웹 애플리케이션 수정: 웹 UI 통해 입력받은 텍스트 → Ollama API 호출 → 응답 파싱 → 기존 UGV 제어 함수 호출
        2.  신규 ROS 2 노드 개발: `/natural_language_command` 토픽 구독 → Ollama API 호출 → 응답 파싱 → `/cmd_vel` 등 표준 ROS 2 토픽 발행 또는 서비스 호출
    *   **핵심 개발 내용:**
        *   프롬프트 엔지니어링: 사용자 명령을 UGV 동작 키워드로 변환 유도
        *   응답 파싱 및 동작 매핑 로직
        *   센서 데이터 통합: LLM 프롬프트에 실시간 센서 정보(예: 배터리 전압) 포함하여 상황 인지 능력 향상
        *   오류 처리 및 안전 메커니즘 고려
*   **(이미지: UGV Beast 주행 사진, ROS 2 Rviz 화면, Ollama 통합 구상도)**

**(발표자 스크립트 - 슬라이드 8)**
"마지막 단계에서는 JetBot보다 더 높은 성능과 복잡성을 가진 UGV Beast 플랫폼을 분석하고, 여기에 LLM을 통합하여 자연어 기반의 상호작용을 구현하기 위한 계획을 수립했습니다.
UGV Beast는 견고한 하드웨어, 강력한 구동계, 그리고 ROS 2를 지원한다는 큰 장점을 가지고 있습니다. 기본적으로 웹 기반 제어 인터페이스와 다양한 ROS 2 예제(조이스틱 제어, SLAM, 내비게이션 등)를 제공하여 개발 기반이 잘 갖추어져 있습니다.
여기에 Ollama 기반 LLM을 통합하는 것을 목표로 설정했습니다. 사용자가 "앞으로 가", "배터리 상태 알려줘" 와 같은 자연어 명령을 내리면, 로봇이 이를 이해하고 해당 동작을 수행하거나 정보를 응답하는 시나리오입니다. Ollama는 성능을 고려하여 네트워크 내 별도 PC에서 실행하는 것을 권장하며, 통합 방식으로는 기존 Flask 웹 앱을 수정하거나, ROS 2 생태계와 더 잘 통합되는 방식인 전용 ROS 2 노드를 개발하는 방안을 고려하고 있습니다. 핵심 과제는 효과적인 프롬프트 엔지니어링, LLM 응답 분석, 그리고 로봇의 실제 동작 및 센서와의 연동 로직 개발이 될 것입니다. 안전을 위한 오류 처리 역시 중요하게 고려해야 합니다."

---

**(슬라이드 9: 종합 결과 및 성과)**

*   **JetBot 기반 성과:**
    *   기본 AI 주행 기능(도로/객체 추종, 충돌 회피) 성공적 구현 및 데모/자료 공개 (YouTube, Hugging Face)
    *   LLM(Gemma) 기반 실시간 영상 분석 및 음성 피드백 제어 시스템 프로토타입 개발
    *   VLLM(IBM Granite) 기반 다중 모드 제어 시스템 및 인터랙티브 Streamlit UI 구축
    *   저비용 플랫폼에서 최신 AI 모델(LLM/VLLM/TTS) 통합 가능성 입증
*   **UGV 기반 성과:**
    *   플랫폼 특성 분석 및 ROS 2 기반 제어/센싱 환경 확인
    *   자연어 상호작용을 위한 LLM(Ollama) 통합 상세 계획 수립
*   **공통 성과:**
    *   단계별 프로젝트 진행을 통한 AI 로봇 제어 기술 심층 학습 및 구현 경험 확보
    *   FastAPI, WebSockets, Streamlit 등 최신 웹 기술 활용 능력 증대
    *   모든 프로젝트 소스 코드 및 설정 방법 GitHub 통해 공개하여 재현성 및 기여 가능성 확보
*   **(이미지: 각 프로젝트 결과물 대표 스크린샷/영상 모음, GitHub 저장소 스크린샷)**

**(발표자 스크립트 - 슬라이드 9)**
"본 연구를 통해 다음과 같은 주요 성과를 달성했습니다. JetBot 플랫폼에서는 기본적인 AI 주행 기능을 성공적으로 구현하고 관련 자료를 공개했으며, 더 나아가 LLM과 VLLM을 통합하여 실시간 영상 분석 기반의 제어 시스템 프로토타입과 다중 모드를 지원하는 고급 제어 시스템 및 인터랙티브 UI를 구축했습니다. 이는 저비용 엣지 플랫폼에서도 최신 AI 모델을 활용할 수 있음을 실증적으로 보여줍니다.
UGV 플랫폼에 대해서는 심층적인 분석을 통해 그 특성과 ROS 2 기반 개발 환경을 파악했으며, 향후 자연어 상호작용을 구현하기 위한 구체적인 LLM 통합 계획을 수립했습니다.
공통적으로는 단계별 프로젝트 수행을 통해 AI 로봇 제어 기술 전반에 대한 이해를 높이고 실제 구현 경험을 쌓을 수 있었습니다. 또한 FastAPI, Streamlit 등 최신 기술 스택 활용 능력을 향상시켰으며, 모든 결과물을 GitHub에 공개하여 지식 공유와 커뮤니티 기여를 실천했습니다."

---

**(슬라이드 10: 논의 (어려웠던 점 및 개선점))**

*   **기술적 어려움:**
    *   **성능 한계:** Jetson Nano 및 Raspberry Pi의 연산 능력 제약으로 인한 실시간 처리 지연(latency) 발생 가능성 (특히 고해상도 영상 처리 및 복잡한 LLM 추론 시)
    *   **네트워크 의존성:** Wi-Fi 환경에 따른 통신 지연 및 불안정성 (WebSocket 통신, Ollama API 호출)
    *   **LLM 신뢰성:** LLM/VLLM의 응답이 항상 정확하거나 일관되지 않음 (Hallucination). 이는 로봇 제어의 안전성 및 예측 가능성에 직접적 영향.
    *   **프롬프트 엔지니어링:** 원하는 로봇 행동을 일관되고 정확하게 유도하는 효과적인 프롬프트 설계의 어려움. 시행착오 필요.
*   **구현상 개선점:**
    *   **장애물 회피 로직:** 현재 구현된 회피 로직(단순 키워드 감지 후 고정 행동) 고도화 필요. LLM/VLLM 출력을 더 정교하게 분석하여 상황(장애물 위치, 크기, 회피 가능 경로 등)에 맞는 동적 회피 기동 구현.
    *   **UI/UX 개선:** 사용자 편의성 증대를 위한 인터페이스 디자인 및 피드백 메커니즘 개선.
    *   **에러 핸들링 강화:** 통신 오류, 모델 응답 실패, 예기치 않은 로봇 상태 등 다양한 예외 상황에 대한 안정적인 처리 로직 보강.
    *   **ROS 2 통합 심화 (UGV):** UGV 프로젝트에서 계획된 LLM 통합 시, 단순 스크립트 연동을 넘어 ROS 2의 메시징, 서비스, 액션 시스템을 적극 활용하여 모듈성과 확장성 확보.
*   **(이미지: 성능 측정 그래프(예시), LLM 오류 사례(예시), 복잡한 환경에서의 로봇 사진 등)**

**(발표자 스크립트 - 슬라이드 10)**
"프로젝트를 진행하며 몇 가지 어려움과 개선점에 직면했습니다. 기술적으로는 Jetson Nano와 Raspberry Pi의 제한된 연산 능력으로 인해 실시간 처리 속도에 한계가 있었으며, 특히 고해상도 영상과 복잡한 AI 모델을 동시에 처리할 때 지연이 발생했습니다. 또한 Wi-Fi 기반 통신은 환경에 따라 불안정할 수 있으며, LLM/VLLM 자체의 응답 신뢰성 문제는 로봇 제어의 안정성에 큰 영향을 미쳤습니다. 원하는 결과를 얻기 위한 프롬프트 엔지니어링 역시 상당한 노력이 필요한 부분이었습니다.
구현상으로는 특히 장애물 회피 로직이 현재 단순하게 구현되어 있어, 향후 LLM의 분석 결과를 바탕으로 더 지능적인 회피 기동을 수행하도록 개선할 필요가 있습니다. 사용자 인터페이스와 예외 처리 로직 또한 지속적인 개선이 요구됩니다. UGV 프로젝트의 경우, 계획된 LLM 통합 시 ROS 2 시스템과의 유기적인 연동을 통해 보다 견고하고 확장 가능한 구조를 만드는 것이 중요할 것입니다."

---

**(슬라이드 11: 결론 및 향후 연구)**

*   **결론:**
    *   JetBot 및 UGV 플랫폼을 활용하여 기본 AI 주행부터 LLM/VLLM 기반의 고급 제어 시스템까지 단계적으로 성공적인 개발 및 탐구를 수행함.
    *   저비용 엣지 디바이스 환경에서도 최신 AI 기술(LLM, VLLM, TTS)을 로봇 제어에 통합할 수 있는 실질적인 가능성을 확인함.
    *   오픈소스 하드웨어/소프트웨어와 최신 웹 기술의 결합을 통해 사용자 친화적이고 상호작용적인 AI 로봇 시스템 구축 방안 제시.
    *   LLM/VLLM의 로보틱스 적용 시 성능, 신뢰성, 안전성 측면의 현재 기술적 한계점 및 고려사항 명확화.
*   **향후 연구 방향:**
    *   **UGV Beast + Ollama 통합 구현:** 계획된 자연어 상호작용 시스템 실제 개발 및 성능 평가.
    *   **지능형 장애물 회피 및 경로 계획:** 정교한 알고리즘(예: VFH, DWA) 도입 또는 강화학습 기반 회피 능력 개발.
    *   **객체 인식 및 추적 고도화:** 특정 객체에 대한 장기 추적 및 상호작용 기능 개발.
    *   **다중 모달 상호작용:** 음성, 제스처 등 다양한 입력 방식을 통합한 로봇 인터페이스 연구.
    *   **성능 최적화:** 모델 경량화(Quantization, Pruning), 코드 최적화, 하드웨어 가속 활용 등을 통한 실시간성 확보.
    *   **다양한 AI 모델 테스트:** 최신 LLM/VLLM 모델 적용 및 성능 비교 평가.
    *   **실 환경 테스트 및 안정성 강화:** 실험실 환경을 넘어 실제 환경에서의 장시간 구동 테스트 및 안정성 확보.
*   **(이미지: 프로젝트 여정을 보여주는 요약 다이어그램, 미래 로봇 상상도 등)**

**(발표자 스크립트 - 슬라이드 11)**
"결론적으로, 본 연구는 JetBot과 UGV라는 오픈소스 플랫폼을 기반으로 AI 로봇 제어 시스템을 단계적으로 발전시켜 나가는 과정을 성공적으로 수행했습니다. 이를 통해 저비용 엣지 환경에서도 LLM, VLLM과 같은 최신 AI 기술을 로봇에 통합할 수 있음을 확인하였고, 그 과정에서 얻은 기술적 경험과 통찰, 그리고 현재 기술의 한계점을 명확히 할 수 있었습니다.
향후 연구로는 현재 계획 단계인 UGV와 Ollama의 통합을 실제로 구현하고 성능을 평가하는 것이 최우선 과제입니다. 더불어, 보다 지능적인 장애물 회피 및 경로 계획 알고리즘 도입, 객체 인식/추적 기능 고도화, 음성 및 제스처를 포함한 다중 모달 상호작용 연구, 그리고 성능 최적화 및 실 환경 테스트를 통해 로봇의 지능과 안정성을 더욱 향상시킬 계획입니다."

---

**(슬라이드 12: Q&A)**

*   **질의응답**
*   **감사합니다.**
*   **연락처:** [이메일 주소], [GitHub 프로필 링크]
*   **참고 자료:** (필요시 주요 참고 논문, 웹사이트 등 기재)

**(발표자 스크립트 - 슬라이드 12)**
"이상으로 발표를 마치겠습니다. 경청해주셔서 감사합니다. 질문 있으시면 편하게 해주시기 바랍니다."

---

**추가 팁:**

*   각 슬라이드에는 핵심 내용을 간결하게 담고, 발표자는 스크립트를 참고하여 부연 설명을 하는 것이 좋습니다.
*   실제 발표 시에는 `[이미지/영상: ...]` 부분에 적절한 시각 자료를 반드시 포함해야 합니다. 데모 영상이나 UI 스크린샷은 매우 효과적입니다.
*   발표 대상과 시간에 맞춰 스크립트의 상세 수준을 조절하세요.
*   발표 연습을 충분히 하여 자연스럽게 설명할 수 있도록 준비하는 것이 중요합니다.

이 대본이 성공적인 발표에 도움이 되기를 바랍니다!
